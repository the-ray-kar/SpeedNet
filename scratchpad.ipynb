{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from dataset_maker import datasetmaker,data_browser\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point 1 selected: (5, 291)\n",
      "Point 2 selected: (619, 296)\n",
      "Point 3 selected: (516, 267)\n",
      "Point 4 selected: (138, 258)\n"
     ]
    }
   ],
   "source": [
    "datamaker  = datasetmaker(\"../sp_data/train.mp4\",\"../sp_data/train.txt\")\n",
    "points = datamaker.get_points_video()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 132, 619)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#these would be final as whole model depends upon it. As width and height calculated depends on these points\n",
    "points\n",
    "width =  int(np.sum(np.sqrt( (points[0]-points[1]) **2) )) \n",
    "height = int(np.sum(np.sqrt( (points[2]-points[1]) **2) ))\n",
    "tensor_shape = 1,3,height,width\n",
    "tensor_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeedNet(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(SpeedNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[1], out_channels=30, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=30, out_channels=1, kernel_size=10)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Calculate the size of the flattened features after the convolution and pooling layers\n",
    "        # This depends on the input image size and the layers' configurations\n",
    "        conv_output_size = self._get_conv_output(input_shape)\n",
    "        \n",
    "        self.fc1 = nn.Linear(conv_output_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        \n",
    "    def _get_conv_output(self, shapes):\n",
    "        x = torch.rand(shapes[1:],dtype=torch.float32)\n",
    "        print(x.shape)\n",
    "        x = self.pool(self.conv2(self.conv1(x)))\n",
    "        sizee = int(torch.prod(torch.tensor(x.size())))\n",
    "        print(sizee)\n",
    "        return sizee\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #print(\"1\",x[0,0,0,0])\n",
    "        x = self.conv1(x)\n",
    "     \n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        #print(\"2\",x[0,0,0,0])\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "        #print(\"3\",x[0,0,0,0])\n",
    "        x = self.flatten(x)\n",
    "        #print(x[0,0])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 132, 619])\n",
      "18240\n"
     ]
    }
   ],
   "source": [
    "model = SpeedNet(tensor_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshayd/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeedNet(\n",
       "  (conv1): Conv2d(3, 30, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(30, 1, kernel_size=(10, 10), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=18240, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.034446001239851406\n",
      "Epoch 1, Loss: 0.028248922310623467\n",
      "Epoch 1, Loss: 0.01836553835401348\n",
      "Epoch 1, Loss: 0.012824621761546416\n",
      "Epoch 1, Loss: 0.013389220892214308\n",
      "Epoch 1, Loss: 0.01480935489430147\n",
      "Epoch 1, Loss: 0.0196737356746898\n",
      "Epoch 1, Loss: 0.024007014854281555\n",
      "Epoch 1, Loss: 0.019445253259995403\n",
      "Epoch 1, Loss: 0.017851966409122243\n",
      "Epoch 1, Loss: 0.01562648847991345\n",
      "Epoch 1, Loss: 0.012200661453546263\n",
      "Epoch 1, Loss: 0.00971560983096852\n",
      "Epoch 1, Loss: 0.010342975691253064\n",
      "Epoch 1, Loss: 0.012320078681497013\n",
      "Epoch 1, Loss: 0.014884791654698989\n",
      "Epoch 1, Loss: 0.017748119877833948\n",
      "Epoch 1, Loss: 0.018561987783394608\n",
      "Epoch 1, Loss: 0.029261061724494487\n",
      "Epoch 1, Loss: 0.026741692038143382\n",
      "Epoch 1, Loss: 0.02539392209520527\n",
      "Epoch 1, Loss: 0.028346078910079658\n",
      "Epoch 1, Loss: 0.029862997017654717\n",
      "Epoch 1, Loss: 0.0286373841528799\n",
      "Epoch 1, Loss: 0.03121762743183211\n",
      "Epoch 1, Loss: 0.03106094958735447\n",
      "Epoch 1, Loss: 0.03171743953929228\n",
      "Epoch 1, Loss: 0.03245514215207567\n",
      "Epoch 1, Loss: 0.03007298189051011\n",
      "Epoch 1, Loss: 0.022996859082988663\n",
      "Epoch 1, Loss: 0.023919822842467065\n",
      "Epoch 1, Loss: 0.0230358138738894\n",
      "Epoch 1, Loss: 0.023453081916360294\n",
      "Epoch 1, Loss: 0.024382292803596047\n",
      "Epoch 1, Loss: 0.024634007472617955\n",
      "Epoch 1, Loss: 0.023099139344458487\n",
      "Epoch 1, Loss: 0.02422814313103171\n",
      "Epoch 1, Loss: 0.02266063615387561\n",
      "Epoch 1, Loss: 0.022236896589690563\n",
      "Epoch 1, Loss: 0.02294982760560279\n",
      "Epoch 1, Loss: 0.02203340717390472\n",
      "Epoch 1, Loss: 0.023716782214594823\n",
      "Epoch 1, Loss: 0.02407513786764706\n",
      "Epoch 1, Loss: 0.023460301417930454\n",
      "Epoch 1, Loss: 0.02357142130533854\n",
      "Epoch 1, Loss: 0.023029032688514858\n",
      "Epoch 1, Loss: 0.022243662815467986\n",
      "Epoch 1, Loss: 0.024795804491230086\n",
      "Epoch 1, Loss: 0.024641994401520373\n",
      "Epoch 1, Loss: 0.02409172955681296\n",
      "Epoch 1, Loss: 0.024460860607670804\n",
      "Epoch 1, Loss: 0.0256301939721201\n",
      "Epoch 1, Loss: 0.02750494265088848\n",
      "Epoch 1, Loss: 0.02928577797085631\n",
      "Epoch 1, Loss: 0.025149748559091606\n",
      "Epoch 1, Loss: 0.027702902999578737\n",
      "Epoch 1, Loss: 0.025828980090571385\n",
      "Epoch 1, Loss: 0.019241141524969363\n",
      "Epoch 1, Loss: 0.014407963472254136\n",
      "Epoch 1, Loss: 0.00823513853783701\n",
      "Epoch 1, Loss: 0.006689689486634498\n",
      "Epoch 1, Loss: 0.006926390703986673\n",
      "Epoch 1, Loss: 0.007770431368958716\n",
      "Epoch 1, Loss: 0.01023010627896178\n",
      "Epoch 1, Loss: 0.010301414938534008\n",
      "Epoch 1, Loss: 0.014808023490157782\n",
      "Epoch 1, Loss: 0.01616907456341912\n",
      "Epoch 1, Loss: 0.012178979013480392\n",
      "Epoch 1, Loss: 0.007392112881529565\n",
      "Epoch 1, Loss: 0.013229527192957261\n",
      "Epoch 1, Loss: 0.017414034675149355\n",
      "Epoch 1, Loss: 0.021359635895373773\n",
      "Epoch 1, Loss: 0.018992379880418964\n",
      "Epoch 1, Loss: 0.015962069642310048\n",
      "Epoch 1, Loss: 0.024698392082663143\n",
      "Epoch 1, Loss: 0.030021269555185356\n",
      "Epoch 1, Loss: 0.028642745672487747\n",
      "Epoch 1, Loss: 0.029418586282169118\n",
      "Epoch 1, Loss: 0.027580934412339154\n",
      "Epoch 1, Loss: 0.025491048775467217\n",
      "Epoch 1, Loss: 0.0295716827990962\n",
      "Epoch 1, Loss: 0.03309522740981158\n",
      "Epoch 1, Loss: 0.030794354607077207\n",
      "Epoch 1, Loss: 0.024889569750019148\n",
      "Epoch 1, Loss: 0.012501783183976716\n",
      "Epoch 1, Loss: 0.008800053316004136\n",
      "Epoch 1, Loss: 0.0064801668653301165\n",
      "Epoch 1, Loss: 0.0060053357891007965\n",
      "Epoch 1, Loss: 0.00501000161264457\n",
      "Epoch 1, Loss: 0.0042274059968836165\n",
      "Epoch 1, Loss: 0.0036666387670180375\n",
      "Epoch 1, Loss: 0.006103008120667701\n",
      "Epoch 1, Loss: 0.000489345251345167\n",
      "Epoch 1, Loss: 2.0652769827375225e-05\n",
      "Epoch 1, Loss: 0.0026594124588311886\n",
      "Epoch 1, Loss: 0.0034669778861251533\n",
      "Epoch 1, Loss: 0.003550269182990579\n",
      "Epoch 1, Loss: 0.0028936195373535155\n",
      "Epoch 1, Loss: 0.003644888260785271\n",
      "Epoch 1, Loss: 0.003954535465614468\n",
      "Epoch 1, Loss: 0.0010791893566355986\n",
      "Epoch 1, Loss: 0.0018265615725049785\n",
      "Epoch 1, Loss: 0.0027245788948208676\n",
      "Epoch 1, Loss: 0.0019460201263427734\n",
      "Epoch 1, Loss: 0.0030724495532465914\n",
      "Epoch 1, Loss: 0.0002685838353400137\n",
      "Epoch 1, Loss: 0.0003709000465916652\n",
      "Epoch 1, Loss: 0.00275935453527114\n",
      "Epoch 1, Loss: 0.0007012706644394819\n",
      "Epoch 1, Loss: 0.001984701343611175\n",
      "Epoch 1, Loss: 0.0004580296722112917\n",
      "Epoch 1, Loss: 0.0024965708863501457\n",
      "Epoch 1, Loss: 0.00084685998804429\n",
      "Epoch 1, Loss: 0.0002707642199946385\n",
      "Epoch 1, Loss: 0.001284016347398945\n",
      "Epoch 1, Loss: 0.0018795402377259497\n",
      "Epoch 1, Loss: 0.00319581200094784\n",
      "Epoch 1, Loss: 0.003111403222177543\n",
      "Epoch 1, Loss: 0.002449691996854894\n",
      "Epoch 1, Loss: 0.001927637025421741\n",
      "Epoch 1, Loss: 0.00481483122881721\n",
      "Epoch 1, Loss: 0.0019933801538803996\n",
      "Epoch 1, Loss: 0.0016518633973364736\n",
      "Epoch 1, Loss: 0.004808752770517386\n",
      "Epoch 1, Loss: 0.003889426811068666\n",
      "Epoch 1, Loss: 0.004823191025677849\n",
      "Epoch 1, Loss: 0.005470860425163718\n",
      "Epoch 1, Loss: 0.0037764272502824373\n",
      "Epoch 1, Loss: 0.0005426562066171683\n",
      "Epoch 1, Loss: 0.001973571964338714\n",
      "Epoch 1, Loss: 0.0010549321829103956\n",
      "Epoch 1, Loss: 0.002682041093414905\n",
      "Epoch 1, Loss: 0.0012595635769413966\n",
      "Epoch 1, Loss: 0.0001207213191425099\n",
      "Epoch 1, Loss: 0.0\n",
      "Epoch 1, Loss: 0.00018687836095398547\n",
      "Epoch 1, Loss: 0.0032555931689692476\n",
      "Epoch 1, Loss: 0.0054422228944067865\n",
      "Epoch 1, Loss: 0.004131619322533701\n",
      "Epoch 1, Loss: 0.0019595019022623696\n",
      "Epoch 1, Loss: 0.00036366815660514084\n",
      "Epoch 1, Loss: 0.0008349458844053979\n",
      "Epoch 1, Loss: 0.001511996400122549\n",
      "Epoch 1, Loss: 0.0010127561232622932\n",
      "Epoch 1, Loss: 2.78469969463699e-07\n",
      "Epoch 1, Loss: 0.0\n",
      "Epoch 1, Loss: 3.3560234541986504e-05\n",
      "Epoch 1, Loss: 0.0017957928601433249\n",
      "Epoch 1, Loss: 0.009299069572897518\n",
      "Epoch 1, Loss: 0.009438591003417969\n",
      "Epoch 1, Loss: 0.008193990669998468\n",
      "Epoch 1, Loss: 0.00765867644665288\n",
      "Epoch 1, Loss: 0.007005020590389476\n",
      "Epoch 1, Loss: 0.0022725570903104893\n",
      "Epoch 1, Loss: 5.8905266079248166e-05\n",
      "Epoch 1, Loss: 2.0752034935296752e-05\n",
      "Epoch 1, Loss: 0.00019655931229684866\n",
      "Epoch 1, Loss: 0.0\n",
      "Epoch 1, Loss: 0.00021591897104300705\n",
      "Epoch 1, Loss: 9.940063252168544e-05\n",
      "Epoch 1, Loss: 0.00016489116584553437\n",
      "Epoch 1, Loss: 0.0062461071388394225\n",
      "Epoch 1, Loss: 0.00538725684670841\n",
      "Epoch 1, Loss: 0.0009222762724932503\n",
      "Epoch 1, Loss: 0.00014953927666533227\n",
      "Epoch 1, Loss: 0.0002442849851122089\n",
      "Epoch 1, Loss: 0.00028122392355227\n",
      "Epoch 1, Loss: 0.00011785943134158265\n",
      "Epoch 1, Loss: 0.0002030004239549824\n",
      "Epoch 1, Loss: 0.0002145834062613693\n",
      "Epoch 1, Loss: 0.0003643336483076507\n",
      "Epoch 1, Loss: 0.00040298976150213503\n",
      "Epoch 1, Loss: 0.0042254133785472195\n",
      "Epoch 1, Loss: 0.010420907712450215\n",
      "Epoch 1, Loss: 0.009982070922851562\n",
      "Epoch 1, Loss: 0.010032660540412455\n",
      "Epoch 1, Loss: 0.007034158145680147\n",
      "Epoch 1, Loss: 0.001219958604550829\n",
      "Epoch 1, Loss: 0.00011467830807554956\n",
      "Epoch 1, Loss: 9.79113403488608e-05\n",
      "Epoch 1, Loss: 0.00026848657458436253\n",
      "Epoch 1, Loss: 0.00025248431691936415\n",
      "Epoch 1, Loss: 0.007248120027429917\n",
      "Epoch 1, Loss: 0.006770965725767846\n",
      "Epoch 1, Loss: 0.0013749834135466932\n",
      "Epoch 1, Loss: 5.8057429451568455e-06\n",
      "Epoch 1, Loss: 0.0\n",
      "Epoch 1, Loss: 0.0\n",
      "Epoch 1, Loss: 8.346230376000498e-05\n",
      "Epoch 1, Loss: 0.002113173615698721\n",
      "Epoch 1, Loss: 0.005188098982268689\n",
      "Epoch 1, Loss: 0.004333154640945734\n",
      "Epoch 1, Loss: 0.001157233200821222\n",
      "Epoch 1, Loss: 0.000391156416313321\n",
      "Epoch 1, Loss: 0.0005093724119896982\n",
      "Epoch 1, Loss: 5.733476549971337e-05\n",
      "Epoch 1, Loss: 0.00031764748049717325\n",
      "Epoch 1, Loss: 0.0005892092106389064\n",
      "Epoch 1, Loss: 0.002637619130751666\n",
      "Epoch 1, Loss: 0.005397841135660808\n",
      "Epoch 1, Loss: 0.0029834719265208524\n",
      "Epoch 1, Loss: 0.00248840145036286\n",
      "Epoch 1, Loss: 0.001342602617600385\n",
      "Epoch 1, Loss: 49253353515.818596\n",
      "Epoch 2, Loss: 0.034446001239851406\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), label)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Print statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    frame_count = 0\n",
    "    for input, label in datamaker.generate_tensor_data(points):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input)\n",
    "        loss = criterion(outputs.squeeze(), label)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        frame_count+=1\n",
    "        if(frame_count%100==0):\n",
    "            print(f'Epoch {epoch+1}, Loss: {loss.item()/datamaker.frame_count}')\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/datamaker.frame_count}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"road_patch.npy\",points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
