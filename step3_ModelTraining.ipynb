{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10199, 2, 256, 64) (10199,)\n"
     ]
    }
   ],
   "source": [
    "#Load the dataset\n",
    "frame_data = np.load(\"../sp_data/Full_data/framedata.npy\",allow_pickle=True)\n",
    "labeldata = np.load(\"../sp_data/Full_data/meanlabels2.npy\",allow_pickle=True)\n",
    "frame_data = np.array(frame_data)\n",
    "print(frame_data.shape,labeldata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1493852, 28.105569)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_data[0].sum(),labeldata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1739558, 9.796058500000001)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def shuffle_data_and_labels(data, labels):\n",
    "    assert len(data) == len(labels), \"Data and labels must have the same number of samples\"\n",
    "    \n",
    "    # Get the number of samples\n",
    "    num_samples = len(labels)\n",
    "    \n",
    "    # Generate a permutation of indices\n",
    "    permutation = np.random.permutation(num_samples)\n",
    "    \n",
    "    # Shuffle the data and labels using the permutation\n",
    "    shuffled_data = data[permutation]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    \n",
    "    return shuffled_data, shuffled_labels\n",
    "\n",
    "frame_data,labeldata = shuffle_data_and_labels(frame_data,labeldata)\n",
    "frame_data[0].sum(),labeldata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10199, 2, 256, 64) (10199,)\n"
     ]
    }
   ],
   "source": [
    "print(frame_data.shape,labeldata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_data_tensor = torch.tensor(frame_data,dtype=torch.float32)/255 #Create tensor and normalize\n",
    "labels_tensor = torch.tensor(2*labeldata/max(labeldata)-1, dtype=torch.float32) #create tensor and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(frame_data) #free some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8159, 2, 256, 64]) torch.Size([8159]) torch.Size([2040, 2, 256, 64]) torch.Size([2040])\n"
     ]
    }
   ],
   "source": [
    "train_samples = int(0.8 * len(frame_data_tensor))\n",
    "train_input = frame_data_tensor[0:train_samples]\n",
    "test_input = frame_data_tensor[train_samples:]\n",
    "train_output = labels_tensor[0:train_samples]\n",
    "test_output = labels_tensor[train_samples:]\n",
    "print(train_input.shape,train_output.shape,test_input.shape,test_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before unsqueeze: torch.Size([2, 2, 256, 64])\n",
      "After unsqueeze: torch.Size([2, 1, 2, 256, 64])\n",
      "After conv3d_1: torch.Size([2, 8, 2, 252, 60])\n",
      "After pool3d_1: torch.Size([2, 8, 2, 252, 29])\n",
      "After conv3d_2: torch.Size([2, 16, 2, 248, 25])\n",
      "After pool3d_2: torch.Size([2, 16, 2, 248, 12])\n",
      "After conv3d_3: torch.Size([2, 32, 1, 244, 8])\n",
      "After squeeze: torch.Size([2, 32, 244, 8])\n",
      "After conv2d_1: torch.Size([2, 64, 235, 8])\n",
      "After pool2d_1: torch.Size([2, 64, 234, 4])\n",
      "After conv2d_2: torch.Size([2, 2, 234, 2])\n",
      "After flatten: torch.Size([2, 936])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class SpeedNet(nn.Module):\n",
    "    def __init__(self, in_channels, f1,f2,f3,f4,f5):\n",
    "        super(SpeedNet, self).__init__()\n",
    "        \n",
    "        self.batchnorm1 = nn.BatchNorm3d(f1)\n",
    "        self.batchnorm2 = nn.BatchNorm3d(f2)\n",
    "        self.batchnorm3 = nn.BatchNorm3d(f3)\n",
    "        self.batchnorm4 = nn.BatchNorm2d(f4)\n",
    "        self.batchnorm5 = nn.BatchNorm2d(f5)\n",
    "        # First 3D convolution layer\n",
    "        self.conv3d_1 = nn.Conv3d(in_channels, f1, kernel_size=(1, 5, 5))\n",
    "        self.pool3d_1 = nn.MaxPool3d(kernel_size=(1, 1, 3), stride=(1, 1, 2))\n",
    "        \n",
    "        # Second 3D convolution layer\n",
    "        self.conv3d_2 = nn.Conv3d(f1, f2, kernel_size=(1, 5, 5))\n",
    "        self.pool3d_2 = nn.MaxPool3d(kernel_size=(1, 1, 2), stride=(1, 1, 2))\n",
    "        \n",
    "        # Third 3D convolution layer\n",
    "        self.conv3d_3 = nn.Conv3d(f2, f3, kernel_size=(2, 5, 5))\n",
    "        \n",
    "        # 2D convolution layer\n",
    "        self.conv2d_1 = nn.Conv2d(f3, f4, kernel_size=(10, 1))\n",
    "        self.pool2d_1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(1, 2))\n",
    "\n",
    "        self.conv2d_2 = nn.Conv2d(f4, f5, kernel_size=(1, 3))\n",
    "        self.pool2d_2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(1, 2))\n",
    "\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(936, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32,1)\n",
    "        \n",
    "    def deb_forward(self, x):\n",
    "        # Reshape input tensor to add the single channel dimension\n",
    "        print(\"Before unsqueeze:\", x.shape)\n",
    "        x = x.unsqueeze(1)\n",
    "        print(\"After unsqueeze:\", x.shape)\n",
    "        \n",
    "        # First 3D convolution and pooling\n",
    "        x = self.conv3d_1(x)\n",
    "        print(\"After conv3d_1:\", x.shape)\n",
    "        x = self.pool3d_1(x)\n",
    "        print(\"After pool3d_1:\", x.shape)\n",
    "        \n",
    "        \n",
    "        # Second 3D convolution and pooling\n",
    "        x = self.conv3d_2(x)\n",
    "        print(\"After conv3d_2:\", x.shape)\n",
    "        x = self.pool3d_2(x)\n",
    "        print(\"After pool3d_2:\", x.shape)\n",
    "        \n",
    "        # Third 3D convolution\n",
    "        x = self.conv3d_3(x)\n",
    "        print(\"After conv3d_3:\", x.shape)\n",
    "        \n",
    "        # Squeeze the third dimension\n",
    "        x = x.squeeze(2)\n",
    "        print(\"After squeeze:\", x.shape)\n",
    "        \n",
    "        # First 2D convolution and pooling\n",
    "        x = self.conv2d_1(x)\n",
    "        print(\"After conv2d_1:\", x.shape)\n",
    "        x = self.pool2d_1(x)\n",
    "        print(\"After pool2d_1:\", x.shape)\n",
    "\n",
    "        # Second 2D convolution and pooling\n",
    "        x = self.conv2d_2(x)\n",
    "        print(\"After conv2d_2:\", x.shape)\n",
    "\n",
    "        \n",
    "        # Flatten the data\n",
    "        x = x.view(x.size(0), -1)\n",
    "        print(\"After flatten:\", x.shape)\n",
    "\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        # x = self.fc3(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reshape input tensor to add the single channel dimension\n",
    "        #print(\"Before unsqueeze:\", x.shape)\n",
    "        x = x.unsqueeze(1)\n",
    "        #print(\"After unsqueeze:\", x.shape)\n",
    "        \n",
    "        # First 3D convolution and pooling\n",
    "        x = self.conv3d_1(x)\n",
    "        #print(\"After conv3d_1:\", x.shape)\n",
    "        x = self.pool3d_1(x)\n",
    "        #print(\"After pool3d_1:\", x.shape)\n",
    "        x = self.batchnorm1(x)\n",
    "        \n",
    "        # Second 3D convolution and pooling\n",
    "        x = self.conv3d_2(x)\n",
    "        #print(\"After conv3d_2:\", x.shape)\n",
    "        x = self.pool3d_2(x)\n",
    "        #print(\"After pool3d_2:\", x.shape)\n",
    "        x = self.batchnorm2(x)\n",
    "        # Third 3D convolution\n",
    "        x = self.conv3d_3(x)\n",
    "        #print(\"After conv3d_3:\", x.shape)\n",
    "        x = self.batchnorm3(x)\n",
    "        # Squeeze the third dimension\n",
    "        x = x.squeeze(2)\n",
    "        #print(\"After squeeze:\", x.shape)\n",
    "        \n",
    "        # First 2D convolution and pooling\n",
    "        x = self.conv2d_1(x)\n",
    "        #print(\"After conv2d_1:\", x.shape)\n",
    "        x = self.pool2d_1(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        #print(\"After pool2d_1:\", x.shape)\n",
    "\n",
    "        # Second 2D convolution and pooling\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.batchnorm5(x)\n",
    "        #print(\"After conv2d_2:\", x.shape)\n",
    "     \n",
    "        \n",
    "        # Flatten the data\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(\"After flatten:\", x.shape)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = SpeedNet(1, 8,16,32,64,2)  # Adjust in_channels to 1 for the first 3D convolution\n",
    "input_tensor = train_input[0:2]\n",
    "\n",
    "output = model.deb_forward(input_tensor)\n",
    "output = model.forward(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0432], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Mini_SpeedNet(nn.Module):\n",
    "#     def __init__(self, input_shape):\n",
    "#         super(Mini_SpeedNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=input_shape[1], out_channels=24, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(in_channels=24, out_channels=3, kernel_size=3, padding=1)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.batchnorm1 = nn.BatchNorm2d(24)\n",
    "#         self.batchnorm2 = nn.BatchNorm2d(3)\n",
    "#         self.flatten = nn.Flatten()\n",
    "        \n",
    "#         # Calculate the size of the flattened features after the convolution and pooling layers\n",
    "#         conv_output_size = self._get_conv_output(input_shape)\n",
    "        \n",
    "#         self.fc1 = nn.Linear(conv_output_size, 64)\n",
    "#         self.fc2 = nn.Linear(64, 16)\n",
    "#         self.fc3 = nn.Linear(16,1)\n",
    "\n",
    "#     def conv_forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.batchnorm1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.pool(x)\n",
    "        \n",
    "#         x = self.conv2(x)\n",
    "#         x = self.batchnorm2(x)\n",
    "#         x = F.relu(x)\n",
    "#         #x = self.pool(x)\n",
    "        \n",
    "#         return x\n",
    "    \n",
    "#     def _get_conv_output(self, shape):\n",
    "#         x = torch.rand(shape[1:], dtype=torch.float32).unsqueeze(0)\n",
    "#         x = self.conv_forward(x)\n",
    "#         sizee = int(torch.prod(torch.tensor(x.size())))\n",
    "#         return sizee\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv_forward(x)\n",
    "#         x = self.flatten(x)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "\n",
    "#         return x\n",
    "    \n",
    "# model = Mini_SpeedNet(frame_data_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adagrad(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeedNet(\n",
       "  (batchnorm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm3): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm5): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3d_1): Conv3d(1, 8, kernel_size=(1, 5, 5), stride=(1, 1, 1))\n",
       "  (pool3d_1): MaxPool3d(kernel_size=(1, 1, 3), stride=(1, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3d_2): Conv3d(8, 16, kernel_size=(1, 5, 5), stride=(1, 1, 1))\n",
       "  (pool3d_2): MaxPool3d(kernel_size=(1, 1, 2), stride=(1, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3d_3): Conv3d(16, 32, kernel_size=(2, 5, 5), stride=(1, 1, 1))\n",
       "  (conv2d_1): Conv2d(32, 64, kernel_size=(10, 1), stride=(1, 1))\n",
       "  (pool2d_1): MaxPool2d(kernel_size=(2, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2d_2): Conv2d(64, 2, kernel_size=(1, 3), stride=(1, 1))\n",
       "  (pool2d_2): MaxPool2d(kernel_size=(2, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=936, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(tensor_data:torch.tensor,tensor_label:torch.tensor,batch_size=64):\n",
    "    start_index = np.random.randint(0,len(tensor_data)-batch_size-1)\n",
    "    end_index = start_index+batch_size\n",
    "    return tensor_data[start_index:end_index],tensor_label[start_index:end_index] #return a view of data\n",
    "\n",
    "def eval_batch(tensor_data:torch.tensor,tensor_label:torch.tensor,epoch,eval_batch_size = 512):\n",
    "    start = np.random.randint(0,len(tensor_label)-eval_batch_size-1)\n",
    "    end = start+eval_batch_size\n",
    "    output_test = model(tensor_data[start:end])\n",
    "    loss_test = criterion(output_test.squeeze(),tensor_label[start:end])\n",
    "    print(f'Epoch {epoch+1}, Loss: {np.sqrt(loss_test.item())*max(labeldata)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating\n",
      "Epoch 1, Loss: 20.667671256642496\n",
      "Epoch 1, Loss: 17.454074981412823\n",
      "Epoch 11, Loss: 10.499508986206925\n",
      "Epoch 21, Loss: 8.040229409069923\n",
      "Epoch 31, Loss: 7.8846761881943515\n",
      "Epoch 41, Loss: 7.276973134988043\n",
      "Epoch 51, Loss: 5.060534390613447\n",
      "Epoch 61, Loss: 8.109516372771969\n",
      "Epoch 71, Loss: 10.435005690167712\n",
      "Epoch 81, Loss: 12.494627474111569\n",
      "Epoch 91, Loss: 7.117615311196486\n",
      "Evaluating\n",
      "Epoch 101, Loss: 7.5466976328767945\n",
      "Epoch 101, Loss: 8.30643028954413\n",
      "Epoch 111, Loss: 12.116681637061419\n",
      "Epoch 121, Loss: 13.904473033190818\n",
      "Epoch 131, Loss: 10.142525351773017\n",
      "Epoch 141, Loss: 7.456214245593057\n",
      "Epoch 151, Loss: 10.709656445264375\n",
      "Epoch 161, Loss: 4.882993119320764\n",
      "Epoch 171, Loss: 10.338516376498639\n",
      "Epoch 181, Loss: 4.794300869122748\n",
      "Epoch 191, Loss: 5.775805542676842\n",
      "Evaluating\n",
      "Epoch 201, Loss: 6.5470925310979124\n",
      "Epoch 201, Loss: 6.993432666499296\n",
      "Epoch 211, Loss: 5.333276463288863\n",
      "Epoch 221, Loss: 13.169918524339785\n",
      "Epoch 231, Loss: 7.154495185174573\n",
      "Epoch 241, Loss: 7.841681835802377\n",
      "Epoch 251, Loss: 6.452233018014551\n",
      "Epoch 261, Loss: 7.033560970425452\n",
      "Epoch 271, Loss: 6.925369595180799\n",
      "Epoch 281, Loss: 6.080407101693521\n",
      "Epoch 291, Loss: 6.033011072419482\n",
      "Evaluating\n",
      "Epoch 301, Loss: 6.791632594985994\n",
      "Epoch 301, Loss: 5.455592729399903\n",
      "Epoch 311, Loss: 8.19296564291021\n",
      "Epoch 321, Loss: 8.523263513486622\n",
      "Epoch 331, Loss: 8.490115805323123\n",
      "Epoch 341, Loss: 5.908432776532996\n",
      "Epoch 351, Loss: 4.865199491839909\n",
      "Epoch 361, Loss: 6.932596433269672\n",
      "Epoch 371, Loss: 9.007157724632808\n",
      "Epoch 381, Loss: 10.697716023917035\n",
      "Epoch 391, Loss: 4.862202290713409\n",
      "Evaluating\n",
      "Epoch 401, Loss: 6.404890915342046\n",
      "Epoch 401, Loss: 5.315612768170396\n",
      "Epoch 411, Loss: 6.161417177656508\n",
      "Epoch 421, Loss: 8.479371128564631\n",
      "Epoch 431, Loss: 5.567049954600881\n",
      "Epoch 441, Loss: 6.100770668031043\n",
      "Epoch 451, Loss: 7.647773031889042\n",
      "Epoch 461, Loss: 6.195058115190546\n",
      "Epoch 471, Loss: 9.261587569059989\n",
      "Epoch 481, Loss: 6.30222820647821\n",
      "Epoch 491, Loss: 6.40550135880991\n",
      "Evaluating\n",
      "Epoch 501, Loss: 6.175213931768094\n",
      "Epoch 501, Loss: 5.864893543919325\n",
      "Epoch 511, Loss: 7.36839369356958\n",
      "Epoch 521, Loss: 7.191004885491264\n",
      "Epoch 531, Loss: 10.825345644876228\n",
      "Epoch 541, Loss: 7.63942461433374\n",
      "Epoch 551, Loss: 7.537338160852317\n",
      "Epoch 561, Loss: 4.051351917173395\n",
      "Epoch 571, Loss: 6.779979596999184\n",
      "Epoch 581, Loss: 5.963464381875949\n",
      "Epoch 591, Loss: 5.618914956263061\n",
      "Evaluating\n",
      "Epoch 601, Loss: 6.091040233136343\n",
      "Epoch 601, Loss: 10.304951948970457\n",
      "Epoch 611, Loss: 8.883432219579605\n",
      "Epoch 621, Loss: 4.579243366474097\n",
      "Epoch 631, Loss: 3.0523287912797326\n",
      "Epoch 641, Loss: 6.450924807144991\n",
      "Epoch 651, Loss: 7.776078019157548\n",
      "Epoch 661, Loss: 6.110827352691509\n",
      "Epoch 671, Loss: 3.0926166943294726\n",
      "Epoch 681, Loss: 6.016376696847385\n",
      "Epoch 691, Loss: 7.869559069512622\n",
      "Evaluating\n",
      "Epoch 701, Loss: 5.965301924785486\n",
      "Epoch 701, Loss: 8.246158274637457\n",
      "Epoch 711, Loss: 5.247714666941294\n",
      "Epoch 721, Loss: 6.463370943034923\n",
      "Epoch 731, Loss: 5.109681455391365\n",
      "Epoch 741, Loss: 6.620336206648863\n",
      "Epoch 751, Loss: 7.201581430531589\n",
      "Epoch 761, Loss: 4.972273640434917\n",
      "Epoch 771, Loss: 7.396795582429347\n",
      "Epoch 781, Loss: 5.909112451412702\n",
      "Epoch 791, Loss: 6.588338126879513\n",
      "Evaluating\n",
      "Epoch 801, Loss: 5.268547849790019\n",
      "Epoch 801, Loss: 7.8770193621093085\n",
      "Epoch 811, Loss: 4.368002519935681\n",
      "Epoch 821, Loss: 7.516528086601322\n",
      "Epoch 831, Loss: 7.108040168776218\n",
      "Epoch 841, Loss: 5.431643223374208\n",
      "Epoch 851, Loss: 8.14390335944518\n",
      "Epoch 861, Loss: 7.234319677258636\n",
      "Epoch 871, Loss: 7.29079091072204\n",
      "Epoch 881, Loss: 5.153981185092482\n",
      "Epoch 891, Loss: 5.249221465114812\n",
      "Evaluating\n",
      "Epoch 901, Loss: 5.408849520683479\n",
      "Epoch 901, Loss: 5.162472088549138\n",
      "Epoch 911, Loss: 5.81670436685367\n",
      "Epoch 921, Loss: 3.7816828227299544\n",
      "Epoch 931, Loss: 5.287083606158192\n",
      "Epoch 941, Loss: 7.021441164405356\n",
      "Epoch 951, Loss: 3.5589340745961113\n",
      "Epoch 961, Loss: 7.176664621031562\n",
      "Epoch 971, Loss: 5.694503310069081\n",
      "Epoch 981, Loss: 6.33853312364731\n",
      "Epoch 991, Loss: 5.541962833214818\n",
      "Evaluating\n",
      "Epoch 1001, Loss: 5.380106357300208\n",
      "Epoch 1001, Loss: 3.1771491276397104\n",
      "Epoch 1011, Loss: 7.084672449016133\n",
      "Epoch 1021, Loss: 4.404345898335709\n",
      "Epoch 1031, Loss: 6.063421855785968\n",
      "Epoch 1041, Loss: 5.794487571820061\n",
      "Epoch 1051, Loss: 6.141064063136206\n",
      "Epoch 1061, Loss: 8.867205757708321\n",
      "Epoch 1071, Loss: 5.2756698345715485\n",
      "Epoch 1081, Loss: 3.440461370228456\n",
      "Epoch 1091, Loss: 5.014170342322515\n",
      "Evaluating\n",
      "Epoch 1101, Loss: 5.779110627214591\n",
      "Epoch 1101, Loss: 4.648787524067995\n",
      "Epoch 1111, Loss: 4.639334672450679\n",
      "Epoch 1121, Loss: 2.6080301463283497\n",
      "Epoch 1131, Loss: 5.611268597889942\n",
      "Epoch 1141, Loss: 5.867989589464176\n",
      "Epoch 1151, Loss: 4.458764220580229\n",
      "Epoch 1161, Loss: 5.040397045349313\n",
      "Epoch 1171, Loss: 4.281910520533106\n",
      "Epoch 1181, Loss: 8.186827350471123\n",
      "Epoch 1191, Loss: 5.1121698911127265\n",
      "Evaluating\n",
      "Epoch 1201, Loss: 5.260398671349088\n",
      "Epoch 1201, Loss: 5.460747277653974\n",
      "Epoch 1211, Loss: 6.827203318577268\n",
      "Epoch 1221, Loss: 6.707816798676899\n",
      "Epoch 1231, Loss: 5.477418739280119\n",
      "Epoch 1241, Loss: 5.933949826370905\n",
      "Epoch 1251, Loss: 5.4516887725089465\n",
      "Epoch 1261, Loss: 7.519499723994847\n",
      "Epoch 1271, Loss: 3.9438540796047366\n",
      "Epoch 1281, Loss: 7.648616036965518\n",
      "Epoch 1291, Loss: 5.33349957457427\n",
      "Evaluating\n",
      "Epoch 1301, Loss: 5.0709130217170735\n",
      "Epoch 1301, Loss: 4.141905992328492\n",
      "Epoch 1311, Loss: 5.617760041249199\n",
      "Epoch 1321, Loss: 8.60961831457437\n",
      "Epoch 1331, Loss: 5.144807063150704\n",
      "Epoch 1341, Loss: 5.174677757614848\n",
      "Epoch 1351, Loss: 5.9636127989264445\n",
      "Epoch 1361, Loss: 4.910091229833818\n",
      "Epoch 1371, Loss: 3.829610091730931\n",
      "Epoch 1381, Loss: 6.899164401608688\n",
      "Epoch 1391, Loss: 5.245794485804857\n",
      "Evaluating\n",
      "Epoch 1401, Loss: 4.727675477348953\n",
      "Epoch 1401, Loss: 3.0170283668936837\n",
      "Epoch 1411, Loss: 4.737956906220499\n",
      "Epoch 1421, Loss: 4.6165538972065585\n",
      "Epoch 1431, Loss: 5.764039378981431\n",
      "Epoch 1441, Loss: 5.801293652650243\n",
      "Epoch 1451, Loss: 4.466239577118105\n",
      "Epoch 1461, Loss: 7.006763962323172\n",
      "Epoch 1471, Loss: 3.3859690900119404\n",
      "Epoch 1481, Loss: 8.169374541505771\n",
      "Epoch 1491, Loss: 7.551241571571221\n",
      "Evaluating\n",
      "Epoch 1501, Loss: 4.806807989954741\n",
      "Epoch 1501, Loss: 5.409261185474277\n",
      "Epoch 1511, Loss: 3.0234673517094537\n",
      "Epoch 1521, Loss: 4.945420669875092\n",
      "Epoch 1531, Loss: 4.865128659477829\n",
      "Epoch 1541, Loss: 7.902014927877172\n",
      "Epoch 1551, Loss: 4.139310478658323\n",
      "Epoch 1561, Loss: 4.933552003318983\n",
      "Epoch 1571, Loss: 5.734362333760714\n",
      "Epoch 1581, Loss: 6.04122088861593\n",
      "Epoch 1591, Loss: 4.882376464991794\n",
      "Evaluating\n",
      "Epoch 1601, Loss: 4.76093860966299\n",
      "Epoch 1601, Loss: 5.647375207575513\n",
      "Epoch 1611, Loss: 5.419756768060219\n",
      "Epoch 1621, Loss: 5.360368429875456\n",
      "Epoch 1631, Loss: 4.2130098521550945\n",
      "Epoch 1641, Loss: 7.407563397386664\n",
      "Epoch 1651, Loss: 7.509104634847629\n",
      "Epoch 1661, Loss: 7.1431949944292\n",
      "Epoch 1671, Loss: 5.444329625775626\n",
      "Epoch 1681, Loss: 6.7405083889315485\n",
      "Epoch 1691, Loss: 6.126487576937534\n",
      "Evaluating\n",
      "Epoch 1701, Loss: 4.592593732213101\n",
      "Epoch 1701, Loss: 4.9325253329814895\n",
      "Epoch 1711, Loss: 3.7465766063639636\n",
      "Epoch 1721, Loss: 7.033862891926002\n",
      "Epoch 1731, Loss: 3.361458436754487\n",
      "Epoch 1741, Loss: 4.613453098839282\n",
      "Epoch 1751, Loss: 5.947692601173863\n",
      "Epoch 1761, Loss: 4.162043789311122\n",
      "Epoch 1771, Loss: 5.9252374700208605\n",
      "Epoch 1781, Loss: 6.709612691542314\n",
      "Epoch 1791, Loss: 6.240080803264326\n",
      "Evaluating\n",
      "Epoch 1801, Loss: 4.750473950621686\n",
      "Epoch 1801, Loss: 4.059098852612684\n",
      "Epoch 1811, Loss: 5.349433969657157\n",
      "Epoch 1821, Loss: 3.5194233511079966\n",
      "Epoch 1831, Loss: 5.325279713861057\n",
      "Epoch 1841, Loss: 5.382128295065848\n",
      "Epoch 1851, Loss: 4.584898152219736\n",
      "Epoch 1861, Loss: 9.744299339929928\n",
      "Epoch 1871, Loss: 4.374844961023502\n",
      "Epoch 1881, Loss: 3.475382068796134\n",
      "Epoch 1891, Loss: 5.9400259225588075\n",
      "Evaluating\n",
      "Epoch 1901, Loss: 4.6302101763736925\n",
      "Epoch 1901, Loss: 4.098201603041272\n",
      "Epoch 1911, Loss: 4.541719881175482\n",
      "Epoch 1921, Loss: 4.564887354294353\n",
      "Epoch 1931, Loss: 6.2573168298428055\n",
      "Epoch 1941, Loss: 7.989719512821623\n",
      "Epoch 1951, Loss: 3.9118957652337527\n",
      "Epoch 1961, Loss: 5.328127123166188\n",
      "Epoch 1971, Loss: 5.3296570565248755\n",
      "Epoch 1981, Loss: 5.400025499798118\n",
      "Epoch 1991, Loss: 4.584447638653741\n",
      "Evaluating\n",
      "Epoch 2001, Loss: 4.623728134515751\n",
      "Epoch 2001, Loss: 6.140116975091375\n",
      "Epoch 2011, Loss: 3.7851290618538416\n",
      "Epoch 2021, Loss: 3.0200926224453033\n",
      "Epoch 2031, Loss: 4.632790694603046\n",
      "Epoch 2041, Loss: 4.091383156419086\n",
      "Epoch 2051, Loss: 2.4130722752769147\n",
      "Epoch 2061, Loss: 4.007099288118682\n",
      "Epoch 2071, Loss: 3.720235437707039\n",
      "Epoch 2081, Loss: 4.701637455550188\n",
      "Epoch 2091, Loss: 6.634080056713252\n",
      "Evaluating\n",
      "Epoch 2101, Loss: 4.47793612353919\n",
      "Epoch 2101, Loss: 5.4091528270131475\n",
      "Epoch 2111, Loss: 5.3558627563645835\n",
      "Epoch 2121, Loss: 6.535101520823408\n",
      "Epoch 2131, Loss: 5.768022484115761\n",
      "Epoch 2141, Loss: 3.272872166243629\n",
      "Epoch 2151, Loss: 4.766442105355015\n",
      "Epoch 2161, Loss: 4.717054136604078\n",
      "Epoch 2171, Loss: 2.9709745170998794\n",
      "Epoch 2181, Loss: 3.7247641044370523\n",
      "Epoch 2191, Loss: 3.8494848288516215\n",
      "Evaluating\n",
      "Epoch 2201, Loss: 4.524449722910555\n",
      "Epoch 2201, Loss: 5.503467811752525\n",
      "Epoch 2211, Loss: 7.151237608659008\n",
      "Epoch 2221, Loss: 3.289928971976013\n",
      "Epoch 2231, Loss: 5.081136551714275\n",
      "Epoch 2241, Loss: 6.403046815799291\n",
      "Epoch 2251, Loss: 4.587574141598316\n",
      "Epoch 2261, Loss: 3.861246950208854\n",
      "Epoch 2271, Loss: 6.801641054956602\n",
      "Epoch 2281, Loss: 6.6831994430698485\n",
      "Epoch 2291, Loss: 2.342791801172231\n",
      "Evaluating\n",
      "Epoch 2301, Loss: 4.502646635518873\n",
      "Epoch 2301, Loss: 7.662878737897857\n",
      "Epoch 2311, Loss: 5.260954360953344\n",
      "Epoch 2321, Loss: 4.9747642016290134\n",
      "Epoch 2331, Loss: 5.186933352118073\n",
      "Epoch 2341, Loss: 2.9123128299284864\n",
      "Epoch 2351, Loss: 4.267961202258451\n",
      "Epoch 2361, Loss: 4.727835744205527\n",
      "Epoch 2371, Loss: 5.014861093571671\n",
      "Epoch 2381, Loss: 5.085762548904029\n",
      "Epoch 2391, Loss: 6.237676382355646\n",
      "Evaluating\n",
      "Epoch 2401, Loss: 4.5758403498692255\n",
      "Epoch 2401, Loss: 5.063413466691984\n",
      "Epoch 2411, Loss: 4.318377042602573\n",
      "Epoch 2421, Loss: 4.903283737526007\n",
      "Epoch 2431, Loss: 4.523545563397378\n",
      "Epoch 2441, Loss: 4.536340306761924\n",
      "Epoch 2451, Loss: 2.200284935414847\n",
      "Epoch 2461, Loss: 5.572607507019432\n",
      "Epoch 2471, Loss: 4.478349011443028\n",
      "Epoch 2481, Loss: 3.6954437697045814\n",
      "Epoch 2491, Loss: 3.6543324357699234\n",
      "Evaluating\n",
      "Epoch 2501, Loss: 4.340961366777157\n",
      "Epoch 2501, Loss: 5.162600458317084\n",
      "Epoch 2511, Loss: 6.9132573773105355\n",
      "Epoch 2521, Loss: 2.9102825776433527\n",
      "Epoch 2531, Loss: 4.309846785153811\n",
      "Epoch 2541, Loss: 4.690505529590175\n",
      "Epoch 2551, Loss: 3.6635306240621515\n",
      "Epoch 2561, Loss: 2.5719409112863354\n",
      "Epoch 2571, Loss: 3.226656608130114\n",
      "Epoch 2581, Loss: 6.147177169121918\n",
      "Epoch 2591, Loss: 4.668683179750198\n",
      "Evaluating\n",
      "Epoch 2601, Loss: 4.322947327178429\n",
      "Epoch 2601, Loss: 5.0272054436481595\n",
      "Epoch 2611, Loss: 5.125535309167208\n",
      "Epoch 2621, Loss: 6.43708022804086\n",
      "Epoch 2631, Loss: 5.526810869011718\n",
      "Epoch 2641, Loss: 2.824378574207546\n",
      "Epoch 2651, Loss: 3.920002284698786\n",
      "Epoch 2661, Loss: 3.2817004978566438\n",
      "Epoch 2671, Loss: 4.263726785044403\n",
      "Epoch 2681, Loss: 3.4109700599956927\n",
      "Epoch 2691, Loss: 4.581521676166629\n",
      "Evaluating\n",
      "Epoch 2701, Loss: 4.475417357029544\n",
      "Epoch 2701, Loss: 4.341133874985932\n",
      "Epoch 2711, Loss: 2.953249500797649\n",
      "Epoch 2721, Loss: 4.360113201219877\n",
      "Epoch 2731, Loss: 5.24668828127369\n",
      "Epoch 2741, Loss: 4.1077370544272505\n",
      "Epoch 2751, Loss: 5.02082866447417\n",
      "Epoch 2761, Loss: 4.07787854969308\n",
      "Epoch 2771, Loss: 3.812856441694864\n",
      "Epoch 2781, Loss: 5.020118785969253\n",
      "Epoch 2791, Loss: 5.3837990772223\n",
      "Evaluating\n",
      "Epoch 2801, Loss: 4.086967776459321\n",
      "Epoch 2801, Loss: 3.380220705561174\n",
      "Epoch 2811, Loss: 5.075839354271667\n",
      "Epoch 2821, Loss: 6.793726511839502\n",
      "Epoch 2831, Loss: 4.280055451778918\n",
      "Epoch 2841, Loss: 4.364814223707473\n",
      "Epoch 2851, Loss: 3.9172563603808577\n",
      "Epoch 2861, Loss: 7.447358838883018\n",
      "Epoch 2871, Loss: 3.060488373349881\n",
      "Epoch 2881, Loss: 4.5262103215697875\n",
      "Epoch 2891, Loss: 3.015212105045039\n",
      "Evaluating\n",
      "Epoch 2901, Loss: 4.411647580166725\n",
      "Epoch 2901, Loss: 6.607509744989533\n",
      "Epoch 2911, Loss: 4.6808954584054625\n",
      "Epoch 2921, Loss: 5.885465049766184\n",
      "Epoch 2931, Loss: 7.056381697767456\n",
      "Epoch 2941, Loss: 3.8484307045734694\n",
      "Epoch 2951, Loss: 2.303512339465937\n",
      "Epoch 2961, Loss: 5.449287545950504\n",
      "Epoch 2971, Loss: 3.7327945096391937\n",
      "Epoch 2981, Loss: 4.137974653228943\n",
      "Epoch 2991, Loss: 4.053854620386018\n",
      "Evaluating\n",
      "Epoch 3001, Loss: 4.2242914556991815\n",
      "Epoch 3001, Loss: 3.9636408430722194\n",
      "Epoch 3011, Loss: 4.79315665348213\n",
      "Epoch 3021, Loss: 5.832308438211481\n",
      "Epoch 3031, Loss: 3.4325257627755037\n",
      "Epoch 3041, Loss: 5.179743094096825\n",
      "Epoch 3051, Loss: 3.2306752423714418\n",
      "Epoch 3061, Loss: 5.325052386337178\n",
      "Epoch 3071, Loss: 7.934703297984667\n",
      "Epoch 3081, Loss: 4.895523634313664\n",
      "Epoch 3091, Loss: 2.788640543916662\n",
      "Evaluating\n",
      "Epoch 3101, Loss: 3.9865227468964983\n",
      "Epoch 3101, Loss: 5.869858525924496\n",
      "Epoch 3111, Loss: 4.9666326204185145\n",
      "Epoch 3121, Loss: 3.7248560291290413\n",
      "Epoch 3131, Loss: 3.718635616554765\n",
      "Epoch 3141, Loss: 4.334845362901822\n",
      "Epoch 3151, Loss: 3.767011053563172\n",
      "Epoch 3161, Loss: 3.1770749623307766\n",
      "Epoch 3171, Loss: 5.341546879393584\n",
      "Epoch 3181, Loss: 4.180036398718345\n",
      "Epoch 3191, Loss: 6.216781978792687\n",
      "Evaluating\n",
      "Epoch 3201, Loss: 3.834602787381626\n",
      "Epoch 3201, Loss: 3.2872927834708006\n",
      "Epoch 3211, Loss: 3.306827213481212\n",
      "Epoch 3221, Loss: 5.245008639680464\n",
      "Epoch 3231, Loss: 5.273160806914828\n",
      "Epoch 3241, Loss: 3.794306317709193\n",
      "Epoch 3251, Loss: 4.004318019522088\n",
      "Epoch 3261, Loss: 3.448957239524967\n",
      "Epoch 3271, Loss: 3.109055396359232\n",
      "Epoch 3281, Loss: 3.9453056525365287\n",
      "Epoch 3291, Loss: 2.561039240007848\n",
      "Evaluating\n",
      "Epoch 3301, Loss: 4.070105847594159\n",
      "Epoch 3301, Loss: 4.112100408240316\n",
      "Epoch 3311, Loss: 4.254136812704884\n",
      "Epoch 3321, Loss: 2.9576532134146567\n",
      "Epoch 3331, Loss: 5.005745622608641\n",
      "Epoch 3341, Loss: 2.6275578543048845\n",
      "Epoch 3351, Loss: 3.819117354446776\n",
      "Epoch 3361, Loss: 5.085547971172823\n",
      "Epoch 3371, Loss: 4.859138534213888\n",
      "Epoch 3381, Loss: 3.7130368628048758\n",
      "Epoch 3391, Loss: 4.102577970185965\n",
      "Evaluating\n",
      "Epoch 3401, Loss: 3.9565581301274655\n",
      "Epoch 3401, Loss: 3.7242850708867827\n",
      "Epoch 3411, Loss: 4.4991783195474\n",
      "Epoch 3421, Loss: 3.3974950906133095\n",
      "Epoch 3431, Loss: 3.659464063274204\n",
      "Epoch 3441, Loss: 3.0708763813552196\n",
      "Epoch 3451, Loss: 4.501614272882462\n",
      "Epoch 3461, Loss: 4.060568345093719\n",
      "Epoch 3471, Loss: 3.56879395787979\n",
      "Epoch 3481, Loss: 3.559612025896297\n",
      "Epoch 3491, Loss: 4.66778156062127\n",
      "Evaluating\n",
      "Epoch 3501, Loss: 4.050844974232426\n",
      "Epoch 3501, Loss: 3.633103994877761\n",
      "Epoch 3511, Loss: 4.492924670466401\n",
      "Epoch 3521, Loss: 4.109632817614726\n",
      "Epoch 3531, Loss: 4.1239480738092675\n",
      "Epoch 3541, Loss: 5.4191170838204785\n",
      "Epoch 3551, Loss: 3.4763008515312883\n",
      "Epoch 3561, Loss: 5.795606253280397\n",
      "Epoch 3571, Loss: 3.8098205058296988\n",
      "Epoch 3581, Loss: 4.098905153823736\n",
      "Epoch 3591, Loss: 3.4297851298540682\n",
      "Evaluating\n",
      "Epoch 3601, Loss: 3.773127320882499\n",
      "Epoch 3601, Loss: 3.3826831471936267\n",
      "Epoch 3611, Loss: 4.442832333159575\n",
      "Epoch 3621, Loss: 3.65508778165704\n",
      "Epoch 3631, Loss: 4.27808098878711\n",
      "Epoch 3641, Loss: 4.046174078979535\n",
      "Epoch 3651, Loss: 3.152253431726508\n",
      "Epoch 3661, Loss: 4.609604598624593\n",
      "Epoch 3671, Loss: 2.7610812896312327\n",
      "Epoch 3681, Loss: 4.199334043508375\n",
      "Epoch 3691, Loss: 3.917965903828271\n",
      "Evaluating\n",
      "Epoch 3701, Loss: 3.557169388891062\n",
      "Epoch 3701, Loss: 4.623005541451863\n",
      "Epoch 3711, Loss: 4.0145231531837915\n",
      "Epoch 3721, Loss: 3.3336660071855886\n",
      "Epoch 3731, Loss: 5.469273004377948\n",
      "Epoch 3741, Loss: 2.9705941708583063\n",
      "Epoch 3751, Loss: 2.7986606422279907\n",
      "Epoch 3761, Loss: 4.555785564372154\n",
      "Epoch 3771, Loss: 4.455314961913325\n",
      "Epoch 3781, Loss: 4.495134520448703\n",
      "Epoch 3791, Loss: 4.243474783496913\n",
      "Evaluating\n",
      "Epoch 3801, Loss: 3.6768051550173437\n",
      "Epoch 3801, Loss: 3.5201833804487865\n",
      "Epoch 3811, Loss: 3.4140925802229565\n",
      "Epoch 3821, Loss: 3.4839708482197422\n",
      "Epoch 3831, Loss: 3.976243157527951\n",
      "Epoch 3841, Loss: 6.751913900970422\n",
      "Epoch 3851, Loss: 4.401431202625865\n",
      "Epoch 3861, Loss: 3.855763609435414\n",
      "Epoch 3871, Loss: 3.158546487522629\n",
      "Epoch 3881, Loss: 3.9348951846004327\n",
      "Epoch 3891, Loss: 4.270546288755381\n",
      "Evaluating\n",
      "Epoch 3901, Loss: 4.19628151323866\n",
      "Epoch 3901, Loss: 4.383828427345846\n",
      "Epoch 3911, Loss: 5.450315498767194\n",
      "Epoch 3921, Loss: 3.0493191464125573\n",
      "Epoch 3931, Loss: 3.709275512862725\n",
      "Epoch 3941, Loss: 3.6907620162054284\n",
      "Epoch 3951, Loss: 3.528064789264538\n",
      "Epoch 3961, Loss: 4.048289844996119\n",
      "Epoch 3971, Loss: 5.408392895505654\n",
      "Epoch 3981, Loss: 5.7828946488472495\n",
      "Epoch 3991, Loss: 4.757573156878763\n",
      "Evaluating\n",
      "Epoch 4001, Loss: 3.744044918265472\n",
      "Epoch 4001, Loss: 4.341351493557228\n",
      "Epoch 4011, Loss: 2.9193625716393434\n",
      "Epoch 4021, Loss: 3.8319860986798906\n",
      "Epoch 4031, Loss: 6.584017423393475\n",
      "Epoch 4041, Loss: 3.3531468059304483\n",
      "Epoch 4051, Loss: 4.433684092957522\n",
      "Epoch 4061, Loss: 4.624986549724711\n",
      "Epoch 4071, Loss: 4.522008972928093\n",
      "Epoch 4081, Loss: 4.315497276227076\n",
      "Epoch 4091, Loss: 4.107381568646501\n",
      "Evaluating\n",
      "Epoch 4101, Loss: 3.980665925666848\n",
      "Epoch 4101, Loss: 5.108162037891496\n",
      "Epoch 4111, Loss: 5.715428894915413\n",
      "Epoch 4121, Loss: 4.773570627835772\n",
      "Epoch 4131, Loss: 2.1035679786541066\n",
      "Epoch 4141, Loss: 5.109341348060385\n",
      "Epoch 4151, Loss: 3.3763350324756702\n",
      "Epoch 4161, Loss: 3.6642764376421275\n",
      "Epoch 4171, Loss: 4.69146022741005\n",
      "Epoch 4181, Loss: 3.421849940395264\n",
      "Epoch 4191, Loss: 3.458880375410529\n",
      "Evaluating\n",
      "Epoch 4201, Loss: 3.910601071677093\n",
      "Epoch 4201, Loss: 3.168010281029908\n",
      "Epoch 4211, Loss: 3.8956561247848995\n",
      "Epoch 4221, Loss: 5.253733940724032\n",
      "Epoch 4231, Loss: 3.735825631263142\n",
      "Epoch 4241, Loss: 6.524153549057147\n",
      "Epoch 4251, Loss: 3.9765712953007304\n",
      "Epoch 4261, Loss: 4.776249290317352\n",
      "Epoch 4271, Loss: 3.3073639285058807\n",
      "Epoch 4281, Loss: 5.0888924434069285\n",
      "Epoch 4291, Loss: 4.366898869972919\n",
      "Evaluating\n",
      "Epoch 4301, Loss: 4.068642152584042\n",
      "Epoch 4301, Loss: 4.189208415121877\n",
      "Epoch 4311, Loss: 2.645033832365876\n",
      "Epoch 4321, Loss: 2.7020039651016527\n",
      "Epoch 4331, Loss: 5.013887200794996\n",
      "Epoch 4341, Loss: 2.9998450327457675\n",
      "Epoch 4351, Loss: 4.029900527920352\n",
      "Epoch 4361, Loss: 5.432832824006349\n",
      "Epoch 4371, Loss: 3.415608791159906\n",
      "Epoch 4381, Loss: 3.788135413131858\n",
      "Epoch 4391, Loss: 2.904435320249103\n",
      "Evaluating\n",
      "Epoch 4401, Loss: 3.748968705061135\n",
      "Epoch 4401, Loss: 5.475729456626532\n",
      "Epoch 4411, Loss: 3.96933097184945\n",
      "Epoch 4421, Loss: 4.858880911283354\n",
      "Epoch 4431, Loss: 3.937969513157203\n",
      "Epoch 4441, Loss: 3.4904886966335407\n",
      "Epoch 4451, Loss: 2.448925583597842\n",
      "Epoch 4461, Loss: 2.6168231684916927\n",
      "Epoch 4471, Loss: 3.0878708384326106\n",
      "Epoch 4481, Loss: 5.781190184122811\n",
      "Epoch 4491, Loss: 4.184980530457357\n",
      "Evaluating\n",
      "Epoch 4501, Loss: 3.635570350904668\n",
      "Epoch 4501, Loss: 3.0635511013394026\n",
      "Epoch 4511, Loss: 3.872912700170214\n",
      "Epoch 4521, Loss: 2.696987540936538\n",
      "Epoch 4531, Loss: 4.977608862304661\n",
      "Epoch 4541, Loss: 3.370679884701367\n",
      "Epoch 4551, Loss: 3.5730886038744583\n",
      "Epoch 4561, Loss: 5.965695927735611\n",
      "Epoch 4571, Loss: 3.8393908792221563\n",
      "Epoch 4581, Loss: 4.529443190482396\n",
      "Epoch 4591, Loss: 3.9475730397401305\n",
      "Evaluating\n",
      "Epoch 4601, Loss: 3.7347417929369535\n",
      "Epoch 4601, Loss: 2.8160714250616063\n",
      "Epoch 4611, Loss: 2.959206090572632\n",
      "Epoch 4621, Loss: 4.28813138844814\n",
      "Epoch 4631, Loss: 2.7880937689295147\n",
      "Epoch 4641, Loss: 5.594090181872118\n",
      "Epoch 4651, Loss: 5.458718309178687\n",
      "Epoch 4661, Loss: 4.085591587513065\n",
      "Epoch 4671, Loss: 3.837612402384934\n",
      "Epoch 4681, Loss: 3.8726514557425613\n",
      "Epoch 4691, Loss: 3.635188745513467\n",
      "Evaluating\n",
      "Epoch 4701, Loss: 3.4667613910005115\n",
      "Epoch 4701, Loss: 5.379312755316775\n",
      "Epoch 4711, Loss: 3.3848502368508417\n",
      "Epoch 4721, Loss: 4.459921085498271\n",
      "Epoch 4731, Loss: 3.749040591748191\n",
      "Epoch 4741, Loss: 3.6487526675869844\n",
      "Epoch 4751, Loss: 3.2292696711842273\n",
      "Epoch 4761, Loss: 3.3638832813609043\n",
      "Epoch 4771, Loss: 4.000128500617278\n",
      "Epoch 4781, Loss: 3.777237019603512\n",
      "Epoch 4791, Loss: 4.8803229528032706\n",
      "Evaluating\n",
      "Epoch 4801, Loss: 3.5068388233034926\n",
      "Epoch 4801, Loss: 2.443177837331759\n",
      "Epoch 4811, Loss: 4.299401527691853\n",
      "Epoch 4821, Loss: 3.1002247131595904\n",
      "Epoch 4831, Loss: 4.800005736962767\n",
      "Epoch 4841, Loss: 1.9077493969851234\n",
      "Epoch 4851, Loss: 2.4307116017337203\n",
      "Epoch 4861, Loss: 3.290923808674862\n",
      "Epoch 4871, Loss: 4.989839364760676\n",
      "Epoch 4881, Loss: 4.7508299843505695\n",
      "Epoch 4891, Loss: 3.4177149302181076\n",
      "Evaluating\n",
      "Epoch 4901, Loss: 3.648351248877273\n",
      "Epoch 4901, Loss: 4.322971514654257\n",
      "Epoch 4911, Loss: 4.438286451893938\n",
      "Epoch 4921, Loss: 3.0626945867379454\n",
      "Epoch 4931, Loss: 4.198746232998636\n",
      "Epoch 4941, Loss: 3.579721783270444\n",
      "Epoch 4951, Loss: 2.578592808241567\n",
      "Epoch 4961, Loss: 4.077602807772455\n",
      "Epoch 4971, Loss: 2.861684194766605\n",
      "Epoch 4981, Loss: 2.2032760220826373\n",
      "Epoch 4991, Loss: 5.383241842693335\n",
      "Evaluating\n",
      "Epoch 5001, Loss: 3.865193804224202\n",
      "Epoch 5001, Loss: 4.000430198315075\n",
      "Epoch 5011, Loss: 3.844349228545391\n",
      "Epoch 5021, Loss: 3.9112335049863938\n",
      "Epoch 5031, Loss: 2.165631880157442\n",
      "Epoch 5041, Loss: 3.5812905184037476\n",
      "Epoch 5051, Loss: 4.214514436355899\n",
      "Epoch 5061, Loss: 3.0018903852572554\n",
      "Epoch 5071, Loss: 4.9835545509903945\n",
      "Epoch 5081, Loss: 3.20224989354127\n",
      "Epoch 5091, Loss: 4.415907405393808\n",
      "Evaluating\n",
      "Epoch 5101, Loss: 3.3980203018173025\n",
      "Epoch 5101, Loss: 5.980770202797218\n",
      "Epoch 5111, Loss: 3.664394596274848\n",
      "Epoch 5121, Loss: 3.959272333580088\n",
      "Epoch 5131, Loss: 2.8182542047267862\n",
      "Epoch 5141, Loss: 4.6599533734101035\n",
      "Epoch 5151, Loss: 3.6583881960535765\n",
      "Epoch 5161, Loss: 4.132620262790086\n",
      "Epoch 5171, Loss: 4.867475870185898\n",
      "Epoch 5181, Loss: 2.7921304588153735\n",
      "Epoch 5191, Loss: 3.181589517781676\n",
      "Evaluating\n",
      "Epoch 5201, Loss: 3.520197395413634\n",
      "Epoch 5201, Loss: 2.3342385170595117\n",
      "Epoch 5211, Loss: 4.310544490600291\n",
      "Epoch 5221, Loss: 6.086183579345918\n",
      "Epoch 5231, Loss: 3.5399049642423406\n",
      "Epoch 5241, Loss: 4.441017788843601\n",
      "Epoch 5251, Loss: 4.424061909436553\n",
      "Epoch 5261, Loss: 2.181087049227306\n",
      "Epoch 5271, Loss: 4.112621465110753\n",
      "Epoch 5281, Loss: 3.133776750862126\n",
      "Epoch 5291, Loss: 3.799153723887563\n",
      "Evaluating\n",
      "Epoch 5301, Loss: 3.6335816743369196\n",
      "Epoch 5301, Loss: 2.8472176466615404\n",
      "Epoch 5311, Loss: 2.727452436153695\n",
      "Epoch 5321, Loss: 2.1265345298764404\n",
      "Epoch 5331, Loss: 3.3246216326803837\n",
      "Epoch 5341, Loss: 2.2825550569345094\n",
      "Epoch 5351, Loss: 4.4540634955040375\n",
      "Epoch 5361, Loss: 3.3432947265447854\n",
      "Epoch 5371, Loss: 5.461730201177732\n",
      "Epoch 5381, Loss: 4.269129238424251\n",
      "Epoch 5391, Loss: 4.7685057456546875\n",
      "Evaluating\n",
      "Epoch 5401, Loss: 3.181985603411896\n",
      "Epoch 5401, Loss: 3.2985784996017844\n",
      "Epoch 5411, Loss: 4.9791900059590635\n",
      "Epoch 5421, Loss: 4.263395705026646\n",
      "Epoch 5431, Loss: 3.837273341267344\n",
      "Epoch 5441, Loss: 3.3198045339224027\n",
      "Epoch 5451, Loss: 4.539556553569733\n",
      "Epoch 5461, Loss: 2.7215722158469475\n",
      "Epoch 5471, Loss: 5.353424875275602\n",
      "Epoch 5481, Loss: 3.990167430081108\n",
      "Epoch 5491, Loss: 3.4686925467174263\n",
      "Evaluating\n",
      "Epoch 5501, Loss: 3.6799120102396965\n",
      "Epoch 5501, Loss: 3.3211016165332166\n",
      "Epoch 5511, Loss: 2.679507724629946\n",
      "Epoch 5521, Loss: 4.425009693447008\n",
      "Epoch 5531, Loss: 2.3329466057836745\n",
      "Epoch 5541, Loss: 3.532518807568829\n",
      "Epoch 5551, Loss: 4.3544806970949\n",
      "Epoch 5561, Loss: 4.583333772657798\n",
      "Epoch 5571, Loss: 3.6230278952387978\n",
      "Epoch 5581, Loss: 4.133243844174388\n",
      "Epoch 5591, Loss: 4.367097837564005\n",
      "Evaluating\n",
      "Epoch 5601, Loss: 3.6885088467841314\n",
      "Epoch 5601, Loss: 2.9416825140320504\n",
      "Epoch 5611, Loss: 5.059477018749488\n",
      "Epoch 5621, Loss: 3.175600103743572\n",
      "Epoch 5631, Loss: 4.142500090786518\n",
      "Epoch 5641, Loss: 2.640886063421669\n",
      "Epoch 5651, Loss: 3.5968845574387474\n",
      "Epoch 5661, Loss: 5.259641887272262\n",
      "Epoch 5671, Loss: 2.4572205132295606\n",
      "Epoch 5681, Loss: 2.184816266040517\n",
      "Epoch 5691, Loss: 3.293018908170442\n",
      "Evaluating\n",
      "Epoch 5701, Loss: 3.137772596959086\n",
      "Epoch 5701, Loss: 3.1297908605622724\n",
      "Epoch 5711, Loss: 3.4187262130225995\n",
      "Epoch 5721, Loss: 2.8111713273186716\n",
      "Epoch 5731, Loss: 1.6291851072606276\n",
      "Epoch 5741, Loss: 4.313355693775257\n",
      "Epoch 5751, Loss: 4.171154746308907\n",
      "Epoch 5761, Loss: 3.991807660082716\n",
      "Epoch 5771, Loss: 4.2879157054853545\n",
      "Epoch 5781, Loss: 3.4567399924408324\n",
      "Epoch 5791, Loss: 3.5939392963865706\n",
      "Evaluating\n",
      "Epoch 5801, Loss: 6.162103602491525\n",
      "Epoch 5801, Loss: 7.31095234593945\n",
      "Epoch 5811, Loss: 4.308195862215727\n",
      "Epoch 5821, Loss: 2.759636126875342\n",
      "Epoch 5831, Loss: 3.722187708742519\n",
      "Epoch 5841, Loss: 5.428678390000012\n",
      "Epoch 5851, Loss: 3.3401034858791356\n",
      "Epoch 5861, Loss: 2.805476953684278\n",
      "Epoch 5871, Loss: 3.218447684215234\n",
      "Epoch 5881, Loss: 3.76974316433532\n",
      "Epoch 5891, Loss: 4.4126135499237575\n",
      "Evaluating\n",
      "Epoch 5901, Loss: 3.6969096227100273\n",
      "Epoch 5901, Loss: 4.240341329102801\n",
      "Epoch 5911, Loss: 3.8305898133114673\n",
      "Epoch 5921, Loss: 5.273310499734306\n",
      "Epoch 5931, Loss: 5.372935873381954\n",
      "Epoch 5941, Loss: 3.7265612655877134\n",
      "Epoch 5951, Loss: 4.864117369636942\n",
      "Epoch 5961, Loss: 3.37421550367728\n",
      "Epoch 5971, Loss: 3.4544140100530782\n",
      "Epoch 5981, Loss: 3.3435291705533556\n",
      "Epoch 5991, Loss: 2.2526062255055774\n",
      "Evaluating\n",
      "Epoch 6001, Loss: 3.528671670713226\n",
      "Epoch 6001, Loss: 4.269721328454823\n",
      "Epoch 6011, Loss: 4.208031283197753\n",
      "Epoch 6021, Loss: 2.8099501759837464\n",
      "Epoch 6031, Loss: 5.3117132275489665\n",
      "Epoch 6041, Loss: 4.180748546370994\n",
      "Epoch 6051, Loss: 4.3767375811771085\n",
      "Epoch 6061, Loss: 3.5808408215380205\n",
      "Epoch 6071, Loss: 3.0091110326347876\n",
      "Epoch 6081, Loss: 2.5391173767463933\n",
      "Epoch 6091, Loss: 4.250788973858005\n",
      "Evaluating\n",
      "Epoch 6101, Loss: 3.4896724025624266\n",
      "Epoch 6101, Loss: 4.790005077009584\n",
      "Epoch 6111, Loss: 3.7035232368203066\n",
      "Epoch 6121, Loss: 4.30471901166877\n",
      "Epoch 6131, Loss: 4.271843761294542\n",
      "Epoch 6141, Loss: 3.3575265969976944\n",
      "Epoch 6151, Loss: 5.036597874299621\n",
      "Epoch 6161, Loss: 3.3655080995887765\n",
      "Epoch 6171, Loss: 4.758797416138887\n",
      "Epoch 6181, Loss: 4.202613364845014\n",
      "Epoch 6191, Loss: 4.734220816549127\n",
      "Evaluating\n",
      "Epoch 6201, Loss: 3.604815815953595\n",
      "Epoch 6201, Loss: 4.019832435782512\n",
      "Epoch 6211, Loss: 3.49627818821874\n",
      "Epoch 6221, Loss: 2.9343700982761645\n",
      "Epoch 6231, Loss: 2.221324428418376\n",
      "Epoch 6241, Loss: 3.542640118648774\n",
      "Epoch 6251, Loss: 4.979345282980214\n",
      "Epoch 6261, Loss: 7.349222318596208\n",
      "Epoch 6271, Loss: 3.987405375652102\n",
      "Epoch 6281, Loss: 4.44512357450709\n",
      "Epoch 6291, Loss: 3.9851591732398863\n",
      "Evaluating\n",
      "Epoch 6301, Loss: 3.4774889560856\n",
      "Epoch 6301, Loss: 3.251319596892291\n",
      "Epoch 6311, Loss: 4.165430427802219\n",
      "Epoch 6321, Loss: 2.73822805551127\n",
      "Epoch 6331, Loss: 3.6737527967217263\n",
      "Epoch 6341, Loss: 3.7296521418514184\n",
      "Epoch 6351, Loss: 4.660072990187302\n",
      "Epoch 6361, Loss: 6.1874808714162794\n",
      "Epoch 6371, Loss: 4.163179637083953\n",
      "Epoch 6381, Loss: 3.94215766085156\n",
      "Epoch 6391, Loss: 3.4663642819771328\n",
      "Evaluating\n",
      "Epoch 6401, Loss: 3.3825096503804204\n",
      "Epoch 6401, Loss: 2.5429640908314837\n",
      "Epoch 6411, Loss: 4.4158573803912935\n",
      "Epoch 6421, Loss: 5.127083472325322\n",
      "Epoch 6431, Loss: 4.450576180244388\n",
      "Epoch 6441, Loss: 3.6053244073329735\n",
      "Epoch 6451, Loss: 2.425969749523749\n",
      "Epoch 6461, Loss: 3.702289528710264\n",
      "Epoch 6471, Loss: 2.990973430807236\n",
      "Epoch 6481, Loss: 3.663819843307168\n",
      "Epoch 6491, Loss: 4.495418066785396\n",
      "Evaluating\n",
      "Epoch 6501, Loss: 3.2735320943573836\n",
      "Epoch 6501, Loss: 2.6085510096315074\n",
      "Epoch 6511, Loss: 3.3320921793130105\n",
      "Epoch 6521, Loss: 3.5221783827564255\n",
      "Epoch 6531, Loss: 2.8288314085033806\n",
      "Epoch 6541, Loss: 4.040617131783407\n",
      "Epoch 6551, Loss: 2.1676477439225126\n",
      "Epoch 6561, Loss: 2.6998061563290108\n",
      "Epoch 6571, Loss: 3.8331938162435892\n",
      "Epoch 6581, Loss: 5.202458102084833\n",
      "Epoch 6591, Loss: 3.811449477705756\n",
      "Evaluating\n",
      "Epoch 6601, Loss: 3.4052982613531366\n",
      "Epoch 6601, Loss: 2.433423783315151\n",
      "Epoch 6611, Loss: 3.3835636641598543\n",
      "Epoch 6621, Loss: 4.282838355351029\n",
      "Epoch 6631, Loss: 2.619629094289511\n",
      "Epoch 6641, Loss: 3.808572702267911\n",
      "Epoch 6651, Loss: 4.7468792587457225\n",
      "Epoch 6661, Loss: 4.008227972758481\n",
      "Epoch 6671, Loss: 3.5879514273933624\n",
      "Epoch 6681, Loss: 4.421876322812898\n",
      "Epoch 6691, Loss: 4.127161512419883\n",
      "Evaluating\n",
      "Epoch 6701, Loss: 3.2238158812512467\n",
      "Epoch 6701, Loss: 2.4645767455224985\n",
      "Epoch 6711, Loss: 2.7748787444739142\n",
      "Epoch 6721, Loss: 2.5942892165210885\n",
      "Epoch 6731, Loss: 2.499251220983793\n",
      "Epoch 6741, Loss: 2.3346000010958345\n",
      "Epoch 6751, Loss: 2.309797058005943\n",
      "Epoch 6761, Loss: 2.8646197878243873\n",
      "Epoch 6771, Loss: 3.4922609313988158\n",
      "Epoch 6781, Loss: 5.689607132125228\n",
      "Epoch 6791, Loss: 3.3128866663349608\n",
      "Evaluating\n",
      "Epoch 6801, Loss: 3.642820534967479\n",
      "Epoch 6801, Loss: 4.978273625037814\n",
      "Epoch 6811, Loss: 2.6380043551596457\n",
      "Epoch 6821, Loss: 3.6597340872761284\n",
      "Epoch 6831, Loss: 3.74210503411264\n",
      "Epoch 6841, Loss: 3.7513783183380034\n",
      "Epoch 6851, Loss: 2.402248215088435\n",
      "Epoch 6861, Loss: 3.9631011277174055\n",
      "Epoch 6871, Loss: 4.933964074196461\n",
      "Epoch 6881, Loss: 4.168156306786477\n",
      "Epoch 6891, Loss: 4.123462556242552\n",
      "Evaluating\n",
      "Epoch 6901, Loss: 3.3507046320463725\n",
      "Epoch 6901, Loss: 3.022253039215176\n",
      "Epoch 6911, Loss: 4.071848964559423\n",
      "Epoch 6921, Loss: 3.1540305833767466\n",
      "Epoch 6931, Loss: 4.687666345211703\n",
      "Epoch 6941, Loss: 2.8248347827676197\n",
      "Epoch 6951, Loss: 4.910221248931838\n",
      "Epoch 6961, Loss: 3.9240758167552316\n",
      "Epoch 6971, Loss: 3.5835899370453617\n",
      "Epoch 6981, Loss: 5.4167865162414826\n",
      "Epoch 6991, Loss: 6.192865699758249\n",
      "Evaluating\n",
      "Epoch 7001, Loss: 3.3552151220917894\n",
      "Epoch 7001, Loss: 3.9842817761052425\n",
      "Epoch 7011, Loss: 3.477954980368312\n",
      "Epoch 7021, Loss: 2.835466905193134\n",
      "Epoch 7031, Loss: 3.70675271491291\n",
      "Epoch 7041, Loss: 3.14013931359231\n",
      "Epoch 7051, Loss: 3.3348497271379176\n",
      "Epoch 7061, Loss: 4.9298268788762005\n",
      "Epoch 7071, Loss: 2.2237838054081185\n",
      "Epoch 7081, Loss: 3.2664196366534033\n",
      "Epoch 7091, Loss: 4.640757366021405\n",
      "Evaluating\n",
      "Epoch 7101, Loss: 3.3510288711711\n",
      "Epoch 7101, Loss: 3.77167039523419\n",
      "Epoch 7111, Loss: 2.7477112972409494\n",
      "Epoch 7121, Loss: 3.4347273280423565\n",
      "Epoch 7131, Loss: 4.310252882594944\n",
      "Epoch 7141, Loss: 3.5407976461182704\n",
      "Epoch 7151, Loss: 4.267509324058685\n",
      "Epoch 7161, Loss: 3.557113290354511\n",
      "Epoch 7171, Loss: 3.0723006105463577\n",
      "Epoch 7181, Loss: 3.038146788258257\n",
      "Epoch 7191, Loss: 3.602847360835074\n",
      "Evaluating\n",
      "Epoch 7201, Loss: 3.313686845342492\n",
      "Epoch 7201, Loss: 2.8321616949756057\n",
      "Epoch 7211, Loss: 2.764237876299873\n",
      "Epoch 7221, Loss: 3.744292519999776\n",
      "Epoch 7231, Loss: 3.9212503085910955\n",
      "Epoch 7241, Loss: 2.5620522579695164\n",
      "Epoch 7251, Loss: 3.513493231219013\n",
      "Epoch 7261, Loss: 3.8488514323472613\n",
      "Epoch 7271, Loss: 3.341036769455501\n",
      "Epoch 7281, Loss: 4.098254966462569\n",
      "Epoch 7291, Loss: 3.823630395319226\n",
      "Evaluating\n",
      "Epoch 7301, Loss: 3.494424651664578\n",
      "Epoch 7301, Loss: 3.6852837616712013\n",
      "Epoch 7311, Loss: 2.90319607673929\n",
      "Epoch 7321, Loss: 2.4495813589282207\n",
      "Epoch 7331, Loss: 4.558673287995317\n",
      "Epoch 7341, Loss: 3.5989072206060575\n",
      "Epoch 7351, Loss: 4.199261448453959\n",
      "Epoch 7361, Loss: 3.16479224621019\n",
      "Epoch 7371, Loss: 4.107979226062593\n",
      "Epoch 7381, Loss: 2.9910071586780833\n",
      "Epoch 7391, Loss: 4.470121894608397\n",
      "Evaluating\n",
      "Epoch 7401, Loss: 3.2680237395954785\n",
      "Epoch 7401, Loss: 4.2108313595871\n",
      "Epoch 7411, Loss: 2.8725368894855396\n",
      "Epoch 7421, Loss: 3.4170963158174876\n",
      "Epoch 7431, Loss: 3.488366124712239\n",
      "Epoch 7441, Loss: 4.542562878232445\n",
      "Epoch 7451, Loss: 3.273795038698653\n",
      "Epoch 7461, Loss: 5.353557744190185\n",
      "Epoch 7471, Loss: 3.096047700094205\n",
      "Epoch 7481, Loss: 2.980597595363345\n",
      "Epoch 7491, Loss: 3.837397686344379\n",
      "Evaluating\n",
      "Epoch 7501, Loss: 3.111023860284366\n",
      "Epoch 7501, Loss: 3.5394023676706983\n",
      "Epoch 7511, Loss: 4.534861960547183\n",
      "Epoch 7521, Loss: 6.478187402568902\n",
      "Epoch 7531, Loss: 1.8825563646230017\n",
      "Epoch 7541, Loss: 3.7931790109736485\n",
      "Epoch 7551, Loss: 3.459660515912356\n",
      "Epoch 7561, Loss: 2.998959520057583\n",
      "Epoch 7571, Loss: 4.899827303233696\n",
      "Epoch 7581, Loss: 3.8724213787337205\n",
      "Epoch 7591, Loss: 3.671034695136227\n",
      "Evaluating\n",
      "Epoch 7601, Loss: 3.1983103140852034\n",
      "Epoch 7601, Loss: 2.7436877559434154\n",
      "Epoch 7611, Loss: 2.8012243708097486\n",
      "Epoch 7621, Loss: 3.7605367064743955\n",
      "Epoch 7631, Loss: 4.695819982802749\n",
      "Epoch 7641, Loss: 3.661574629277321\n",
      "Epoch 7651, Loss: 5.132395785331952\n",
      "Epoch 7661, Loss: 3.2080361163181306\n",
      "Epoch 7671, Loss: 3.21210922374309\n",
      "Epoch 7681, Loss: 2.077905416769657\n",
      "Epoch 7691, Loss: 2.786141484374969\n",
      "Evaluating\n",
      "Epoch 7701, Loss: 3.276252962973835\n",
      "Epoch 7701, Loss: 4.173400353716518\n",
      "Epoch 7711, Loss: 2.1719003443956053\n",
      "Epoch 7721, Loss: 4.059703618071944\n",
      "Epoch 7731, Loss: 2.0400811597778756\n",
      "Epoch 7741, Loss: 2.3771666269283047\n",
      "Epoch 7751, Loss: 2.3398110904994565\n",
      "Epoch 7761, Loss: 2.5522392539183274\n",
      "Epoch 7771, Loss: 2.6184434744754124\n",
      "Epoch 7781, Loss: 3.0477241185867707\n",
      "Epoch 7791, Loss: 2.8902928279734037\n",
      "Evaluating\n",
      "Epoch 7801, Loss: 3.2541688985153385\n",
      "Epoch 7801, Loss: 3.5297113491202055\n",
      "Epoch 7811, Loss: 3.1800038714709027\n",
      "Epoch 7821, Loss: 2.878081317180379\n",
      "Epoch 7831, Loss: 3.6971631694555613\n",
      "Epoch 7841, Loss: 3.669586598925775\n",
      "Epoch 7851, Loss: 4.261444625874221\n",
      "Epoch 7861, Loss: 3.622886639872449\n",
      "Epoch 7871, Loss: 3.2221582338857133\n",
      "Epoch 7881, Loss: 3.863163222872858\n",
      "Epoch 7891, Loss: 2.913365211634351\n",
      "Evaluating\n",
      "Epoch 7901, Loss: 3.204359194441182\n",
      "Epoch 7901, Loss: 2.856555081985893\n",
      "Epoch 7911, Loss: 4.314858906406985\n",
      "Epoch 7921, Loss: 2.509587187313271\n",
      "Epoch 7931, Loss: 2.710813237038221\n",
      "Epoch 7941, Loss: 2.6028961241552886\n",
      "Epoch 7951, Loss: 3.100697212317364\n",
      "Epoch 7961, Loss: 3.0985447927445584\n",
      "Epoch 7971, Loss: 4.134118303127321\n",
      "Epoch 7981, Loss: 2.1868278298561132\n",
      "Epoch 7991, Loss: 3.2717916080167124\n",
      "Evaluating\n",
      "Epoch 8001, Loss: 3.1526874200565316\n",
      "Epoch 8001, Loss: 5.285159058987572\n",
      "Epoch 8011, Loss: 2.520441019361244\n",
      "Epoch 8021, Loss: 3.02984902890884\n",
      "Epoch 8031, Loss: 4.139529457384679\n",
      "Epoch 8041, Loss: 4.442641398289373\n",
      "Epoch 8051, Loss: 2.534843794880759\n",
      "Epoch 8061, Loss: 3.8768283043973986\n",
      "Epoch 8071, Loss: 3.529346672404192\n",
      "Epoch 8081, Loss: 3.628848315120912\n",
      "Epoch 8091, Loss: 3.6498303663471936\n",
      "Evaluating\n",
      "Epoch 8101, Loss: 2.9032740680611635\n",
      "Epoch 8101, Loss: 4.048319856938097\n",
      "Epoch 8111, Loss: 2.5767026937528814\n",
      "Epoch 8121, Loss: 1.8991591359267577\n",
      "Epoch 8131, Loss: 4.457979211098734\n",
      "Epoch 8141, Loss: 2.999038826695518\n",
      "Epoch 8151, Loss: 3.3353889975013473\n",
      "Epoch 8161, Loss: 2.0044819221679817\n",
      "Epoch 8171, Loss: 2.4322860526132324\n",
      "Epoch 8181, Loss: 2.7229029101739672\n",
      "Epoch 8191, Loss: 2.1162602108283615\n",
      "Evaluating\n",
      "Epoch 8201, Loss: 3.279435496194158\n",
      "Epoch 8201, Loss: 4.9452676034431\n",
      "Epoch 8211, Loss: 3.908698636490051\n",
      "Epoch 8221, Loss: 4.961374200309423\n",
      "Epoch 8231, Loss: 4.024667831476668\n",
      "Epoch 8241, Loss: 2.6128002763844274\n",
      "Epoch 8251, Loss: 2.3606085698219372\n",
      "Epoch 8261, Loss: 4.426447208303962\n",
      "Epoch 8271, Loss: 3.803328618497844\n",
      "Epoch 8281, Loss: 3.9460433680578\n",
      "Epoch 8291, Loss: 3.8709633995177395\n",
      "Evaluating\n",
      "Epoch 8301, Loss: 3.156286543550264\n",
      "Epoch 8301, Loss: 2.5063020260234063\n",
      "Epoch 8311, Loss: 2.214446452717281\n",
      "Epoch 8321, Loss: 2.813970558886685\n",
      "Epoch 8331, Loss: 2.788105653611773\n",
      "Epoch 8341, Loss: 2.5432322120764788\n",
      "Epoch 8351, Loss: 2.0679929756189286\n",
      "Epoch 8361, Loss: 4.279250736650667\n",
      "Epoch 8371, Loss: 3.598078686323626\n",
      "Epoch 8381, Loss: 1.7708560706274328\n",
      "Epoch 8391, Loss: 3.4889456156906866\n",
      "Evaluating\n",
      "Epoch 8401, Loss: 3.1316045082809394\n",
      "Epoch 8401, Loss: 2.7889686102598072\n",
      "Epoch 8411, Loss: 2.9898539174275447\n",
      "Epoch 8421, Loss: 1.8713584753823307\n",
      "Epoch 8431, Loss: 2.66825105449731\n",
      "Epoch 8441, Loss: 4.791833145481613\n",
      "Epoch 8451, Loss: 3.1837919525844645\n",
      "Epoch 8461, Loss: 1.465909158722354\n",
      "Epoch 8471, Loss: 4.021728256556114\n",
      "Epoch 8481, Loss: 3.3944004715514464\n",
      "Epoch 8491, Loss: 3.4541188752067127\n",
      "Evaluating\n",
      "Epoch 8501, Loss: 3.361795109326201\n",
      "Epoch 8501, Loss: 4.6616394179689165\n",
      "Epoch 8511, Loss: 2.9895264670331985\n",
      "Epoch 8521, Loss: 2.5190570140009347\n",
      "Epoch 8531, Loss: 5.787572977091618\n",
      "Epoch 8541, Loss: 4.505538836183696\n",
      "Epoch 8551, Loss: 3.1607535635318267\n",
      "Epoch 8561, Loss: 2.921605792810115\n",
      "Epoch 8571, Loss: 2.166946321227465\n",
      "Epoch 8581, Loss: 3.564635021389172\n",
      "Epoch 8591, Loss: 2.5499442732284776\n",
      "Evaluating\n",
      "Epoch 8601, Loss: 3.196543613230246\n",
      "Epoch 8601, Loss: 4.599172970512937\n",
      "Epoch 8611, Loss: 3.240809146114607\n",
      "Epoch 8621, Loss: 3.2284062598979575\n",
      "Epoch 8631, Loss: 2.5068142118255654\n",
      "Epoch 8641, Loss: 3.4406243472205915\n",
      "Epoch 8651, Loss: 2.4069600132084212\n",
      "Epoch 8661, Loss: 3.0406701203956312\n",
      "Epoch 8671, Loss: 3.3321251061852233\n",
      "Epoch 8681, Loss: 3.19570788143287\n",
      "Epoch 8691, Loss: 2.4646819112944782\n",
      "Evaluating\n",
      "Epoch 8701, Loss: 3.0007252536261015\n",
      "Epoch 8701, Loss: 2.912435833865633\n",
      "Epoch 8711, Loss: 2.9434699738533627\n",
      "Epoch 8721, Loss: 3.558795034142411\n",
      "Epoch 8731, Loss: 2.696849249589479\n",
      "Epoch 8741, Loss: 2.360264250198755\n",
      "Epoch 8751, Loss: 3.016212105997823\n",
      "Epoch 8761, Loss: 2.687530531345199\n",
      "Epoch 8771, Loss: 2.77918184136934\n",
      "Epoch 8781, Loss: 2.784992512393453\n",
      "Epoch 8791, Loss: 4.275043692659882\n",
      "Evaluating\n",
      "Epoch 8801, Loss: 3.3351735206021016\n",
      "Epoch 8801, Loss: 4.68032250283948\n",
      "Epoch 8811, Loss: 2.9249263640020304\n",
      "Epoch 8821, Loss: 3.4514136298755433\n",
      "Epoch 8831, Loss: 2.730268188865731\n",
      "Epoch 8841, Loss: 2.453742361413952\n",
      "Epoch 8851, Loss: 3.3764450576137555\n",
      "Epoch 8861, Loss: 2.7942437927566792\n",
      "Epoch 8871, Loss: 3.0602403064065586\n",
      "Epoch 8881, Loss: 3.4297665589206483\n",
      "Epoch 8891, Loss: 3.920655739081421\n",
      "Evaluating\n",
      "Epoch 8901, Loss: 3.268286451198722\n",
      "Epoch 8901, Loss: 3.4213028832865833\n",
      "Epoch 8911, Loss: 2.786054135268291\n",
      "Epoch 8921, Loss: 2.7754482863721517\n",
      "Epoch 8931, Loss: 2.3281122969591364\n",
      "Epoch 8941, Loss: 2.732777386731547\n",
      "Epoch 8951, Loss: 3.425325648126416\n",
      "Epoch 8961, Loss: 2.978424763992249\n",
      "Epoch 8971, Loss: 2.472721427590397\n",
      "Epoch 8981, Loss: 2.730928060628693\n",
      "Epoch 8991, Loss: 2.649297959979972\n",
      "Evaluating\n",
      "Epoch 9001, Loss: 3.003669840346481\n",
      "Epoch 9001, Loss: 4.201188824804639\n",
      "Epoch 9011, Loss: 2.1760179101441324\n",
      "Epoch 9021, Loss: 2.5661042045873925\n",
      "Epoch 9031, Loss: 2.6761312377581343\n",
      "Epoch 9041, Loss: 2.4138308346399473\n",
      "Epoch 9051, Loss: 2.876824450277383\n",
      "Epoch 9061, Loss: 4.318519420350965\n",
      "Epoch 9071, Loss: 2.915024790509631\n",
      "Epoch 9081, Loss: 3.4290557425859354\n",
      "Epoch 9091, Loss: 1.621506797768509\n",
      "Evaluating\n",
      "Epoch 9101, Loss: 2.9803386786042916\n",
      "Epoch 9101, Loss: 2.7146169724456173\n",
      "Epoch 9111, Loss: 3.4413432586363246\n",
      "Epoch 9121, Loss: 3.5115957299018965\n",
      "Epoch 9131, Loss: 1.8404123754029655\n",
      "Epoch 9141, Loss: 3.776171892189274\n",
      "Epoch 9151, Loss: 2.3145214491983594\n",
      "Epoch 9161, Loss: 3.564952712655903\n",
      "Epoch 9171, Loss: 2.923777783646251\n",
      "Epoch 9181, Loss: 3.4735922130908623\n",
      "Epoch 9191, Loss: 4.479599939574985\n",
      "Evaluating\n",
      "Epoch 9201, Loss: 3.0253857508917648\n",
      "Epoch 9201, Loss: 2.2164316235957737\n",
      "Epoch 9211, Loss: 2.720742687919976\n",
      "Epoch 9221, Loss: 2.5921037648280176\n",
      "Epoch 9231, Loss: 4.277139898685965\n",
      "Epoch 9241, Loss: 3.3251097462111474\n",
      "Epoch 9251, Loss: 4.769759155281001\n",
      "Epoch 9261, Loss: 4.4564819751703375\n",
      "Epoch 9271, Loss: 2.25773896487768\n",
      "Epoch 9281, Loss: 3.841623215894946\n",
      "Epoch 9291, Loss: 2.6148445286444173\n",
      "Evaluating\n",
      "Epoch 9301, Loss: 3.098843971222363\n",
      "Epoch 9301, Loss: 2.622074971104726\n",
      "Epoch 9311, Loss: 2.9278646916010787\n",
      "Epoch 9321, Loss: 4.159754346709424\n",
      "Epoch 9331, Loss: 3.847668723920872\n",
      "Epoch 9341, Loss: 2.0345074785620096\n",
      "Epoch 9351, Loss: 5.425404941366118\n",
      "Epoch 9361, Loss: 3.8274342365610714\n",
      "Epoch 9371, Loss: 3.662547429472785\n",
      "Epoch 9381, Loss: 2.961826387183895\n",
      "Epoch 9391, Loss: 1.5917746651413767\n",
      "Evaluating\n",
      "Epoch 9401, Loss: 2.7357636924310547\n",
      "Epoch 9401, Loss: 4.32608562897682\n",
      "Epoch 9411, Loss: 2.447881768264285\n",
      "Epoch 9421, Loss: 3.4488320201726177\n",
      "Epoch 9431, Loss: 2.659241537360433\n",
      "Epoch 9441, Loss: 2.5969659547451496\n",
      "Epoch 9451, Loss: 3.9933231205511746\n",
      "Epoch 9461, Loss: 3.717232014028578\n",
      "Epoch 9471, Loss: 2.4531653667250395\n",
      "Epoch 9481, Loss: 2.843696422158078\n",
      "Epoch 9491, Loss: 2.8763684235980254\n",
      "Evaluating\n",
      "Epoch 9501, Loss: 3.0405397109414536\n",
      "Epoch 9501, Loss: 4.3611491745455515\n",
      "Epoch 9511, Loss: 2.8026997155860345\n",
      "Epoch 9521, Loss: 4.501806468587949\n",
      "Epoch 9531, Loss: 2.9881766331727215\n",
      "Epoch 9541, Loss: 3.550765129034287\n",
      "Epoch 9551, Loss: 3.448659075483942\n",
      "Epoch 9561, Loss: 3.9538714886558224\n",
      "Epoch 9571, Loss: 3.1943385598751957\n",
      "Epoch 9581, Loss: 3.6718914872101016\n",
      "Epoch 9591, Loss: 2.868912250703835\n",
      "Evaluating\n",
      "Epoch 9601, Loss: 2.9712195041927045\n",
      "Epoch 9601, Loss: 2.659729393354978\n",
      "Epoch 9611, Loss: 3.9688027912892614\n",
      "Epoch 9621, Loss: 3.9143420311887387\n",
      "Epoch 9631, Loss: 2.477678808644841\n",
      "Epoch 9641, Loss: 3.1531528750456896\n",
      "Epoch 9651, Loss: 2.3072250341278324\n",
      "Epoch 9661, Loss: 2.745832742501736\n",
      "Epoch 9671, Loss: 3.237711798567331\n",
      "Epoch 9681, Loss: 2.5633823137204814\n",
      "Epoch 9691, Loss: 2.8712993940198404\n",
      "Evaluating\n",
      "Epoch 9701, Loss: 2.695153819228517\n",
      "Epoch 9701, Loss: 3.65563067194088\n",
      "Epoch 9711, Loss: 3.2111886876037574\n",
      "Epoch 9721, Loss: 2.511474451118631\n",
      "Epoch 9731, Loss: 6.259189512399484\n",
      "Epoch 9741, Loss: 2.412520425121275\n",
      "Epoch 9751, Loss: 4.177366394261765\n",
      "Epoch 9761, Loss: 3.2023462399725133\n",
      "Epoch 9771, Loss: 4.277496253115272\n",
      "Epoch 9781, Loss: 2.550020507474333\n",
      "Epoch 9791, Loss: 2.3391058894216767\n",
      "Evaluating\n",
      "Epoch 9801, Loss: 3.039803525185476\n",
      "Epoch 9801, Loss: 1.4928114968656636\n",
      "Epoch 9811, Loss: 2.815590913215699\n",
      "Epoch 9821, Loss: 2.965145499358988\n",
      "Epoch 9831, Loss: 2.578865648411582\n",
      "Epoch 9841, Loss: 2.6655443801567222\n",
      "Epoch 9851, Loss: 3.4078500805437293\n",
      "Epoch 9861, Loss: 3.8721798782432395\n",
      "Epoch 9871, Loss: 3.316904278931649\n",
      "Epoch 9881, Loss: 3.4510133678669193\n",
      "Epoch 9891, Loss: 2.487634456570898\n",
      "Evaluating\n",
      "Epoch 9901, Loss: 3.256926404409016\n",
      "Epoch 9901, Loss: 4.840147849870443\n",
      "Epoch 9911, Loss: 4.893770562429996\n",
      "Epoch 9921, Loss: 3.5797316568522044\n",
      "Epoch 9931, Loss: 2.9939910623848345\n",
      "Epoch 9941, Loss: 2.775627098153571\n",
      "Epoch 9951, Loss: 2.812935570996872\n",
      "Epoch 9961, Loss: 3.132970107150597\n",
      "Epoch 9971, Loss: 3.7001866640307757\n",
      "Epoch 9981, Loss: 4.1127057949125225\n",
      "Epoch 9991, Loss: 3.3440775005228103\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "for epoch in range(10000):\n",
    "    # Zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    #random_batch = np.random.randint(5000,6000)\n",
    "    x_data,y_data = get_batch(frame_data_tensor,labels_tensor,batch_size=16)\n",
    "    outputs = model(x_data)\n",
    "    loss = criterion(outputs.squeeze(), y_data)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch%100==0:\n",
    "        print(\"Evaluating\")\n",
    "        eval_batch(test_input,test_output,epoch)\n",
    "    if epoch%10==0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {np.sqrt(loss.item())*max(labeldata)}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating\n",
      "Epoch 10001, Loss: 2.914099858637356\n",
      "Epoch 10001, Loss: 3.869545974511474\n",
      "Epoch 10011, Loss: 2.9406415184192345\n",
      "Epoch 10021, Loss: 3.012004517009309\n",
      "Epoch 10031, Loss: 2.6718178857404107\n",
      "Epoch 10041, Loss: 3.922567768240186\n",
      "Epoch 10051, Loss: 2.758993129692507\n",
      "Epoch 10061, Loss: 3.3290303261962784\n",
      "Epoch 10071, Loss: 3.1228345242233746\n",
      "Epoch 10081, Loss: 3.1759401330676496\n",
      "Epoch 10091, Loss: 4.230361201741598\n",
      "Evaluating\n",
      "Epoch 10101, Loss: 2.956982179994613\n",
      "Epoch 10101, Loss: 3.6579143599371413\n",
      "Epoch 10111, Loss: 1.4994506638924525\n",
      "Epoch 10121, Loss: 3.671458302724617\n",
      "Epoch 10131, Loss: 3.2058182956473513\n",
      "Epoch 10141, Loss: 3.3685366868647377\n",
      "Epoch 10151, Loss: 3.1881853289610924\n",
      "Epoch 10161, Loss: 2.808858130998592\n",
      "Epoch 10171, Loss: 5.206559122386152\n",
      "Epoch 10181, Loss: 3.1368856448432045\n",
      "Epoch 10191, Loss: 2.742755248513745\n",
      "Evaluating\n",
      "Epoch 10201, Loss: 2.7465320393515937\n",
      "Epoch 10201, Loss: 3.1620476582571126\n",
      "Epoch 10211, Loss: 2.7058251187585993\n",
      "Epoch 10221, Loss: 2.669770646345347\n",
      "Epoch 10231, Loss: 3.765537290851885\n",
      "Epoch 10241, Loss: 2.522184847504739\n",
      "Epoch 10251, Loss: 2.71718682909406\n",
      "Epoch 10261, Loss: 3.323118749765329\n",
      "Epoch 10271, Loss: 3.476209132463151\n",
      "Epoch 10281, Loss: 3.3241479567354117\n",
      "Epoch 10291, Loss: 4.909838530043043\n",
      "Evaluating\n",
      "Epoch 10301, Loss: 3.322895718445023\n",
      "Epoch 10301, Loss: 2.7829617186834144\n",
      "Epoch 10311, Loss: 2.283524823424805\n",
      "Epoch 10321, Loss: 2.6075530910847986\n",
      "Epoch 10331, Loss: 2.320525368680626\n",
      "Epoch 10341, Loss: 1.8878871345024135\n",
      "Epoch 10351, Loss: 4.127983567880855\n",
      "Epoch 10361, Loss: 1.923157540147256\n",
      "Epoch 10371, Loss: 3.716383495483413\n",
      "Epoch 10381, Loss: 4.675565301257391\n",
      "Epoch 10391, Loss: 2.8940260512608584\n",
      "Evaluating\n",
      "Epoch 10401, Loss: 2.9655325036851012\n",
      "Epoch 10401, Loss: 2.8231434433628735\n",
      "Epoch 10411, Loss: 2.774428520672965\n",
      "Epoch 10421, Loss: 4.12543801836879\n",
      "Epoch 10431, Loss: 3.4060489503595086\n",
      "Epoch 10441, Loss: 5.177805954159641\n",
      "Epoch 10451, Loss: 2.624057559508109\n",
      "Epoch 10461, Loss: 1.601447875060453\n",
      "Epoch 10471, Loss: 3.499086061199225\n",
      "Epoch 10481, Loss: 2.872767843268749\n",
      "Epoch 10491, Loss: 2.5824648362851725\n",
      "Evaluating\n",
      "Epoch 10501, Loss: 3.052673144120008\n",
      "Epoch 10501, Loss: 2.5733129276302216\n",
      "Epoch 10511, Loss: 3.203447694482368\n",
      "Epoch 10521, Loss: 2.192286569899546\n",
      "Epoch 10531, Loss: 3.417159130433616\n",
      "Epoch 10541, Loss: 3.274329072357141\n",
      "Epoch 10551, Loss: 2.0782741065882693\n",
      "Epoch 10561, Loss: 2.4730384777511936\n",
      "Epoch 10571, Loss: 3.7643154957523444\n",
      "Epoch 10581, Loss: 2.2701411051283853\n",
      "Epoch 10591, Loss: 3.993975272068466\n",
      "Evaluating\n",
      "Epoch 10601, Loss: 3.240271974106769\n",
      "Epoch 10601, Loss: 4.003624699486326\n",
      "Epoch 10611, Loss: 3.5475518139585915\n",
      "Epoch 10621, Loss: 3.4303663599818446\n",
      "Epoch 10631, Loss: 2.191311541091995\n",
      "Epoch 10641, Loss: 3.6670409140801223\n",
      "Epoch 10651, Loss: 4.6044367569055344\n",
      "Epoch 10661, Loss: 2.4756517929314015\n",
      "Epoch 10671, Loss: 3.0979118839836834\n",
      "Epoch 10681, Loss: 3.4707291336250936\n",
      "Epoch 10691, Loss: 3.537424989280267\n",
      "Evaluating\n",
      "Epoch 10701, Loss: 3.1463435623272966\n",
      "Epoch 10701, Loss: 3.545294031127939\n",
      "Epoch 10711, Loss: 4.205348585838751\n",
      "Epoch 10721, Loss: 2.61821821070062\n",
      "Epoch 10731, Loss: 3.6414562597431988\n",
      "Epoch 10741, Loss: 2.9319441446977144\n",
      "Epoch 10751, Loss: 2.974382942719625\n",
      "Epoch 10761, Loss: 2.0517908516067656\n",
      "Epoch 10771, Loss: 2.3908659652594757\n",
      "Epoch 10781, Loss: 4.226200621918891\n",
      "Epoch 10791, Loss: 2.313306301783146\n",
      "Evaluating\n",
      "Epoch 10801, Loss: 2.863705060475912\n",
      "Epoch 10801, Loss: 1.814304636580096\n",
      "Epoch 10811, Loss: 2.433331564920439\n",
      "Epoch 10821, Loss: 3.586586400920483\n",
      "Epoch 10831, Loss: 3.2782455810124245\n",
      "Epoch 10841, Loss: 3.2034890692365057\n",
      "Epoch 10851, Loss: 2.8421819932756867\n",
      "Epoch 10861, Loss: 3.1559329639453924\n",
      "Epoch 10871, Loss: 2.965470551623845\n",
      "Epoch 10881, Loss: 4.527565288304818\n",
      "Epoch 10891, Loss: 2.453995926043343\n",
      "Evaluating\n",
      "Epoch 10901, Loss: 2.8288699328299503\n",
      "Epoch 10901, Loss: 2.62374563802602\n",
      "Epoch 10911, Loss: 4.536912780796007\n",
      "Epoch 10921, Loss: 3.240368553474112\n",
      "Epoch 10931, Loss: 3.6137396558075303\n",
      "Epoch 10941, Loss: 4.807655661200387\n",
      "Epoch 10951, Loss: 2.9954504959698096\n",
      "Epoch 10961, Loss: 2.5797553551251315\n",
      "Epoch 10971, Loss: 3.0424289512981444\n",
      "Epoch 10981, Loss: 3.3772117536832456\n",
      "Epoch 10991, Loss: 3.205455937214414\n",
      "Evaluating\n",
      "Epoch 11001, Loss: 2.8787111435419437\n",
      "Epoch 11001, Loss: 3.0684609217999377\n",
      "Epoch 11011, Loss: 3.22010701302671\n",
      "Epoch 11021, Loss: 4.387172931285259\n",
      "Epoch 11031, Loss: 2.624581274086603\n",
      "Epoch 11041, Loss: 3.000890887407369\n",
      "Epoch 11051, Loss: 3.865529844854418\n",
      "Epoch 11061, Loss: 2.635143387235042\n",
      "Epoch 11071, Loss: 1.3908102224972152\n",
      "Epoch 11081, Loss: 3.5527699025021975\n",
      "Epoch 11091, Loss: 2.591049210892973\n",
      "Evaluating\n",
      "Epoch 11101, Loss: 3.0814524462065394\n",
      "Epoch 11101, Loss: 2.7745468889747755\n",
      "Epoch 11111, Loss: 1.7092428087624116\n",
      "Epoch 11121, Loss: 1.5774020919096412\n",
      "Epoch 11131, Loss: 4.006665037431977\n",
      "Epoch 11141, Loss: 2.040591646601031\n",
      "Epoch 11151, Loss: 3.7891276074650126\n",
      "Epoch 11161, Loss: 2.2968108390428337\n",
      "Epoch 11171, Loss: 2.821402677464831\n",
      "Epoch 11181, Loss: 2.8232228636780055\n",
      "Epoch 11191, Loss: 2.31358878462138\n",
      "Evaluating\n",
      "Epoch 11201, Loss: 2.824633408358606\n",
      "Epoch 11201, Loss: 3.1404514115454965\n",
      "Epoch 11211, Loss: 2.590485602239747\n",
      "Epoch 11221, Loss: 4.137194273305138\n",
      "Epoch 11231, Loss: 3.0039529747769365\n",
      "Epoch 11241, Loss: 2.7573501977918524\n",
      "Epoch 11251, Loss: 3.182840785821033\n",
      "Epoch 11261, Loss: 3.697623612092055\n",
      "Epoch 11271, Loss: 5.028453236316849\n",
      "Epoch 11281, Loss: 2.599024644687013\n",
      "Epoch 11291, Loss: 2.4386769584748382\n",
      "Evaluating\n",
      "Epoch 11301, Loss: 3.002391726185242\n",
      "Epoch 11301, Loss: 3.987653747135692\n",
      "Epoch 11311, Loss: 3.5151773051683177\n",
      "Epoch 11321, Loss: 2.61499011341423\n",
      "Epoch 11331, Loss: 3.3372502179498946\n",
      "Epoch 11341, Loss: 2.5434594849480696\n",
      "Epoch 11351, Loss: 2.6366329078777446\n",
      "Epoch 11361, Loss: 2.053739098209669\n",
      "Epoch 11371, Loss: 4.045251485020764\n",
      "Epoch 11381, Loss: 2.2459392172584947\n",
      "Epoch 11391, Loss: 2.2933050909149006\n",
      "Evaluating\n",
      "Epoch 11401, Loss: 2.970051141896865\n",
      "Epoch 11401, Loss: 2.754154971003923\n",
      "Epoch 11411, Loss: 4.5040850390015015\n",
      "Epoch 11421, Loss: 2.157712037910398\n",
      "Epoch 11431, Loss: 3.0658280059776\n",
      "Epoch 11441, Loss: 3.878212878959239\n",
      "Epoch 11451, Loss: 4.227393086366596\n",
      "Epoch 11461, Loss: 2.546075279938731\n",
      "Epoch 11471, Loss: 2.1618580750797105\n",
      "Epoch 11481, Loss: 5.258272511684384\n",
      "Epoch 11491, Loss: 2.7057135413039637\n",
      "Evaluating\n",
      "Epoch 11501, Loss: 3.0588019158191435\n",
      "Epoch 11501, Loss: 2.548824464508139\n",
      "Epoch 11511, Loss: 2.5632531886374315\n",
      "Epoch 11521, Loss: 3.375518182710834\n",
      "Epoch 11531, Loss: 3.4830888653075873\n",
      "Epoch 11541, Loss: 4.737339714017026\n",
      "Epoch 11551, Loss: 3.543293963892417\n",
      "Epoch 11561, Loss: 3.0814028613012385\n",
      "Epoch 11571, Loss: 2.324436686899864\n",
      "Epoch 11581, Loss: 2.5650287705396906\n",
      "Epoch 11591, Loss: 3.6935449465437213\n",
      "Evaluating\n",
      "Epoch 11601, Loss: 2.83189713751049\n",
      "Epoch 11601, Loss: 3.340539741337052\n",
      "Epoch 11611, Loss: 2.674581121041944\n",
      "Epoch 11621, Loss: 3.4326338795931908\n",
      "Epoch 11631, Loss: 2.2038357463094935\n",
      "Epoch 11641, Loss: 2.385888613846414\n",
      "Epoch 11651, Loss: 2.4520271863772165\n",
      "Epoch 11661, Loss: 3.857564068575078\n",
      "Epoch 11671, Loss: 3.274170749288942\n",
      "Epoch 11681, Loss: 1.4540390493811766\n",
      "Epoch 11691, Loss: 2.6762675731314847\n",
      "Evaluating\n",
      "Epoch 11701, Loss: 2.7851319792205667\n",
      "Epoch 11701, Loss: 2.320181606647493\n",
      "Epoch 11711, Loss: 2.584511857090431\n",
      "Epoch 11721, Loss: 2.5265385692381974\n",
      "Epoch 11731, Loss: 3.4011945466159026\n",
      "Epoch 11741, Loss: 1.9481076941842437\n",
      "Epoch 11751, Loss: 3.856524563490028\n",
      "Epoch 11761, Loss: 2.9081048120759263\n",
      "Epoch 11771, Loss: 2.291674196248187\n",
      "Epoch 11781, Loss: 3.287851947997418\n",
      "Epoch 11791, Loss: 2.2290153727602657\n",
      "Evaluating\n",
      "Epoch 11801, Loss: 2.817663000756546\n",
      "Epoch 11801, Loss: 3.289302219234549\n",
      "Epoch 11811, Loss: 3.144812028252834\n",
      "Epoch 11821, Loss: 1.4965615034512685\n",
      "Epoch 11831, Loss: 2.867888489323191\n",
      "Epoch 11841, Loss: 3.942542425710334\n",
      "Epoch 11851, Loss: 3.306238519177261\n",
      "Epoch 11861, Loss: 2.5147234276687165\n",
      "Epoch 11871, Loss: 3.1543195969303324\n",
      "Epoch 11881, Loss: 2.922661252924558\n",
      "Epoch 11891, Loss: 2.1002725017319603\n",
      "Evaluating\n",
      "Epoch 11901, Loss: 2.922269074999133\n",
      "Epoch 11901, Loss: 2.6312759782706814\n",
      "Epoch 11911, Loss: 3.928453662481117\n",
      "Epoch 11921, Loss: 3.7924678668014926\n",
      "Epoch 11931, Loss: 2.154187917580425\n",
      "Epoch 11941, Loss: 1.876052452028118\n",
      "Epoch 11951, Loss: 3.3377477359153516\n",
      "Epoch 11961, Loss: 2.6749084488242025\n",
      "Epoch 11971, Loss: 2.653107459334647\n",
      "Epoch 11981, Loss: 2.8396486303534707\n",
      "Epoch 11991, Loss: 2.5963793822680965\n",
      "Evaluating\n",
      "Epoch 12001, Loss: 2.857769330725977\n",
      "Epoch 12001, Loss: 2.7560309257173357\n",
      "Epoch 12011, Loss: 3.2569094478466445\n",
      "Epoch 12021, Loss: 3.077439342532008\n",
      "Epoch 12031, Loss: 2.5374702126664235\n",
      "Epoch 12041, Loss: 3.5151454645361238\n",
      "Epoch 12051, Loss: 2.941164315518282\n",
      "Epoch 12061, Loss: 2.314364917628171\n",
      "Epoch 12071, Loss: 3.318441374748716\n",
      "Epoch 12081, Loss: 2.5482211756140916\n",
      "Epoch 12091, Loss: 2.6659884100976137\n",
      "Evaluating\n",
      "Epoch 12101, Loss: 2.66899206016471\n",
      "Epoch 12101, Loss: 2.982153710324515\n",
      "Epoch 12111, Loss: 2.385196881608824\n",
      "Epoch 12121, Loss: 3.3474059598957053\n",
      "Epoch 12131, Loss: 2.4163293816835383\n",
      "Epoch 12141, Loss: 2.885900891482692\n",
      "Epoch 12151, Loss: 2.5717460762953537\n",
      "Epoch 12161, Loss: 2.612102247332667\n",
      "Epoch 12171, Loss: 4.310869216545583\n",
      "Epoch 12181, Loss: 2.9579632815082926\n",
      "Epoch 12191, Loss: 3.9624437090073994\n",
      "Evaluating\n",
      "Epoch 12201, Loss: 2.67958920377139\n",
      "Epoch 12201, Loss: 3.0626814835459912\n",
      "Epoch 12211, Loss: 2.597318515053736\n",
      "Epoch 12221, Loss: 2.7680502641277793\n",
      "Epoch 12231, Loss: 2.8993514875018387\n",
      "Epoch 12241, Loss: 4.115146312962048\n",
      "Epoch 12251, Loss: 3.174415223419728\n",
      "Epoch 12261, Loss: 1.5755200131012523\n",
      "Epoch 12271, Loss: 2.8227757855681306\n",
      "Epoch 12281, Loss: 3.275441840546724\n",
      "Epoch 12291, Loss: 2.394129676819903\n",
      "Evaluating\n",
      "Epoch 12301, Loss: 2.7330512695120524\n",
      "Epoch 12301, Loss: 2.734551143095012\n",
      "Epoch 12311, Loss: 2.755594323708419\n",
      "Epoch 12321, Loss: 3.5729235285629906\n",
      "Epoch 12331, Loss: 3.011997916279072\n",
      "Epoch 12341, Loss: 4.049836170813816\n",
      "Epoch 12351, Loss: 3.93872374579087\n",
      "Epoch 12361, Loss: 1.9134474689629877\n",
      "Epoch 12371, Loss: 2.6139257778005414\n",
      "Epoch 12381, Loss: 2.8007158091384654\n",
      "Epoch 12391, Loss: 2.728785929727434\n",
      "Evaluating\n",
      "Epoch 12401, Loss: 2.6024482615463813\n",
      "Epoch 12401, Loss: 2.081948394915545\n",
      "Epoch 12411, Loss: 2.7413209734291875\n",
      "Epoch 12421, Loss: 2.0878817155270344\n",
      "Epoch 12431, Loss: 3.2291715057016512\n",
      "Epoch 12441, Loss: 2.9898704183034295\n",
      "Epoch 12451, Loss: 2.6447643380473997\n",
      "Epoch 12461, Loss: 3.5274774256267722\n",
      "Epoch 12471, Loss: 3.2867485342290914\n",
      "Epoch 12481, Loss: 2.993711290183312\n",
      "Epoch 12491, Loss: 4.525884125549003\n",
      "Evaluating\n",
      "Epoch 12501, Loss: 2.940194513288198\n",
      "Epoch 12501, Loss: 2.427430111739401\n",
      "Epoch 12511, Loss: 1.6606690352484663\n",
      "Epoch 12521, Loss: 3.3337309460031737\n",
      "Epoch 12531, Loss: 3.082218103135265\n",
      "Epoch 12541, Loss: 3.2872095670003545\n",
      "Epoch 12551, Loss: 2.4355075622941516\n",
      "Epoch 12561, Loss: 3.885207573032872\n",
      "Epoch 12571, Loss: 2.371323899467425\n",
      "Epoch 12581, Loss: 2.663527290692132\n",
      "Epoch 12591, Loss: 2.302756451416282\n",
      "Evaluating\n",
      "Epoch 12601, Loss: 2.802340937179937\n",
      "Epoch 12601, Loss: 2.7285809749793506\n",
      "Epoch 12611, Loss: 2.3788672916889495\n",
      "Epoch 12621, Loss: 2.838394072216689\n",
      "Epoch 12631, Loss: 3.2953030638093077\n",
      "Epoch 12641, Loss: 3.6500954544742683\n",
      "Epoch 12651, Loss: 3.231184042637318\n",
      "Epoch 12661, Loss: 2.9150688696643865\n",
      "Epoch 12671, Loss: 4.088885054768643\n",
      "Epoch 12681, Loss: 3.9963014627107962\n",
      "Epoch 12691, Loss: 3.9379878378236617\n",
      "Evaluating\n",
      "Epoch 12701, Loss: 2.735369215222559\n",
      "Epoch 12701, Loss: 3.8164927496686616\n",
      "Epoch 12711, Loss: 4.284912360630389\n",
      "Epoch 12721, Loss: 3.161546364220394\n",
      "Epoch 12731, Loss: 3.839347918509003\n",
      "Epoch 12741, Loss: 2.0540758296697277\n",
      "Epoch 12751, Loss: 4.320366495372898\n",
      "Epoch 12761, Loss: 2.58970038164765\n",
      "Epoch 12771, Loss: 2.5370704444803356\n",
      "Epoch 12781, Loss: 2.34873787715575\n",
      "Epoch 12791, Loss: 2.732930026216918\n",
      "Evaluating\n",
      "Epoch 12801, Loss: 2.761617414663511\n",
      "Epoch 12801, Loss: 2.443177837331759\n",
      "Epoch 12811, Loss: 5.798949574673265\n",
      "Epoch 12821, Loss: 2.6493837033795065\n",
      "Epoch 12831, Loss: 2.5441423394626237\n",
      "Epoch 12841, Loss: 2.3936572885940035\n",
      "Epoch 12851, Loss: 4.2567372140530235\n",
      "Epoch 12861, Loss: 2.146756712069922\n",
      "Epoch 12871, Loss: 1.779993523464226\n",
      "Epoch 12881, Loss: 3.690981073022231\n",
      "Epoch 12891, Loss: 3.265958149189721\n",
      "Evaluating\n",
      "Epoch 12901, Loss: 2.8292883304921475\n",
      "Epoch 12901, Loss: 2.7863332199364343\n",
      "Epoch 12911, Loss: 2.7605081167090515\n",
      "Epoch 12921, Loss: 2.5746913627249692\n",
      "Epoch 12931, Loss: 2.9069004434515984\n",
      "Epoch 12941, Loss: 3.056905210088051\n",
      "Epoch 12951, Loss: 2.9770306928647363\n",
      "Epoch 12961, Loss: 3.032484651647302\n",
      "Epoch 12971, Loss: 4.542755772870628\n",
      "Epoch 12981, Loss: 3.858621231868343\n",
      "Epoch 12991, Loss: 3.1807343462065854\n",
      "Evaluating\n",
      "Epoch 13001, Loss: 2.5636994253655523\n",
      "Epoch 13001, Loss: 2.224758669440479\n",
      "Epoch 13011, Loss: 2.883651727783456\n",
      "Epoch 13021, Loss: 3.4755167130887448\n",
      "Epoch 13031, Loss: 2.313587988943516\n",
      "Epoch 13041, Loss: 3.115355822711821\n",
      "Epoch 13051, Loss: 2.0475698393641486\n",
      "Epoch 13061, Loss: 2.2342959079423723\n",
      "Epoch 13071, Loss: 2.8445091205361477\n",
      "Epoch 13081, Loss: 2.707550717796016\n",
      "Epoch 13091, Loss: 2.460232570149806\n",
      "Evaluating\n",
      "Epoch 13101, Loss: 2.635337307234866\n",
      "Epoch 13101, Loss: 2.6904860479171955\n",
      "Epoch 13111, Loss: 3.1854820475058183\n",
      "Epoch 13121, Loss: 2.597446371904766\n",
      "Epoch 13131, Loss: 2.887721859578513\n",
      "Epoch 13141, Loss: 3.9883140261253702\n",
      "Epoch 13151, Loss: 3.410899575576041\n",
      "Epoch 13161, Loss: 3.5781299291775865\n",
      "Epoch 13171, Loss: 2.4947532183573258\n",
      "Epoch 13181, Loss: 1.6461299459985\n",
      "Epoch 13191, Loss: 3.048386409469029\n",
      "Evaluating\n",
      "Epoch 13201, Loss: 2.5627324569715753\n",
      "Epoch 13201, Loss: 2.8606981302801344\n",
      "Epoch 13211, Loss: 2.6824551986230127\n",
      "Epoch 13221, Loss: 2.903797380023386\n",
      "Epoch 13231, Loss: 2.4423338006184316\n",
      "Epoch 13241, Loss: 1.8071282834838849\n",
      "Epoch 13251, Loss: 4.119373834125104\n",
      "Epoch 13261, Loss: 2.0927286642548926\n",
      "Epoch 13271, Loss: 2.9134407824289204\n",
      "Epoch 13281, Loss: 3.5841333848039922\n",
      "Epoch 13291, Loss: 2.9723976879998366\n",
      "Evaluating\n",
      "Epoch 13301, Loss: 2.929623508772585\n",
      "Epoch 13301, Loss: 3.326251461580453\n",
      "Epoch 13311, Loss: 1.915391979574036\n",
      "Epoch 13321, Loss: 2.679676588427632\n",
      "Epoch 13331, Loss: 4.201532518045091\n",
      "Epoch 13341, Loss: 2.5684147052789212\n",
      "Epoch 13351, Loss: 2.6550265224481224\n",
      "Epoch 13361, Loss: 2.031877174255943\n",
      "Epoch 13371, Loss: 3.942263568322871\n",
      "Epoch 13381, Loss: 3.14270399837067\n",
      "Epoch 13391, Loss: 3.0217303814698067\n",
      "Evaluating\n",
      "Epoch 13401, Loss: 2.8082873676014586\n",
      "Epoch 13401, Loss: 4.440915153599972\n",
      "Epoch 13411, Loss: 3.4948894711302088\n",
      "Epoch 13421, Loss: 3.543034186046098\n",
      "Epoch 13431, Loss: 2.2208801035472914\n",
      "Epoch 13441, Loss: 2.5266303729189596\n",
      "Epoch 13451, Loss: 3.279880494600229\n",
      "Epoch 13461, Loss: 2.639630349575167\n",
      "Epoch 13471, Loss: 2.4697225895171226\n",
      "Epoch 13481, Loss: 3.6587132435379854\n",
      "Epoch 13491, Loss: 2.6763977112167248\n",
      "Evaluating\n",
      "Epoch 13501, Loss: 2.459582254228303\n",
      "Epoch 13501, Loss: 2.4717391999199165\n",
      "Epoch 13511, Loss: 3.4562068733609568\n",
      "Epoch 13521, Loss: 1.70912347192296\n",
      "Epoch 13531, Loss: 1.9393288570565932\n",
      "Epoch 13541, Loss: 2.7209974858415893\n",
      "Epoch 13551, Loss: 2.5101682268012384\n",
      "Epoch 13561, Loss: 3.013824788642956\n",
      "Epoch 13571, Loss: 2.7275906606873748\n",
      "Epoch 13581, Loss: 2.882263679142818\n",
      "Epoch 13591, Loss: 3.293821119288858\n",
      "Evaluating\n",
      "Epoch 13601, Loss: 2.9270074652197926\n",
      "Epoch 13601, Loss: 3.835868230524477\n",
      "Epoch 13611, Loss: 2.444815341129088\n",
      "Epoch 13621, Loss: 2.2332419538056603\n",
      "Epoch 13631, Loss: 3.086744721653449\n",
      "Epoch 13641, Loss: 4.078276148633783\n",
      "Epoch 13651, Loss: 1.9231708453595209\n",
      "Epoch 13661, Loss: 4.605020273479367\n",
      "Epoch 13671, Loss: 2.9158024560819045\n",
      "Epoch 13681, Loss: 3.465270855361471\n",
      "Epoch 13691, Loss: 3.5695201647726567\n",
      "Evaluating\n",
      "Epoch 13701, Loss: 2.569349730072667\n",
      "Epoch 13701, Loss: 3.2902813560303232\n",
      "Epoch 13711, Loss: 3.3903003982582103\n",
      "Epoch 13721, Loss: 2.207941279856697\n",
      "Epoch 13731, Loss: 2.4785054612508324\n",
      "Epoch 13741, Loss: 1.907380270248254\n",
      "Epoch 13751, Loss: 2.5849653910312873\n",
      "Epoch 13761, Loss: 2.4756904593335642\n",
      "Epoch 13771, Loss: 2.8501127227189027\n",
      "Epoch 13781, Loss: 3.996375165022083\n",
      "Epoch 13791, Loss: 3.1650114126345263\n",
      "Evaluating\n",
      "Epoch 13801, Loss: 2.7268447860512937\n",
      "Epoch 13801, Loss: 2.1155272184943903\n",
      "Epoch 13811, Loss: 2.3647306562601518\n",
      "Epoch 13821, Loss: 2.322726444217994\n",
      "Epoch 13831, Loss: 2.3545261776882844\n",
      "Epoch 13841, Loss: 3.495883273625775\n",
      "Epoch 13851, Loss: 2.4717360718943686\n",
      "Epoch 13861, Loss: 3.6482868643910855\n",
      "Epoch 13871, Loss: 3.111974380705119\n",
      "Epoch 13881, Loss: 2.679270967635316\n",
      "Epoch 13891, Loss: 2.831879716151627\n",
      "Evaluating\n",
      "Epoch 13901, Loss: 2.7304763879407496\n",
      "Epoch 13901, Loss: 3.018742430553991\n",
      "Epoch 13911, Loss: 4.0121886237691875\n",
      "Epoch 13921, Loss: 3.148572288952525\n",
      "Epoch 13931, Loss: 3.311925551569662\n",
      "Epoch 13941, Loss: 2.2709689703037927\n",
      "Epoch 13951, Loss: 2.546298105886193\n",
      "Epoch 13961, Loss: 5.260737131098605\n",
      "Epoch 13971, Loss: 2.586622876427304\n",
      "Epoch 13981, Loss: 2.5506604676264315\n",
      "Epoch 13991, Loss: 2.4160266050821155\n",
      "Evaluating\n",
      "Epoch 14001, Loss: 2.602034563803495\n",
      "Epoch 14001, Loss: 2.8174191664836448\n",
      "Epoch 14011, Loss: 3.8654559336618513\n",
      "Epoch 14021, Loss: 2.487278042863098\n",
      "Epoch 14031, Loss: 2.4129089383397857\n",
      "Epoch 14041, Loss: 2.450085190785603\n",
      "Epoch 14051, Loss: 2.544675990463537\n",
      "Epoch 14061, Loss: 2.614709496452918\n",
      "Epoch 14071, Loss: 2.1720470564765595\n",
      "Epoch 14081, Loss: 2.5969720508868988\n",
      "Epoch 14091, Loss: 3.8000391800145796\n",
      "Evaluating\n",
      "Epoch 14101, Loss: 2.646373379610134\n",
      "Epoch 14101, Loss: 3.016673476765135\n",
      "Epoch 14111, Loss: 2.09133465669926\n",
      "Epoch 14121, Loss: 2.299517319059404\n",
      "Epoch 14131, Loss: 4.712981512709884\n",
      "Epoch 14141, Loss: 2.543035177565497\n",
      "Epoch 14151, Loss: 2.5105837173720067\n",
      "Epoch 14161, Loss: 2.464351609955899\n",
      "Epoch 14171, Loss: 1.620085991511474\n",
      "Epoch 14181, Loss: 2.146708948083231\n",
      "Epoch 14191, Loss: 3.291227201430263\n",
      "Evaluating\n",
      "Epoch 14201, Loss: 2.6711329362060856\n",
      "Epoch 14201, Loss: 3.306039406102972\n",
      "Epoch 14211, Loss: 1.5698384858739538\n",
      "Epoch 14221, Loss: 3.235299018709988\n",
      "Epoch 14231, Loss: 1.89518566097122\n",
      "Epoch 14241, Loss: 2.6783113585434535\n",
      "Epoch 14251, Loss: 2.793863502714777\n",
      "Epoch 14261, Loss: 3.8377718486151235\n",
      "Epoch 14271, Loss: 2.794621264429195\n",
      "Epoch 14281, Loss: 1.6529422474821278\n",
      "Epoch 14291, Loss: 2.6855406490624047\n",
      "Evaluating\n",
      "Epoch 14301, Loss: 2.6244080235321445\n",
      "Epoch 14301, Loss: 2.9775339938198306\n",
      "Epoch 14311, Loss: 2.747103303513831\n",
      "Epoch 14321, Loss: 2.1634456669938302\n",
      "Epoch 14331, Loss: 2.7998020326439432\n",
      "Epoch 14341, Loss: 3.580232294028948\n",
      "Epoch 14351, Loss: 4.745806309330519\n",
      "Epoch 14361, Loss: 2.6303755640689785\n",
      "Epoch 14371, Loss: 2.848990196030644\n",
      "Epoch 14381, Loss: 2.533832540451683\n",
      "Epoch 14391, Loss: 2.800050688268124\n",
      "Evaluating\n",
      "Epoch 14401, Loss: 2.489960548315941\n",
      "Epoch 14401, Loss: 3.8750635892482914\n",
      "Epoch 14411, Loss: 4.540094562728431\n",
      "Epoch 14421, Loss: 3.344306275078913\n",
      "Epoch 14431, Loss: 2.842560352332561\n",
      "Epoch 14441, Loss: 1.9400182559181771\n",
      "Epoch 14451, Loss: 2.6977615958217993\n",
      "Epoch 14461, Loss: 2.507144205570142\n",
      "Epoch 14471, Loss: 3.958600514124027\n",
      "Epoch 14481, Loss: 2.5225561788349427\n",
      "Epoch 14491, Loss: 2.604167854524357\n",
      "Evaluating\n",
      "Epoch 14501, Loss: 2.5601106715774247\n",
      "Epoch 14501, Loss: 3.792507475388768\n",
      "Epoch 14511, Loss: 2.289919548326642\n",
      "Epoch 14521, Loss: 3.0353115818744274\n",
      "Epoch 14531, Loss: 2.6081721589408295\n",
      "Epoch 14541, Loss: 1.504145210258918\n",
      "Epoch 14551, Loss: 3.696935914303962\n",
      "Epoch 14561, Loss: 2.3796209732805225\n",
      "Epoch 14571, Loss: 3.1267240146155215\n",
      "Epoch 14581, Loss: 2.6649270349240552\n",
      "Epoch 14591, Loss: 2.3663232581213443\n",
      "Evaluating\n",
      "Epoch 14601, Loss: 2.509957301988588\n",
      "Epoch 14601, Loss: 3.409578235834826\n",
      "Epoch 14611, Loss: 2.44589847352605\n",
      "Epoch 14621, Loss: 6.000403090682191\n",
      "Epoch 14631, Loss: 1.677050633950683\n",
      "Epoch 14641, Loss: 2.35390038514645\n",
      "Epoch 14651, Loss: 3.472854427959836\n",
      "Epoch 14661, Loss: 2.8621994175848893\n",
      "Epoch 14671, Loss: 1.6200486074731315\n",
      "Epoch 14681, Loss: 3.827333616859941\n",
      "Epoch 14691, Loss: 2.890275503831521\n",
      "Evaluating\n",
      "Epoch 14701, Loss: 2.619542517505231\n",
      "Epoch 14701, Loss: 2.720749453972438\n",
      "Epoch 14711, Loss: 2.8572185177400558\n",
      "Epoch 14721, Loss: 3.1623153318606216\n",
      "Epoch 14731, Loss: 2.9604135556799696\n",
      "Epoch 14741, Loss: 3.0568861804804173\n",
      "Epoch 14751, Loss: 2.566142081956018\n",
      "Epoch 14761, Loss: 3.2742994998086425\n",
      "Epoch 14771, Loss: 2.4062615623504446\n",
      "Epoch 14781, Loss: 2.303888152970032\n",
      "Epoch 14791, Loss: 3.6877429747313233\n",
      "Evaluating\n",
      "Epoch 14801, Loss: 2.5529539392030958\n",
      "Epoch 14801, Loss: 2.161433123523461\n",
      "Epoch 14811, Loss: 2.6814792916707217\n",
      "Epoch 14821, Loss: 2.9367554348927496\n",
      "Epoch 14831, Loss: 2.867252947070342\n",
      "Epoch 14841, Loss: 3.516772528710476\n",
      "Epoch 14851, Loss: 2.1237817445399765\n",
      "Epoch 14861, Loss: 3.319345366328719\n",
      "Epoch 14871, Loss: 3.0310858062676016\n",
      "Epoch 14881, Loss: 2.8571957098605045\n",
      "Epoch 14891, Loss: 3.0580005377283244\n",
      "Evaluating\n",
      "Epoch 14901, Loss: 2.5271506764706344\n",
      "Epoch 14901, Loss: 2.078537251939967\n",
      "Epoch 14911, Loss: 2.0379222233747556\n",
      "Epoch 14921, Loss: 2.606406615688216\n",
      "Epoch 14931, Loss: 2.893212882348221\n",
      "Epoch 14941, Loss: 3.4485977954700964\n",
      "Epoch 14951, Loss: 2.9087907917448894\n",
      "Epoch 14961, Loss: 3.0250679886505925\n",
      "Epoch 14971, Loss: 3.2897293194205344\n",
      "Epoch 14981, Loss: 2.697543228645153\n",
      "Epoch 14991, Loss: 3.194273322979172\n",
      "Evaluating\n",
      "Epoch 15001, Loss: 2.506214913177318\n",
      "Epoch 15001, Loss: 3.1768533831573844\n",
      "Epoch 15011, Loss: 1.9690951692044927\n",
      "Epoch 15021, Loss: 2.2985905795318704\n",
      "Epoch 15031, Loss: 1.6321947304898774\n",
      "Epoch 15041, Loss: 2.3247560641940743\n",
      "Epoch 15051, Loss: 2.492695260113774\n",
      "Epoch 15061, Loss: 3.2383389860390053\n",
      "Epoch 15071, Loss: 2.1231525357362266\n",
      "Epoch 15081, Loss: 3.8962218140465734\n",
      "Epoch 15091, Loss: 2.7497003740611854\n",
      "Evaluating\n",
      "Epoch 15101, Loss: 2.674534592647098\n",
      "Epoch 15101, Loss: 3.192647733322493\n",
      "Epoch 15111, Loss: 3.590610198355844\n",
      "Epoch 15121, Loss: 2.749542639906926\n",
      "Epoch 15131, Loss: 2.222417995725797\n",
      "Epoch 15141, Loss: 2.6633264377463632\n",
      "Epoch 15151, Loss: 2.928115171640103\n",
      "Epoch 15161, Loss: 2.9857844096295527\n",
      "Epoch 15171, Loss: 1.6829074164510858\n",
      "Epoch 15181, Loss: 4.328702856459611\n",
      "Epoch 15191, Loss: 3.0163583364352147\n",
      "Evaluating\n",
      "Epoch 15201, Loss: 2.575883970962314\n",
      "Epoch 15201, Loss: 3.120922821489717\n",
      "Epoch 15211, Loss: 2.53036715735366\n",
      "Epoch 15221, Loss: 2.688211576734033\n",
      "Epoch 15231, Loss: 3.4973161270033923\n",
      "Epoch 15241, Loss: 2.162159407936248\n",
      "Epoch 15251, Loss: 1.6211444880996073\n",
      "Epoch 15261, Loss: 1.7640210198075412\n",
      "Epoch 15271, Loss: 3.159790451428872\n",
      "Epoch 15281, Loss: 2.593721911519954\n",
      "Epoch 15291, Loss: 1.9919739022005687\n",
      "Evaluating\n",
      "Epoch 15301, Loss: 2.4297997468158927\n",
      "Epoch 15301, Loss: 2.5502927952827625\n",
      "Epoch 15311, Loss: 2.621469017676061\n",
      "Epoch 15321, Loss: 2.63172257238491\n",
      "Epoch 15331, Loss: 2.985521749818519\n",
      "Epoch 15341, Loss: 4.806618645054293\n",
      "Epoch 15351, Loss: 1.653976656462972\n",
      "Epoch 15361, Loss: 3.7127467176004996\n",
      "Epoch 15371, Loss: 2.7994945685564523\n",
      "Epoch 15381, Loss: 3.9766885072528746\n",
      "Epoch 15391, Loss: 3.1561302314869044\n",
      "Evaluating\n",
      "Epoch 15401, Loss: 2.523285253813932\n",
      "Epoch 15401, Loss: 2.5775305839353786\n",
      "Epoch 15411, Loss: 2.612664715781321\n",
      "Epoch 15421, Loss: 2.411417872735054\n",
      "Epoch 15431, Loss: 2.5230888504238984\n",
      "Epoch 15441, Loss: 2.192345432336164\n",
      "Epoch 15451, Loss: 2.363537895626287\n",
      "Epoch 15461, Loss: 2.5039086268334527\n",
      "Epoch 15471, Loss: 4.672965543439441\n",
      "Epoch 15481, Loss: 2.141330116080141\n",
      "Epoch 15491, Loss: 3.016975160442561\n",
      "Evaluating\n",
      "Epoch 15501, Loss: 2.4436566985006394\n",
      "Epoch 15501, Loss: 2.5975817345500603\n",
      "Epoch 15511, Loss: 3.2036215797832615\n",
      "Epoch 15521, Loss: 3.215060268177465\n",
      "Epoch 15531, Loss: 3.403615961394474\n",
      "Epoch 15541, Loss: 2.3048858429740218\n",
      "Epoch 15551, Loss: 2.59206172158409\n",
      "Epoch 15561, Loss: 3.927240364968084\n",
      "Epoch 15571, Loss: 2.228449499196288\n",
      "Epoch 15581, Loss: 3.386697320439767\n",
      "Epoch 15591, Loss: 2.4638555519181278\n",
      "Evaluating\n",
      "Epoch 15601, Loss: 2.682816423402272\n",
      "Epoch 15601, Loss: 2.9841113770662284\n",
      "Epoch 15611, Loss: 2.7608157885370455\n",
      "Epoch 15621, Loss: 1.9029978859422843\n",
      "Epoch 15631, Loss: 4.48528332568095\n",
      "Epoch 15641, Loss: 2.5391634866024213\n",
      "Epoch 15651, Loss: 3.1938430257641026\n",
      "Epoch 15661, Loss: 2.9981006396890457\n",
      "Epoch 15671, Loss: 2.638925047203172\n",
      "Epoch 15681, Loss: 2.645093128566479\n",
      "Epoch 15691, Loss: 3.4648779321266323\n",
      "Evaluating\n",
      "Epoch 15701, Loss: 2.5569509013638974\n",
      "Epoch 15701, Loss: 3.1698344668686467\n",
      "Epoch 15711, Loss: 3.6201287194331493\n",
      "Epoch 15721, Loss: 3.1252474243419757\n",
      "Epoch 15731, Loss: 1.745638585314811\n",
      "Epoch 15741, Loss: 2.771664140141265\n",
      "Epoch 15751, Loss: 3.70803438526407\n",
      "Epoch 15761, Loss: 2.322722402222164\n",
      "Epoch 15771, Loss: 2.7349013138426703\n",
      "Epoch 15781, Loss: 2.7203925894054066\n",
      "Epoch 15791, Loss: 3.3048240886663214\n",
      "Evaluating\n",
      "Epoch 15801, Loss: 2.4260651311516943\n",
      "Epoch 15801, Loss: 2.2538724762759035\n",
      "Epoch 15811, Loss: 2.516538810259537\n",
      "Epoch 15821, Loss: 1.7146975058875444\n",
      "Epoch 15831, Loss: 2.326794673588036\n",
      "Epoch 15841, Loss: 4.293177437532233\n",
      "Epoch 15851, Loss: 2.5130601283278864\n",
      "Epoch 15861, Loss: 2.3105798823329207\n",
      "Epoch 15871, Loss: 2.8508207930832308\n",
      "Epoch 15881, Loss: 1.5681824169127319\n",
      "Epoch 15891, Loss: 2.92092698230527\n",
      "Evaluating\n",
      "Epoch 15901, Loss: 2.5667080237779856\n",
      "Epoch 15901, Loss: 3.042537861273145\n",
      "Epoch 15911, Loss: 2.0364447795464127\n",
      "Epoch 15921, Loss: 3.2760516902765433\n",
      "Epoch 15931, Loss: 2.994303638467425\n",
      "Epoch 15941, Loss: 2.072479475682963\n",
      "Epoch 15951, Loss: 3.8930787703029366\n",
      "Epoch 15961, Loss: 2.777700235022767\n",
      "Epoch 15971, Loss: 2.7774833806200414\n",
      "Epoch 15981, Loss: 2.9553847816982\n",
      "Epoch 15991, Loss: 3.0123116794380884\n",
      "Evaluating\n",
      "Epoch 16001, Loss: 2.540525234830226\n",
      "Epoch 16001, Loss: 2.087294337078312\n",
      "Epoch 16011, Loss: 3.5032870826266813\n",
      "Epoch 16021, Loss: 2.3336359192334863\n",
      "Epoch 16031, Loss: 2.7434424464878093\n",
      "Epoch 16041, Loss: 3.3319366009657303\n",
      "Epoch 16051, Loss: 2.3226965649845988\n",
      "Epoch 16061, Loss: 3.2180292002230106\n",
      "Epoch 16071, Loss: 3.0545061843042003\n",
      "Epoch 16081, Loss: 2.647307571918768\n",
      "Epoch 16091, Loss: 2.3740266959472343\n",
      "Evaluating\n",
      "Epoch 16101, Loss: 2.693236965053393\n",
      "Epoch 16101, Loss: 2.559937084807825\n",
      "Epoch 16111, Loss: 2.887934005836787\n",
      "Epoch 16121, Loss: 2.8707477143019524\n",
      "Epoch 16131, Loss: 2.2692204590346754\n",
      "Epoch 16141, Loss: 2.1483263296882678\n",
      "Epoch 16151, Loss: 2.4592070282887764\n",
      "Epoch 16161, Loss: 2.3983161165688855\n",
      "Epoch 16171, Loss: 2.4485403041727345\n",
      "Epoch 16181, Loss: 3.1105503257793554\n",
      "Epoch 16191, Loss: 2.20584740773246\n",
      "Evaluating\n",
      "Epoch 16201, Loss: 2.4306379873333364\n",
      "Epoch 16201, Loss: 1.7662705557466751\n",
      "Epoch 16211, Loss: 3.741347378077938\n",
      "Epoch 16221, Loss: 3.1325135240991475\n",
      "Epoch 16231, Loss: 2.561551548310199\n",
      "Epoch 16241, Loss: 2.916174672075256\n",
      "Epoch 16251, Loss: 2.8174668634865787\n",
      "Epoch 16261, Loss: 2.7348410031231247\n",
      "Epoch 16271, Loss: 2.3388412858129137\n",
      "Epoch 16281, Loss: 1.9941638442311735\n",
      "Epoch 16291, Loss: 3.0229145772494443\n",
      "Evaluating\n",
      "Epoch 16301, Loss: 2.380632006377536\n",
      "Epoch 16301, Loss: 2.6641521464615883\n",
      "Epoch 16311, Loss: 2.3624688987393005\n",
      "Epoch 16321, Loss: 2.2418506349365575\n",
      "Epoch 16331, Loss: 2.9304363738394046\n",
      "Epoch 16341, Loss: 3.128697836797479\n",
      "Epoch 16351, Loss: 1.788617209331905\n",
      "Epoch 16361, Loss: 3.257772876843818\n",
      "Epoch 16371, Loss: 1.9876337364295202\n",
      "Epoch 16381, Loss: 1.8532364028672685\n",
      "Epoch 16391, Loss: 2.258643181624258\n",
      "Evaluating\n",
      "Epoch 16401, Loss: 2.7562267595941607\n",
      "Epoch 16401, Loss: 2.651036049530865\n",
      "Epoch 16411, Loss: 2.143633007603946\n",
      "Epoch 16421, Loss: 2.095257804216089\n",
      "Epoch 16431, Loss: 3.3335754441373235\n",
      "Epoch 16441, Loss: 2.220967218450131\n",
      "Epoch 16451, Loss: 3.333245530861292\n",
      "Epoch 16461, Loss: 1.5825719527847744\n",
      "Epoch 16471, Loss: 2.5855572584818454\n",
      "Epoch 16481, Loss: 2.294611940410671\n",
      "Epoch 16491, Loss: 2.791641869603992\n",
      "Evaluating\n",
      "Epoch 16501, Loss: 2.590908959684341\n",
      "Epoch 16501, Loss: 2.8496305865634897\n",
      "Epoch 16511, Loss: 1.9068944581486873\n",
      "Epoch 16521, Loss: 2.570309052329164\n",
      "Epoch 16531, Loss: 3.0269786837110457\n",
      "Epoch 16541, Loss: 3.7737773295206773\n",
      "Epoch 16551, Loss: 2.979735646415178\n",
      "Epoch 16561, Loss: 2.701675559299877\n",
      "Epoch 16571, Loss: 3.0375907460395806\n",
      "Epoch 16581, Loss: 2.324240984262528\n",
      "Epoch 16591, Loss: 2.280615659066418\n",
      "Evaluating\n",
      "Epoch 16601, Loss: 2.66728002697512\n",
      "Epoch 16601, Loss: 2.8878192652042953\n",
      "Epoch 16611, Loss: 3.2497838347845827\n",
      "Epoch 16621, Loss: 4.323269077667119\n",
      "Epoch 16631, Loss: 3.8375425586284395\n",
      "Epoch 16641, Loss: 4.128497626137388\n",
      "Epoch 16651, Loss: 2.590948321732143\n",
      "Epoch 16661, Loss: 3.4439186155031485\n",
      "Epoch 16671, Loss: 3.40894529263136\n",
      "Epoch 16681, Loss: 2.780615786441162\n",
      "Epoch 16691, Loss: 4.385293714272732\n",
      "Evaluating\n",
      "Epoch 16701, Loss: 2.377513996110496\n",
      "Epoch 16701, Loss: 2.9341529023777535\n",
      "Epoch 16711, Loss: 2.5569448537955908\n",
      "Epoch 16721, Loss: 2.8365366349610386\n",
      "Epoch 16731, Loss: 2.9419037845210303\n",
      "Epoch 16741, Loss: 2.628610507434022\n",
      "Epoch 16751, Loss: 2.582754231039853\n",
      "Epoch 16761, Loss: 2.9884533507492375\n",
      "Epoch 16771, Loss: 1.8084441324376184\n",
      "Epoch 16781, Loss: 2.2172380300405345\n",
      "Epoch 16791, Loss: 2.783590317445777\n",
      "Evaluating\n",
      "Epoch 16801, Loss: 2.457167096968023\n",
      "Epoch 16801, Loss: 2.9310047051140056\n",
      "Epoch 16811, Loss: 3.1476757475517916\n",
      "Epoch 16821, Loss: 2.7050887586124954\n",
      "Epoch 16831, Loss: 3.1856481299211437\n",
      "Epoch 16841, Loss: 2.6724206889870987\n",
      "Epoch 16851, Loss: 2.832968215224214\n",
      "Epoch 16861, Loss: 3.11409410888703\n",
      "Epoch 16871, Loss: 3.5416009090094076\n",
      "Epoch 16881, Loss: 4.25466245447869\n",
      "Epoch 16891, Loss: 2.72715882076092\n",
      "Evaluating\n",
      "Epoch 16901, Loss: 2.585009543665479\n",
      "Epoch 16901, Loss: 4.720326958880315\n",
      "Epoch 16911, Loss: 2.6782367139794685\n",
      "Epoch 16921, Loss: 3.6835516204365164\n",
      "Epoch 16931, Loss: 2.7953095413792526\n",
      "Epoch 16941, Loss: 1.9316710996506052\n",
      "Epoch 16951, Loss: 3.352042149690848\n",
      "Epoch 16961, Loss: 2.1854842414646365\n",
      "Epoch 16971, Loss: 1.665057819757025\n",
      "Epoch 16981, Loss: 2.148114496989692\n",
      "Epoch 16991, Loss: 2.780608106799139\n",
      "Evaluating\n",
      "Epoch 17001, Loss: 2.4969887826942387\n",
      "Epoch 17001, Loss: 2.465577207167136\n",
      "Epoch 17011, Loss: 2.5216265799283866\n",
      "Epoch 17021, Loss: 3.5547764453536312\n",
      "Epoch 17031, Loss: 3.0323440555120236\n",
      "Epoch 17041, Loss: 2.886819936516687\n",
      "Epoch 17051, Loss: 3.0293437245093813\n",
      "Epoch 17061, Loss: 1.606261660822943\n",
      "Epoch 17071, Loss: 1.5102787723666666\n",
      "Epoch 17081, Loss: 2.372599026069366\n",
      "Epoch 17091, Loss: 2.7815692213515257\n",
      "Evaluating\n",
      "Epoch 17101, Loss: 2.7130807023590995\n",
      "Epoch 17101, Loss: 3.496944702872915\n",
      "Epoch 17111, Loss: 3.7026141029899757\n",
      "Epoch 17121, Loss: 1.956452188304384\n",
      "Epoch 17131, Loss: 2.980896137008381\n",
      "Epoch 17141, Loss: 3.4774175965589995\n",
      "Epoch 17151, Loss: 2.4363147487057706\n",
      "Epoch 17161, Loss: 2.179691276354851\n",
      "Epoch 17171, Loss: 2.264596738574141\n",
      "Epoch 17181, Loss: 2.9873539678028154\n",
      "Epoch 17191, Loss: 2.6351963394293865\n",
      "Evaluating\n",
      "Epoch 17201, Loss: 2.474472846970419\n",
      "Epoch 17201, Loss: 3.8619385652777627\n",
      "Epoch 17211, Loss: 4.401736175398283\n",
      "Epoch 17221, Loss: 1.6783940448570152\n",
      "Epoch 17231, Loss: 2.490594210529653\n",
      "Epoch 17241, Loss: 3.5182186413790193\n",
      "Epoch 17251, Loss: 2.361465290485852\n",
      "Epoch 17261, Loss: 2.448674350700828\n",
      "Epoch 17271, Loss: 3.385613615803133\n",
      "Epoch 17281, Loss: 2.5281421766431635\n",
      "Epoch 17291, Loss: 2.5112790299307086\n",
      "Evaluating\n",
      "Epoch 17301, Loss: 2.587557297650127\n",
      "Epoch 17301, Loss: 2.1682554657085356\n",
      "Epoch 17311, Loss: 1.9736520257648282\n",
      "Epoch 17321, Loss: 3.1647878254998316\n",
      "Epoch 17331, Loss: 2.6699652229413764\n",
      "Epoch 17341, Loss: 3.498323658458727\n",
      "Epoch 17351, Loss: 2.5783343615722143\n",
      "Epoch 17361, Loss: 2.2284969981224285\n",
      "Epoch 17371, Loss: 2.6666741287713633\n",
      "Epoch 17381, Loss: 2.250664904089086\n",
      "Epoch 17391, Loss: 1.6572303635985302\n",
      "Evaluating\n",
      "Epoch 17401, Loss: 2.548872421003579\n",
      "Epoch 17401, Loss: 1.4943819595393266\n",
      "Epoch 17411, Loss: 3.5847868512302075\n",
      "Epoch 17421, Loss: 2.930347169490085\n",
      "Epoch 17431, Loss: 3.4287550962657964\n",
      "Epoch 17441, Loss: 1.9437762205172502\n",
      "Epoch 17451, Loss: 3.0149736234226134\n",
      "Epoch 17461, Loss: 2.6876666994343053\n",
      "Epoch 17471, Loss: 2.834344813187794\n",
      "Epoch 17481, Loss: 2.5659607246361067\n",
      "Epoch 17491, Loss: 3.0834540543286564\n",
      "Evaluating\n",
      "Epoch 17501, Loss: 2.4430641354540095\n",
      "Epoch 17501, Loss: 5.322654623832703\n",
      "Epoch 17511, Loss: 3.5427459142531674\n",
      "Epoch 17521, Loss: 3.8061356485846987\n",
      "Epoch 17531, Loss: 2.7480630064836387\n",
      "Epoch 17541, Loss: 3.0811667545177763\n",
      "Epoch 17551, Loss: 3.7166582996066966\n",
      "Epoch 17561, Loss: 4.811559850931606\n",
      "Epoch 17571, Loss: 2.7230838203088608\n",
      "Epoch 17581, Loss: 2.178458974360574\n",
      "Epoch 17591, Loss: 1.5636701307529102\n",
      "Evaluating\n",
      "Epoch 17601, Loss: 2.398507540426198\n",
      "Epoch 17601, Loss: 1.8954117756094135\n",
      "Epoch 17611, Loss: 2.627466073900905\n",
      "Epoch 17621, Loss: 3.0257211236267447\n",
      "Epoch 17631, Loss: 3.056241151249475\n",
      "Epoch 17641, Loss: 1.8772354630599037\n",
      "Epoch 17651, Loss: 2.171648088325501\n",
      "Epoch 17661, Loss: 2.065203161684504\n",
      "Epoch 17671, Loss: 3.870688897192383\n",
      "Epoch 17681, Loss: 2.692417304415478\n",
      "Epoch 17691, Loss: 2.4499240960883277\n",
      "Evaluating\n",
      "Epoch 17701, Loss: 2.5310840910680072\n",
      "Epoch 17701, Loss: 2.7110149176119096\n",
      "Epoch 17711, Loss: 2.7446706524978506\n",
      "Epoch 17721, Loss: 2.3554783487665927\n",
      "Epoch 17731, Loss: 2.86670125922554\n",
      "Epoch 17741, Loss: 3.24646087029698\n",
      "Epoch 17751, Loss: 3.628246012868879\n",
      "Epoch 17761, Loss: 1.7744483946440377\n",
      "Epoch 17771, Loss: 3.296711751480828\n",
      "Epoch 17781, Loss: 2.8207887700800334\n",
      "Epoch 17791, Loss: 2.900604305028054\n",
      "Evaluating\n",
      "Epoch 17801, Loss: 2.5921862869151813\n",
      "Epoch 17801, Loss: 2.2696723530594176\n",
      "Epoch 17811, Loss: 2.2479423540491483\n",
      "Epoch 17821, Loss: 2.4160739973819325\n",
      "Epoch 17831, Loss: 3.5686233190829526\n",
      "Epoch 17841, Loss: 2.178140795967998\n",
      "Epoch 17851, Loss: 2.878981757709149\n",
      "Epoch 17861, Loss: 3.1994727666112097\n",
      "Epoch 17871, Loss: 2.1092680406991184\n",
      "Epoch 17881, Loss: 2.0948307663038443\n",
      "Epoch 17891, Loss: 2.107472371503299\n",
      "Evaluating\n",
      "Epoch 17901, Loss: 2.3666710520966836\n",
      "Epoch 17901, Loss: 2.216292085758898\n",
      "Epoch 17911, Loss: 3.0303619032528357\n",
      "Epoch 17921, Loss: 1.8941998822684007\n",
      "Epoch 17931, Loss: 2.614985326423015\n",
      "Epoch 17941, Loss: 2.6518983501706694\n",
      "Epoch 17951, Loss: 1.821417847601468\n",
      "Epoch 17961, Loss: 2.122174629378356\n",
      "Epoch 17971, Loss: 2.256190064072993\n",
      "Epoch 17981, Loss: 2.338323167903211\n",
      "Epoch 17991, Loss: 2.306323341960808\n",
      "Evaluating\n",
      "Epoch 18001, Loss: 2.6246472044917377\n",
      "Epoch 18001, Loss: 2.0490068288235874\n",
      "Epoch 18011, Loss: 2.329764461196213\n",
      "Epoch 18021, Loss: 2.3512581492770157\n",
      "Epoch 18031, Loss: 1.1310500962481729\n",
      "Epoch 18041, Loss: 2.249959743813677\n",
      "Epoch 18051, Loss: 1.6149694819747804\n",
      "Epoch 18061, Loss: 2.509307988205549\n",
      "Epoch 18071, Loss: 2.865746213309493\n",
      "Epoch 18081, Loss: 1.490855044270528\n",
      "Epoch 18091, Loss: 2.734260309773375\n",
      "Evaluating\n",
      "Epoch 18101, Loss: 2.2947806490915865\n",
      "Epoch 18101, Loss: 1.7578245077425445\n",
      "Epoch 18111, Loss: 4.638250815652066\n",
      "Epoch 18121, Loss: 2.3835201187483395\n",
      "Epoch 18131, Loss: 3.0631424658133897\n",
      "Epoch 18141, Loss: 2.0713267450808948\n",
      "Epoch 18151, Loss: 3.107770629607857\n",
      "Epoch 18161, Loss: 2.3314779896087647\n",
      "Epoch 18171, Loss: 1.876980776496797\n",
      "Epoch 18181, Loss: 2.0106669279471987\n",
      "Epoch 18191, Loss: 2.139862473364211\n",
      "Evaluating\n",
      "Epoch 18201, Loss: 2.6463810314224587\n",
      "Epoch 18201, Loss: 3.5700055269441187\n",
      "Epoch 18211, Loss: 3.480757643083013\n",
      "Epoch 18221, Loss: 2.463997805278927\n",
      "Epoch 18231, Loss: 2.6530716561908436\n",
      "Epoch 18241, Loss: 2.333013281712637\n",
      "Epoch 18251, Loss: 2.7972592442636905\n",
      "Epoch 18261, Loss: 2.831184204360454\n",
      "Epoch 18271, Loss: 3.849741907209884\n",
      "Epoch 18281, Loss: 4.281346946729246\n",
      "Epoch 18291, Loss: 2.262054294360739\n",
      "Evaluating\n",
      "Epoch 18301, Loss: 2.6288088308516877\n",
      "Epoch 18301, Loss: 3.119207897210398\n",
      "Epoch 18311, Loss: 2.781844917845273\n",
      "Epoch 18321, Loss: 3.2163605576937226\n",
      "Epoch 18331, Loss: 2.255587344902506\n",
      "Epoch 18341, Loss: 2.3116305887155835\n",
      "Epoch 18351, Loss: 1.988998708550441\n",
      "Epoch 18361, Loss: 1.5856975069228978\n",
      "Epoch 18371, Loss: 2.268979671710899\n",
      "Epoch 18381, Loss: 2.1893218387902564\n",
      "Epoch 18391, Loss: 2.2728285598063636\n",
      "Evaluating\n",
      "Epoch 18401, Loss: 2.5191990732090654\n",
      "Epoch 18401, Loss: 1.9451311793257184\n",
      "Epoch 18411, Loss: 2.172191809464423\n",
      "Epoch 18421, Loss: 2.3571905221175333\n",
      "Epoch 18431, Loss: 1.6493217855506201\n",
      "Epoch 18441, Loss: 2.7560699333019034\n",
      "Epoch 18451, Loss: 2.0251197157236054\n",
      "Epoch 18461, Loss: 4.40485511984197\n",
      "Epoch 18471, Loss: 2.6027133670414386\n",
      "Epoch 18481, Loss: 4.2278588313758245\n",
      "Epoch 18491, Loss: 3.2176527698255883\n",
      "Evaluating\n",
      "Epoch 18501, Loss: 2.9324574438502697\n",
      "Epoch 18501, Loss: 3.465583525820068\n",
      "Epoch 18511, Loss: 3.0032355264503647\n",
      "Epoch 18521, Loss: 3.298637097493271\n",
      "Epoch 18531, Loss: 2.1014809235639467\n",
      "Epoch 18541, Loss: 2.8350648732821817\n",
      "Epoch 18551, Loss: 3.3339760013378315\n",
      "Epoch 18561, Loss: 2.004847954013307\n",
      "Epoch 18571, Loss: 1.9879967588272907\n",
      "Epoch 18581, Loss: 2.421534953971574\n",
      "Epoch 18591, Loss: 2.951832439426069\n",
      "Evaluating\n",
      "Epoch 18601, Loss: 2.404062615928535\n",
      "Epoch 18601, Loss: 3.1789350617144714\n",
      "Epoch 18611, Loss: 2.7470243630069366\n",
      "Epoch 18621, Loss: 2.598075500362609\n",
      "Epoch 18631, Loss: 2.9493392094751276\n",
      "Epoch 18641, Loss: 2.5054165070273835\n",
      "Epoch 18651, Loss: 1.6428144189992404\n",
      "Epoch 18661, Loss: 3.2078102484247832\n",
      "Epoch 18671, Loss: 2.0449169072864533\n",
      "Epoch 18681, Loss: 2.0661982316066014\n",
      "Epoch 18691, Loss: 2.506476683300204\n",
      "Evaluating\n",
      "Epoch 18701, Loss: 2.383266007670412\n",
      "Epoch 18701, Loss: 2.7588947788744074\n",
      "Epoch 18711, Loss: 2.3781650798110703\n",
      "Epoch 18721, Loss: 3.015041152391403\n",
      "Epoch 18731, Loss: 2.60599679576153\n",
      "Epoch 18741, Loss: 3.3115566815415396\n",
      "Epoch 18751, Loss: 2.0710309512671423\n",
      "Epoch 18761, Loss: 2.0463283221572706\n",
      "Epoch 18771, Loss: 2.4812966449084537\n",
      "Epoch 18781, Loss: 1.4151402624966205\n",
      "Epoch 18791, Loss: 2.0769153354334438\n",
      "Evaluating\n",
      "Epoch 18801, Loss: 2.57938683424196\n",
      "Epoch 18801, Loss: 3.1103290973782483\n",
      "Epoch 18811, Loss: 2.444589891705019\n",
      "Epoch 18821, Loss: 3.1393617476096827\n",
      "Epoch 18831, Loss: 2.646276408362553\n",
      "Epoch 18841, Loss: 3.768775173998951\n",
      "Epoch 18851, Loss: 1.6910795733040498\n",
      "Epoch 18861, Loss: 2.4579313708722017\n",
      "Epoch 18871, Loss: 2.3578409718967865\n",
      "Epoch 18881, Loss: 2.6816135703004598\n",
      "Epoch 18891, Loss: 2.902576637107387\n",
      "Evaluating\n",
      "Epoch 18901, Loss: 2.361864306425303\n",
      "Epoch 18901, Loss: 1.7999844776547869\n",
      "Epoch 18911, Loss: 2.6797191805447125\n",
      "Epoch 18921, Loss: 3.1860481022063367\n",
      "Epoch 18931, Loss: 2.2032974111732675\n",
      "Epoch 18941, Loss: 2.6740963876094477\n",
      "Epoch 18951, Loss: 1.9123192050511397\n",
      "Epoch 18961, Loss: 2.297720911007668\n",
      "Epoch 18971, Loss: 2.5574910930988883\n",
      "Epoch 18981, Loss: 2.976427980042286\n",
      "Epoch 18991, Loss: 2.7469393888565428\n",
      "Evaluating\n",
      "Epoch 19001, Loss: 2.398276893515677\n",
      "Epoch 19001, Loss: 2.87742589049938\n",
      "Epoch 19011, Loss: 4.229423595710807\n",
      "Epoch 19021, Loss: 2.5905337823560326\n",
      "Epoch 19031, Loss: 2.007726953794578\n",
      "Epoch 19041, Loss: 3.6258999772575504\n",
      "Epoch 19051, Loss: 2.5150564825337693\n",
      "Epoch 19061, Loss: 2.2358185617183075\n",
      "Epoch 19071, Loss: 2.7670285702944755\n",
      "Epoch 19081, Loss: 2.3275812719241236\n",
      "Epoch 19091, Loss: 2.376136223078529\n",
      "Evaluating\n",
      "Epoch 19101, Loss: 2.291932839922605\n",
      "Epoch 19101, Loss: 2.5032320053128876\n",
      "Epoch 19111, Loss: 2.4569397090050353\n",
      "Epoch 19121, Loss: 2.2710035831040933\n",
      "Epoch 19131, Loss: 4.667663561186751\n",
      "Epoch 19141, Loss: 3.1849580856844297\n",
      "Epoch 19151, Loss: 1.9262405487912757\n",
      "Epoch 19161, Loss: 2.330683939172838\n",
      "Epoch 19171, Loss: 2.4057261322316994\n",
      "Epoch 19181, Loss: 3.119609306790943\n",
      "Epoch 19191, Loss: 2.5177121699305047\n",
      "Evaluating\n",
      "Epoch 19201, Loss: 2.5560413015690466\n",
      "Epoch 19201, Loss: 2.9879955085061933\n",
      "Epoch 19211, Loss: 2.2517276261952146\n",
      "Epoch 19221, Loss: 2.367915177548918\n",
      "Epoch 19231, Loss: 3.622181500144136\n",
      "Epoch 19241, Loss: 1.919080757115846\n",
      "Epoch 19251, Loss: 1.9993037583591204\n",
      "Epoch 19261, Loss: 1.7933641323496277\n",
      "Epoch 19271, Loss: 1.6173417672354993\n",
      "Epoch 19281, Loss: 2.9520377334914465\n",
      "Epoch 19291, Loss: 3.481154908154622\n",
      "Evaluating\n",
      "Epoch 19301, Loss: 2.575999742640839\n",
      "Epoch 19301, Loss: 1.9281818961169417\n",
      "Epoch 19311, Loss: 2.3570044120474414\n",
      "Epoch 19321, Loss: 3.8478583720843056\n",
      "Epoch 19331, Loss: 3.037019205997308\n",
      "Epoch 19341, Loss: 2.549467179896826\n",
      "Epoch 19351, Loss: 2.4299667210873634\n",
      "Epoch 19361, Loss: 2.3233574640867984\n",
      "Epoch 19371, Loss: 3.3211088223644887\n",
      "Epoch 19381, Loss: 2.1627164097964884\n",
      "Epoch 19391, Loss: 3.9536059089259705\n",
      "Evaluating\n",
      "Epoch 19401, Loss: 2.352528814853441\n",
      "Epoch 19401, Loss: 2.97950828852906\n",
      "Epoch 19411, Loss: 2.722138979915457\n",
      "Epoch 19421, Loss: 2.0774835844387547\n",
      "Epoch 19431, Loss: 3.6253816795904124\n",
      "Epoch 19441, Loss: 2.1348376409344643\n",
      "Epoch 19451, Loss: 2.9694744135292592\n",
      "Epoch 19461, Loss: 2.872907406227172\n",
      "Epoch 19471, Loss: 2.554142859123922\n",
      "Epoch 19481, Loss: 2.4255953954053213\n",
      "Epoch 19491, Loss: 1.864558039272543\n",
      "Evaluating\n",
      "Epoch 19501, Loss: 2.3525781122993425\n",
      "Epoch 19501, Loss: 2.526962732633071\n",
      "Epoch 19511, Loss: 2.728025401375372\n",
      "Epoch 19521, Loss: 2.0080348229053637\n",
      "Epoch 19531, Loss: 3.6737481867151844\n",
      "Epoch 19541, Loss: 1.6803024974272227\n",
      "Epoch 19551, Loss: 2.8543171051564458\n",
      "Epoch 19561, Loss: 2.2270424935647513\n",
      "Epoch 19571, Loss: 2.649194424948204\n",
      "Epoch 19581, Loss: 2.515489607477595\n",
      "Epoch 19591, Loss: 3.5964882007919488\n",
      "Evaluating\n",
      "Epoch 19601, Loss: 2.4033789485026134\n",
      "Epoch 19601, Loss: 3.4076660884089507\n",
      "Epoch 19611, Loss: 1.8134543696380552\n",
      "Epoch 19621, Loss: 2.875624521311514\n",
      "Epoch 19631, Loss: 3.441998055269274\n",
      "Epoch 19641, Loss: 2.0531395311518383\n",
      "Epoch 19651, Loss: 2.1930131290535835\n",
      "Epoch 19661, Loss: 2.011420011929666\n",
      "Epoch 19671, Loss: 2.1648675604247045\n",
      "Epoch 19681, Loss: 2.1966945292719817\n",
      "Epoch 19691, Loss: 1.7255681872162716\n",
      "Evaluating\n",
      "Epoch 19701, Loss: 2.4141795622808844\n",
      "Epoch 19701, Loss: 2.8604075090522088\n",
      "Epoch 19711, Loss: 3.4542501914771844\n",
      "Epoch 19721, Loss: 2.9931002505747224\n",
      "Epoch 19731, Loss: 3.386150564289684\n",
      "Epoch 19741, Loss: 2.314872572723984\n",
      "Epoch 19751, Loss: 2.2908610456637923\n",
      "Epoch 19761, Loss: 2.093038806250179\n",
      "Epoch 19771, Loss: 1.481772958400671\n",
      "Epoch 19781, Loss: 2.4369663636063086\n",
      "Epoch 19791, Loss: 3.4567138976165594\n",
      "Evaluating\n",
      "Epoch 19801, Loss: 2.29952604500218\n",
      "Epoch 19801, Loss: 1.659249422640312\n",
      "Epoch 19811, Loss: 3.135472669178853\n",
      "Epoch 19821, Loss: 2.7371879667392314\n",
      "Epoch 19831, Loss: 2.8471908793294296\n",
      "Epoch 19841, Loss: 2.6285589633051347\n",
      "Epoch 19851, Loss: 2.4070472766361153\n",
      "Epoch 19861, Loss: 1.9341847014257978\n",
      "Epoch 19871, Loss: 2.686671307758528\n",
      "Epoch 19881, Loss: 2.852524246243818\n",
      "Epoch 19891, Loss: 1.8244981210132554\n",
      "Evaluating\n",
      "Epoch 19901, Loss: 2.3593153370499507\n",
      "Epoch 19901, Loss: 1.082547166963793\n",
      "Epoch 19911, Loss: 2.5133651318529875\n",
      "Epoch 19921, Loss: 2.3419526168121343\n",
      "Epoch 19931, Loss: 2.0398097142743192\n",
      "Epoch 19941, Loss: 2.6368089855484578\n",
      "Epoch 19951, Loss: 3.224949846497887\n",
      "Epoch 19961, Loss: 3.4088186034827035\n",
      "Epoch 19971, Loss: 1.675574642719416\n",
      "Epoch 19981, Loss: 3.1001827915610187\n",
      "Epoch 19991, Loss: 3.0212885494280157\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10000,20000):\n",
    "    # Zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    #random_batch = np.random.randint(5000,6000)\n",
    "    x_data,y_data = get_batch(frame_data_tensor,labels_tensor,batch_size=16)\n",
    "    outputs = model(x_data)\n",
    "    loss = criterion(outputs.squeeze(), y_data)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch%100==0:\n",
    "        print(\"Evaluating\")\n",
    "        eval_batch(test_input,test_output,epoch)\n",
    "    if epoch%10==0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {np.sqrt(loss.item())*max(labeldata)}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7173]], grad_fn=<AddmmBackward0>) tensor(-0.8600)\n",
      "tensor([[-0.4619]], grad_fn=<AddmmBackward0>) tensor(-0.5226)\n",
      "tensor([[-0.0661]], grad_fn=<AddmmBackward0>) tensor(0.0893)\n",
      "tensor([[-0.4872]], grad_fn=<AddmmBackward0>) tensor(-0.4520)\n",
      "tensor([[-0.0105]], grad_fn=<AddmmBackward0>) tensor(0.0200)\n",
      "tensor([[-0.7481]], grad_fn=<AddmmBackward0>) tensor(-0.7905)\n",
      "tensor([[-0.9477]], grad_fn=<AddmmBackward0>) tensor(-0.8508)\n",
      "tensor([[-0.0238]], grad_fn=<AddmmBackward0>) tensor(-0.2766)\n",
      "tensor([[-0.5705]], grad_fn=<AddmmBackward0>) tensor(-0.7100)\n",
      "tensor([[-0.5310]], grad_fn=<AddmmBackward0>) tensor(-0.9158)\n",
      "tensor([[-0.7649]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.6215]], grad_fn=<AddmmBackward0>) tensor(-0.6346)\n",
      "tensor([[-0.6998]], grad_fn=<AddmmBackward0>) tensor(-0.7321)\n",
      "tensor([[0.8169]], grad_fn=<AddmmBackward0>) tensor(0.5803)\n",
      "tensor([[-0.1530]], grad_fn=<AddmmBackward0>) tensor(-0.5388)\n",
      "tensor([[-0.2513]], grad_fn=<AddmmBackward0>) tensor(-0.2215)\n",
      "tensor([[-0.2241]], grad_fn=<AddmmBackward0>) tensor(0.1645)\n",
      "tensor([[-0.9850]], grad_fn=<AddmmBackward0>) tensor(-0.1754)\n",
      "tensor([[-0.5163]], grad_fn=<AddmmBackward0>) tensor(0.2761)\n",
      "tensor([[-0.6757]], grad_fn=<AddmmBackward0>) tensor(-0.4005)\n",
      "tensor([[0.5144]], grad_fn=<AddmmBackward0>) tensor(0.5412)\n",
      "tensor([[-0.4885]], grad_fn=<AddmmBackward0>) tensor(-0.6608)\n",
      "tensor([[-0.3312]], grad_fn=<AddmmBackward0>) tensor(-0.3930)\n",
      "tensor([[-1.0186]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.1852]], grad_fn=<AddmmBackward0>) tensor(-0.1782)\n",
      "tensor([[-0.0307]], grad_fn=<AddmmBackward0>) tensor(-0.1840)\n",
      "tensor([[0.6258]], grad_fn=<AddmmBackward0>) tensor(0.7195)\n",
      "tensor([[-0.1852]], grad_fn=<AddmmBackward0>) tensor(-0.9153)\n",
      "tensor([[-0.0064]], grad_fn=<AddmmBackward0>) tensor(0.5014)\n",
      "tensor([[-0.4796]], grad_fn=<AddmmBackward0>) tensor(-0.2379)\n",
      "tensor([[-0.1538]], grad_fn=<AddmmBackward0>) tensor(0.1282)\n",
      "tensor([[-0.1914]], grad_fn=<AddmmBackward0>) tensor(0.2586)\n",
      "tensor([[0.3252]], grad_fn=<AddmmBackward0>) tensor(0.7847)\n",
      "tensor([[-0.9665]], grad_fn=<AddmmBackward0>) tensor(-0.8845)\n",
      "tensor([[-0.8076]], grad_fn=<AddmmBackward0>) tensor(-0.9745)\n",
      "tensor([[-0.5289]], grad_fn=<AddmmBackward0>) tensor(-0.4540)\n",
      "tensor([[-0.4401]], grad_fn=<AddmmBackward0>) tensor(-0.3127)\n",
      "tensor([[0.9279]], grad_fn=<AddmmBackward0>) tensor(0.3634)\n",
      "tensor([[-0.6849]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.8305]], grad_fn=<AddmmBackward0>) tensor(-0.6890)\n",
      "tensor([[0.1695]], grad_fn=<AddmmBackward0>) tensor(0.6259)\n",
      "tensor([[-0.5198]], grad_fn=<AddmmBackward0>) tensor(-0.1139)\n",
      "tensor([[-0.4840]], grad_fn=<AddmmBackward0>) tensor(-0.5123)\n",
      "tensor([[-0.3713]], grad_fn=<AddmmBackward0>) tensor(-0.9144)\n",
      "tensor([[-0.4136]], grad_fn=<AddmmBackward0>) tensor(0.3277)\n",
      "tensor([[-0.4546]], grad_fn=<AddmmBackward0>) tensor(-0.3756)\n",
      "tensor([[-0.5139]], grad_fn=<AddmmBackward0>) tensor(-0.5538)\n",
      "tensor([[-0.4103]], grad_fn=<AddmmBackward0>) tensor(-0.2499)\n",
      "tensor([[-0.4692]], grad_fn=<AddmmBackward0>) tensor(-0.6656)\n",
      "tensor([[-0.6554]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.2673]], grad_fn=<AddmmBackward0>) tensor(0.5682)\n",
      "tensor([[-0.7054]], grad_fn=<AddmmBackward0>) tensor(-0.5395)\n",
      "tensor([[-0.0743]], grad_fn=<AddmmBackward0>) tensor(0.6859)\n",
      "tensor([[-0.5430]], grad_fn=<AddmmBackward0>) tensor(-0.4253)\n",
      "tensor([[-0.7762]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.4053]], grad_fn=<AddmmBackward0>) tensor(0.7386)\n",
      "tensor([[0.1674]], grad_fn=<AddmmBackward0>) tensor(0.5479)\n",
      "tensor([[-0.7755]], grad_fn=<AddmmBackward0>) tensor(-0.6358)\n",
      "tensor([[-0.7892]], grad_fn=<AddmmBackward0>) tensor(-0.6662)\n",
      "tensor([[-0.7967]], grad_fn=<AddmmBackward0>) tensor(-0.7943)\n",
      "tensor([[0.7861]], grad_fn=<AddmmBackward0>) tensor(0.7307)\n",
      "tensor([[-0.6187]], grad_fn=<AddmmBackward0>) tensor(-0.7451)\n",
      "tensor([[-0.1866]], grad_fn=<AddmmBackward0>) tensor(-0.2210)\n",
      "tensor([[0.2327]], grad_fn=<AddmmBackward0>) tensor(0.2360)\n",
      "tensor([[-0.4649]], grad_fn=<AddmmBackward0>) tensor(-0.2906)\n",
      "tensor([[0.1432]], grad_fn=<AddmmBackward0>) tensor(0.2649)\n",
      "tensor([[0.3918]], grad_fn=<AddmmBackward0>) tensor(0.6430)\n",
      "tensor([[-0.7586]], grad_fn=<AddmmBackward0>) tensor(-0.9265)\n",
      "tensor([[-0.6318]], grad_fn=<AddmmBackward0>) tensor(-0.6209)\n",
      "tensor([[0.4563]], grad_fn=<AddmmBackward0>) tensor(0.8322)\n",
      "tensor([[-0.2300]], grad_fn=<AddmmBackward0>) tensor(-0.0305)\n",
      "tensor([[0.4009]], grad_fn=<AddmmBackward0>) tensor(0.5512)\n",
      "tensor([[-0.7833]], grad_fn=<AddmmBackward0>) tensor(-0.6919)\n",
      "tensor([[0.0941]], grad_fn=<AddmmBackward0>) tensor(0.0198)\n",
      "tensor([[-0.7118]], grad_fn=<AddmmBackward0>) tensor(-0.6492)\n",
      "tensor([[-0.4027]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.5314]], grad_fn=<AddmmBackward0>) tensor(0.0528)\n",
      "tensor([[-0.3589]], grad_fn=<AddmmBackward0>) tensor(0.3225)\n",
      "tensor([[-0.2029]], grad_fn=<AddmmBackward0>) tensor(-0.3201)\n",
      "tensor([[0.0085]], grad_fn=<AddmmBackward0>) tensor(-0.1769)\n",
      "tensor([[-0.8404]], grad_fn=<AddmmBackward0>) tensor(-0.6535)\n",
      "tensor([[0.3525]], grad_fn=<AddmmBackward0>) tensor(0.7128)\n",
      "tensor([[-0.8055]], grad_fn=<AddmmBackward0>) tensor(-0.7045)\n",
      "tensor([[-0.4902]], grad_fn=<AddmmBackward0>) tensor(-0.7773)\n",
      "tensor([[-0.8127]], grad_fn=<AddmmBackward0>) tensor(-0.8734)\n",
      "tensor([[0.3743]], grad_fn=<AddmmBackward0>) tensor(0.7222)\n",
      "tensor([[0.4277]], grad_fn=<AddmmBackward0>) tensor(0.5514)\n",
      "tensor([[-0.9458]], grad_fn=<AddmmBackward0>) tensor(-0.8170)\n",
      "tensor([[-0.5787]], grad_fn=<AddmmBackward0>) tensor(-0.5287)\n",
      "tensor([[-0.6625]], grad_fn=<AddmmBackward0>) tensor(-0.5444)\n",
      "tensor([[-0.7321]], grad_fn=<AddmmBackward0>) tensor(-0.6020)\n",
      "tensor([[-0.9345]], grad_fn=<AddmmBackward0>) tensor(-0.7764)\n",
      "tensor([[-0.3329]], grad_fn=<AddmmBackward0>) tensor(-0.2671)\n",
      "tensor([[-0.3763]], grad_fn=<AddmmBackward0>) tensor(-0.9633)\n",
      "tensor([[0.1064]], grad_fn=<AddmmBackward0>) tensor(0.2406)\n",
      "tensor([[0.1263]], grad_fn=<AddmmBackward0>) tensor(0.5764)\n",
      "tensor([[-0.3588]], grad_fn=<AddmmBackward0>) tensor(-0.9636)\n",
      "tensor([[0.2240]], grad_fn=<AddmmBackward0>) tensor(0.5357)\n",
      "tensor([[-0.5441]], grad_fn=<AddmmBackward0>) tensor(-0.1178)\n",
      "tensor([[-0.6973]], grad_fn=<AddmmBackward0>) tensor(-0.6040)\n",
      "tensor([[-0.5256]], grad_fn=<AddmmBackward0>) tensor(-0.5686)\n",
      "tensor([[-0.1014]], grad_fn=<AddmmBackward0>) tensor(0.3568)\n",
      "tensor([[0.3771]], grad_fn=<AddmmBackward0>) tensor(0.5663)\n",
      "tensor([[0.0624]], grad_fn=<AddmmBackward0>) tensor(0.6758)\n",
      "tensor([[0.6342]], grad_fn=<AddmmBackward0>) tensor(0.6934)\n",
      "tensor([[0.1984]], grad_fn=<AddmmBackward0>) tensor(0.6724)\n",
      "tensor([[-0.7515]], grad_fn=<AddmmBackward0>) tensor(-0.7397)\n",
      "tensor([[-0.7359]], grad_fn=<AddmmBackward0>) tensor(-0.8173)\n",
      "tensor([[0.0411]], grad_fn=<AddmmBackward0>) tensor(0.0113)\n",
      "tensor([[-0.6751]], grad_fn=<AddmmBackward0>) tensor(-0.8982)\n",
      "tensor([[-0.7754]], grad_fn=<AddmmBackward0>) tensor(-0.4932)\n",
      "tensor([[-0.6882]], grad_fn=<AddmmBackward0>) tensor(-0.4546)\n",
      "tensor([[-1.1369]], grad_fn=<AddmmBackward0>) tensor(-0.4445)\n",
      "tensor([[0.0939]], grad_fn=<AddmmBackward0>) tensor(0.1494)\n",
      "tensor([[-0.3894]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.8248]], grad_fn=<AddmmBackward0>) tensor(-0.8663)\n",
      "tensor([[-0.0387]], grad_fn=<AddmmBackward0>) tensor(0.1212)\n",
      "tensor([[0.7178]], grad_fn=<AddmmBackward0>) tensor(0.6420)\n",
      "tensor([[-0.6414]], grad_fn=<AddmmBackward0>) tensor(-0.5214)\n",
      "tensor([[-0.7778]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.0377]], grad_fn=<AddmmBackward0>) tensor(0.5298)\n",
      "tensor([[-0.6122]], grad_fn=<AddmmBackward0>) tensor(-0.4566)\n",
      "tensor([[-0.8856]], grad_fn=<AddmmBackward0>) tensor(-0.9388)\n",
      "tensor([[-0.8613]], grad_fn=<AddmmBackward0>) tensor(-0.8344)\n",
      "tensor([[-0.4520]], grad_fn=<AddmmBackward0>) tensor(-0.2874)\n",
      "tensor([[-0.1383]], grad_fn=<AddmmBackward0>) tensor(-0.1310)\n",
      "tensor([[-0.7107]], grad_fn=<AddmmBackward0>) tensor(-0.8494)\n",
      "tensor([[-0.8333]], grad_fn=<AddmmBackward0>) tensor(-0.7498)\n",
      "tensor([[-0.5633]], grad_fn=<AddmmBackward0>) tensor(-0.4447)\n",
      "tensor([[-0.1852]], grad_fn=<AddmmBackward0>) tensor(-0.2466)\n",
      "tensor([[-0.8381]], grad_fn=<AddmmBackward0>) tensor(-0.7662)\n",
      "tensor([[0.0253]], grad_fn=<AddmmBackward0>) tensor(0.2658)\n",
      "tensor([[-0.1109]], grad_fn=<AddmmBackward0>) tensor(0.0155)\n",
      "tensor([[0.5879]], grad_fn=<AddmmBackward0>) tensor(0.5882)\n",
      "tensor([[-0.1231]], grad_fn=<AddmmBackward0>) tensor(-0.0216)\n",
      "tensor([[0.1479]], grad_fn=<AddmmBackward0>) tensor(0.1490)\n",
      "tensor([[-0.1423]], grad_fn=<AddmmBackward0>) tensor(-0.2531)\n",
      "tensor([[-0.5711]], grad_fn=<AddmmBackward0>) tensor(-0.4399)\n",
      "tensor([[0.5153]], grad_fn=<AddmmBackward0>) tensor(0.5172)\n",
      "tensor([[0.2384]], grad_fn=<AddmmBackward0>) tensor(0.3840)\n",
      "tensor([[0.3858]], grad_fn=<AddmmBackward0>) tensor(0.7953)\n",
      "tensor([[-0.6686]], grad_fn=<AddmmBackward0>) tensor(-0.8950)\n",
      "tensor([[0.2497]], grad_fn=<AddmmBackward0>) tensor(0.7765)\n",
      "tensor([[0.1533]], grad_fn=<AddmmBackward0>) tensor(0.7223)\n",
      "tensor([[-0.6345]], grad_fn=<AddmmBackward0>) tensor(-0.6258)\n",
      "tensor([[0.2208]], grad_fn=<AddmmBackward0>) tensor(0.6329)\n",
      "tensor([[-0.1574]], grad_fn=<AddmmBackward0>) tensor(-0.4490)\n",
      "tensor([[0.4948]], grad_fn=<AddmmBackward0>) tensor(0.5138)\n",
      "tensor([[0.7270]], grad_fn=<AddmmBackward0>) tensor(0.7007)\n",
      "tensor([[-0.1146]], grad_fn=<AddmmBackward0>) tensor(-0.3186)\n",
      "tensor([[-0.7965]], grad_fn=<AddmmBackward0>) tensor(-0.5834)\n",
      "tensor([[0.1523]], grad_fn=<AddmmBackward0>) tensor(0.0206)\n",
      "tensor([[-0.8326]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.4057]], grad_fn=<AddmmBackward0>) tensor(-0.0972)\n",
      "tensor([[-0.6748]], grad_fn=<AddmmBackward0>) tensor(-0.9681)\n",
      "tensor([[0.7473]], grad_fn=<AddmmBackward0>) tensor(0.5758)\n",
      "tensor([[0.1896]], grad_fn=<AddmmBackward0>) tensor(0.5224)\n",
      "tensor([[0.3742]], grad_fn=<AddmmBackward0>) tensor(0.5294)\n",
      "tensor([[-0.3973]], grad_fn=<AddmmBackward0>) tensor(-0.3740)\n",
      "tensor([[0.4704]], grad_fn=<AddmmBackward0>) tensor(-0.3802)\n",
      "tensor([[-0.5688]], grad_fn=<AddmmBackward0>) tensor(-0.4958)\n",
      "tensor([[-0.5533]], grad_fn=<AddmmBackward0>) tensor(-0.7654)\n",
      "tensor([[-0.1667]], grad_fn=<AddmmBackward0>) tensor(-0.1018)\n",
      "tensor([[-0.1336]], grad_fn=<AddmmBackward0>) tensor(-0.1468)\n",
      "tensor([[-0.9561]], grad_fn=<AddmmBackward0>) tensor(-0.8923)\n",
      "tensor([[-0.8813]], grad_fn=<AddmmBackward0>) tensor(-0.8406)\n",
      "tensor([[-0.6501]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.5960]], grad_fn=<AddmmBackward0>) tensor(-0.7050)\n",
      "tensor([[-0.8573]], grad_fn=<AddmmBackward0>) tensor(-0.8532)\n",
      "tensor([[0.1083]], grad_fn=<AddmmBackward0>) tensor(0.4833)\n",
      "tensor([[0.6318]], grad_fn=<AddmmBackward0>) tensor(0.5718)\n",
      "tensor([[0.2160]], grad_fn=<AddmmBackward0>) tensor(0.2228)\n",
      "tensor([[-0.4665]], grad_fn=<AddmmBackward0>) tensor(0.0048)\n",
      "tensor([[-0.4393]], grad_fn=<AddmmBackward0>) tensor(-0.4811)\n",
      "tensor([[-0.1790]], grad_fn=<AddmmBackward0>) tensor(-0.0182)\n",
      "tensor([[-0.0360]], grad_fn=<AddmmBackward0>) tensor(0.5698)\n",
      "tensor([[-0.5220]], grad_fn=<AddmmBackward0>) tensor(0.0614)\n",
      "tensor([[0.9264]], grad_fn=<AddmmBackward0>) tensor(0.8333)\n",
      "tensor([[0.3247]], grad_fn=<AddmmBackward0>) tensor(0.7206)\n",
      "tensor([[0.2163]], grad_fn=<AddmmBackward0>) tensor(0.5485)\n",
      "tensor([[0.0655]], grad_fn=<AddmmBackward0>) tensor(0.0407)\n",
      "tensor([[0.2476]], grad_fn=<AddmmBackward0>) tensor(0.5092)\n",
      "tensor([[0.4537]], grad_fn=<AddmmBackward0>) tensor(0.5455)\n",
      "tensor([[-1.0033]], grad_fn=<AddmmBackward0>) tensor(-0.5878)\n",
      "tensor([[0.6468]], grad_fn=<AddmmBackward0>) tensor(0.5910)\n",
      "tensor([[0.2636]], grad_fn=<AddmmBackward0>) tensor(0.5168)\n",
      "tensor([[-0.0511]], grad_fn=<AddmmBackward0>) tensor(-0.0002)\n",
      "tensor([[-0.5078]], grad_fn=<AddmmBackward0>) tensor(-0.2545)\n",
      "tensor([[0.2775]], grad_fn=<AddmmBackward0>) tensor(0.5542)\n",
      "tensor([[0.0143]], grad_fn=<AddmmBackward0>) tensor(0.1430)\n",
      "tensor([[-0.6743]], grad_fn=<AddmmBackward0>) tensor(-0.6510)\n",
      "tensor([[-0.6764]], grad_fn=<AddmmBackward0>) tensor(-0.6488)\n",
      "tensor([[-0.0582]], grad_fn=<AddmmBackward0>) tensor(0.1326)\n",
      "tensor([[-0.6870]], grad_fn=<AddmmBackward0>) tensor(-0.8874)\n",
      "tensor([[0.1763]], grad_fn=<AddmmBackward0>) tensor(0.5681)\n",
      "tensor([[0.2488]], grad_fn=<AddmmBackward0>) tensor(0.3700)\n",
      "tensor([[-0.3586]], grad_fn=<AddmmBackward0>) tensor(-0.4533)\n",
      "tensor([[0.5435]], grad_fn=<AddmmBackward0>) tensor(0.7646)\n",
      "tensor([[-0.2229]], grad_fn=<AddmmBackward0>) tensor(-0.0123)\n",
      "tensor([[-0.4602]], grad_fn=<AddmmBackward0>) tensor(-0.5978)\n",
      "tensor([[-0.6480]], grad_fn=<AddmmBackward0>) tensor(-0.6701)\n",
      "tensor([[0.4350]], grad_fn=<AddmmBackward0>) tensor(0.8309)\n",
      "tensor([[-0.4790]], grad_fn=<AddmmBackward0>) tensor(-0.4736)\n",
      "tensor([[-1.0640]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.4110]], grad_fn=<AddmmBackward0>) tensor(-0.3746)\n",
      "tensor([[-0.5371]], grad_fn=<AddmmBackward0>) tensor(-0.0335)\n",
      "tensor([[-0.6657]], grad_fn=<AddmmBackward0>) tensor(-0.5667)\n",
      "tensor([[0.5297]], grad_fn=<AddmmBackward0>) tensor(0.5708)\n",
      "tensor([[0.5134]], grad_fn=<AddmmBackward0>) tensor(0.5746)\n",
      "tensor([[-0.2701]], grad_fn=<AddmmBackward0>) tensor(-0.9234)\n",
      "tensor([[0.3085]], grad_fn=<AddmmBackward0>) tensor(0.5226)\n",
      "tensor([[-0.7286]], grad_fn=<AddmmBackward0>) tensor(-0.8441)\n",
      "tensor([[-0.0806]], grad_fn=<AddmmBackward0>) tensor(-0.3506)\n",
      "tensor([[0.0044]], grad_fn=<AddmmBackward0>) tensor(0.3802)\n",
      "tensor([[-0.6671]], grad_fn=<AddmmBackward0>) tensor(0.4945)\n",
      "tensor([[0.7699]], grad_fn=<AddmmBackward0>) tensor(0.8489)\n",
      "tensor([[-0.1641]], grad_fn=<AddmmBackward0>) tensor(0.5099)\n",
      "tensor([[-0.7862]], grad_fn=<AddmmBackward0>) tensor(-0.4976)\n",
      "tensor([[-0.1573]], grad_fn=<AddmmBackward0>) tensor(-0.3167)\n",
      "tensor([[-1.0592]], grad_fn=<AddmmBackward0>) tensor(-0.4590)\n",
      "tensor([[-0.6954]], grad_fn=<AddmmBackward0>) tensor(-0.4810)\n",
      "tensor([[0.3808]], grad_fn=<AddmmBackward0>) tensor(0.8052)\n",
      "tensor([[0.6205]], grad_fn=<AddmmBackward0>) tensor(0.8426)\n",
      "tensor([[-0.7431]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.0470]], grad_fn=<AddmmBackward0>) tensor(0.3061)\n",
      "tensor([[0.2119]], grad_fn=<AddmmBackward0>) tensor(0.3583)\n",
      "tensor([[0.3270]], grad_fn=<AddmmBackward0>) tensor(0.5344)\n",
      "tensor([[0.3693]], grad_fn=<AddmmBackward0>) tensor(0.6832)\n",
      "tensor([[-0.7463]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.1684]], grad_fn=<AddmmBackward0>) tensor(-0.8643)\n",
      "tensor([[-0.7191]], grad_fn=<AddmmBackward0>) tensor(-0.4698)\n",
      "tensor([[-0.5965]], grad_fn=<AddmmBackward0>) tensor(-0.6207)\n",
      "tensor([[0.4042]], grad_fn=<AddmmBackward0>) tensor(0.7210)\n",
      "tensor([[0.6286]], grad_fn=<AddmmBackward0>) tensor(0.5917)\n",
      "tensor([[0.4371]], grad_fn=<AddmmBackward0>) tensor(0.6785)\n",
      "tensor([[-0.0619]], grad_fn=<AddmmBackward0>) tensor(-0.2821)\n",
      "tensor([[0.2730]], grad_fn=<AddmmBackward0>) tensor(0.1774)\n",
      "tensor([[0.6031]], grad_fn=<AddmmBackward0>) tensor(0.4057)\n",
      "tensor([[-0.4330]], grad_fn=<AddmmBackward0>) tensor(-0.1029)\n",
      "tensor([[-0.9558]], grad_fn=<AddmmBackward0>) tensor(-0.9621)\n",
      "tensor([[0.2748]], grad_fn=<AddmmBackward0>) tensor(0.5235)\n",
      "tensor([[-0.3138]], grad_fn=<AddmmBackward0>) tensor(-0.4613)\n",
      "tensor([[-0.7001]], grad_fn=<AddmmBackward0>) tensor(-0.9507)\n",
      "tensor([[-0.5353]], grad_fn=<AddmmBackward0>) tensor(-0.5234)\n",
      "tensor([[-0.7266]], grad_fn=<AddmmBackward0>) tensor(-0.6867)\n",
      "tensor([[-0.0921]], grad_fn=<AddmmBackward0>) tensor(0.0191)\n",
      "tensor([[-0.6491]], grad_fn=<AddmmBackward0>) tensor(-0.7575)\n",
      "tensor([[-0.0717]], grad_fn=<AddmmBackward0>) tensor(-0.9073)\n",
      "tensor([[0.3885]], grad_fn=<AddmmBackward0>) tensor(0.6515)\n",
      "tensor([[-0.5479]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.5935]], grad_fn=<AddmmBackward0>) tensor(-0.3373)\n",
      "tensor([[-0.8258]], grad_fn=<AddmmBackward0>) tensor(-0.9189)\n",
      "tensor([[0.2444]], grad_fn=<AddmmBackward0>) tensor(0.8479)\n",
      "tensor([[0.3906]], grad_fn=<AddmmBackward0>) tensor(0.2339)\n",
      "tensor([[-0.7647]], grad_fn=<AddmmBackward0>) tensor(-0.8604)\n",
      "tensor([[0.2237]], grad_fn=<AddmmBackward0>) tensor(0.7031)\n",
      "tensor([[-0.6544]], grad_fn=<AddmmBackward0>) tensor(-0.8891)\n",
      "tensor([[-0.7977]], grad_fn=<AddmmBackward0>) tensor(-0.8492)\n",
      "tensor([[0.2965]], grad_fn=<AddmmBackward0>) tensor(0.5199)\n",
      "tensor([[-0.8558]], grad_fn=<AddmmBackward0>) tensor(-0.7908)\n",
      "tensor([[0.4373]], grad_fn=<AddmmBackward0>) tensor(0.6596)\n",
      "tensor([[0.1062]], grad_fn=<AddmmBackward0>) tensor(0.2568)\n",
      "tensor([[0.0448]], grad_fn=<AddmmBackward0>) tensor(0.0107)\n",
      "tensor([[-0.6997]], grad_fn=<AddmmBackward0>) tensor(-0.4721)\n",
      "tensor([[0.5924]], grad_fn=<AddmmBackward0>) tensor(0.5951)\n",
      "tensor([[0.1655]], grad_fn=<AddmmBackward0>) tensor(0.1900)\n",
      "tensor([[-0.6477]], grad_fn=<AddmmBackward0>) tensor(-0.7096)\n",
      "tensor([[0.3716]], grad_fn=<AddmmBackward0>) tensor(0.5689)\n",
      "tensor([[-0.3822]], grad_fn=<AddmmBackward0>) tensor(-0.2942)\n",
      "tensor([[-0.1449]], grad_fn=<AddmmBackward0>) tensor(-0.1308)\n",
      "tensor([[0.2236]], grad_fn=<AddmmBackward0>) tensor(0.5097)\n",
      "tensor([[-1.0476]], grad_fn=<AddmmBackward0>) tensor(-0.9394)\n",
      "tensor([[-0.0353]], grad_fn=<AddmmBackward0>) tensor(0.4483)\n",
      "tensor([[0.3646]], grad_fn=<AddmmBackward0>) tensor(0.0635)\n",
      "tensor([[0.3045]], grad_fn=<AddmmBackward0>) tensor(0.6943)\n",
      "tensor([[0.3737]], grad_fn=<AddmmBackward0>) tensor(0.6272)\n",
      "tensor([[0.3917]], grad_fn=<AddmmBackward0>) tensor(0.6695)\n",
      "tensor([[-0.6362]], grad_fn=<AddmmBackward0>) tensor(-0.3737)\n",
      "tensor([[-0.4136]], grad_fn=<AddmmBackward0>) tensor(0.6942)\n",
      "tensor([[-1.0387]], grad_fn=<AddmmBackward0>) tensor(-0.7790)\n",
      "tensor([[0.1692]], grad_fn=<AddmmBackward0>) tensor(0.2954)\n",
      "tensor([[-0.9635]], grad_fn=<AddmmBackward0>) tensor(-0.9374)\n",
      "tensor([[0.0445]], grad_fn=<AddmmBackward0>) tensor(0.4742)\n",
      "tensor([[-0.5650]], grad_fn=<AddmmBackward0>) tensor(-0.2382)\n",
      "tensor([[-0.6256]], grad_fn=<AddmmBackward0>) tensor(-0.6060)\n",
      "tensor([[0.5382]], grad_fn=<AddmmBackward0>) tensor(0.5421)\n",
      "tensor([[0.2662]], grad_fn=<AddmmBackward0>) tensor(0.7387)\n",
      "tensor([[0.1900]], grad_fn=<AddmmBackward0>) tensor(0.6079)\n",
      "tensor([[0.3411]], grad_fn=<AddmmBackward0>) tensor(0.5361)\n",
      "tensor([[0.4586]], grad_fn=<AddmmBackward0>) tensor(0.7448)\n",
      "tensor([[-0.3657]], grad_fn=<AddmmBackward0>) tensor(-0.2689)\n",
      "tensor([[0.4977]], grad_fn=<AddmmBackward0>) tensor(0.7308)\n",
      "tensor([[-1.1202]], grad_fn=<AddmmBackward0>) tensor(-0.8716)\n",
      "tensor([[0.8654]], grad_fn=<AddmmBackward0>) tensor(0.7930)\n",
      "tensor([[0.9011]], grad_fn=<AddmmBackward0>) tensor(0.5906)\n",
      "tensor([[-0.6473]], grad_fn=<AddmmBackward0>) tensor(-0.9928)\n",
      "tensor([[-0.1040]], grad_fn=<AddmmBackward0>) tensor(-0.8104)\n",
      "tensor([[-0.1253]], grad_fn=<AddmmBackward0>) tensor(0.6946)\n",
      "tensor([[-0.6549]], grad_fn=<AddmmBackward0>) tensor(-0.7209)\n",
      "tensor([[0.3984]], grad_fn=<AddmmBackward0>) tensor(-0.1349)\n",
      "tensor([[-0.5906]], grad_fn=<AddmmBackward0>) tensor(-0.9247)\n",
      "tensor([[-0.7584]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.3222]], grad_fn=<AddmmBackward0>) tensor(-0.1466)\n",
      "tensor([[0.4180]], grad_fn=<AddmmBackward0>) tensor(0.5506)\n",
      "tensor([[-0.7913]], grad_fn=<AddmmBackward0>) tensor(-0.8485)\n",
      "tensor([[-0.1128]], grad_fn=<AddmmBackward0>) tensor(-0.3161)\n",
      "tensor([[0.3684]], grad_fn=<AddmmBackward0>) tensor(0.7432)\n",
      "tensor([[-0.7692]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.5173]], grad_fn=<AddmmBackward0>) tensor(-0.9145)\n",
      "tensor([[-0.6115]], grad_fn=<AddmmBackward0>) tensor(-0.6986)\n",
      "tensor([[-0.5770]], grad_fn=<AddmmBackward0>) tensor(-0.4955)\n",
      "tensor([[-0.7002]], grad_fn=<AddmmBackward0>) tensor(-0.8476)\n",
      "tensor([[-0.4766]], grad_fn=<AddmmBackward0>) tensor(0.1617)\n",
      "tensor([[-0.4390]], grad_fn=<AddmmBackward0>) tensor(-0.2789)\n",
      "tensor([[-0.1485]], grad_fn=<AddmmBackward0>) tensor(0.0495)\n",
      "tensor([[-0.6756]], grad_fn=<AddmmBackward0>) tensor(-0.8883)\n",
      "tensor([[0.5016]], grad_fn=<AddmmBackward0>) tensor(0.5148)\n",
      "tensor([[-0.3012]], grad_fn=<AddmmBackward0>) tensor(-0.1270)\n",
      "tensor([[-0.6180]], grad_fn=<AddmmBackward0>) tensor(-0.4783)\n",
      "tensor([[0.0269]], grad_fn=<AddmmBackward0>) tensor(-0.2710)\n",
      "tensor([[0.8323]], grad_fn=<AddmmBackward0>) tensor(0.7579)\n",
      "tensor([[0.8036]], grad_fn=<AddmmBackward0>) tensor(0.5624)\n",
      "tensor([[0.0866]], grad_fn=<AddmmBackward0>) tensor(0.6257)\n",
      "tensor([[-0.1919]], grad_fn=<AddmmBackward0>) tensor(0.0233)\n",
      "tensor([[-0.5680]], grad_fn=<AddmmBackward0>) tensor(-0.6059)\n",
      "tensor([[-0.2916]], grad_fn=<AddmmBackward0>) tensor(0.3048)\n",
      "tensor([[-0.7440]], grad_fn=<AddmmBackward0>) tensor(-0.8238)\n",
      "tensor([[-0.7107]], grad_fn=<AddmmBackward0>) tensor(-0.8057)\n",
      "tensor([[-0.6410]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.6649]], grad_fn=<AddmmBackward0>) tensor(-0.2260)\n",
      "tensor([[0.1753]], grad_fn=<AddmmBackward0>) tensor(0.3086)\n",
      "tensor([[0.1253]], grad_fn=<AddmmBackward0>) tensor(0.5133)\n",
      "tensor([[-1.0903]], grad_fn=<AddmmBackward0>) tensor(-0.9141)\n",
      "tensor([[-0.8621]], grad_fn=<AddmmBackward0>) tensor(-0.6466)\n",
      "tensor([[0.0064]], grad_fn=<AddmmBackward0>) tensor(-0.0951)\n",
      "tensor([[-0.5065]], grad_fn=<AddmmBackward0>) tensor(-0.9614)\n",
      "tensor([[0.7138]], grad_fn=<AddmmBackward0>) tensor(0.7138)\n",
      "tensor([[-0.8021]], grad_fn=<AddmmBackward0>) tensor(-0.8333)\n",
      "tensor([[-0.3824]], grad_fn=<AddmmBackward0>) tensor(-0.2457)\n",
      "tensor([[0.4207]], grad_fn=<AddmmBackward0>) tensor(0.8041)\n",
      "tensor([[-0.4791]], grad_fn=<AddmmBackward0>) tensor(-0.8215)\n",
      "tensor([[-0.3790]], grad_fn=<AddmmBackward0>) tensor(-0.0909)\n",
      "tensor([[-0.3251]], grad_fn=<AddmmBackward0>) tensor(0.1326)\n",
      "tensor([[-0.4667]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.4204]], grad_fn=<AddmmBackward0>) tensor(0.5717)\n",
      "tensor([[-0.0420]], grad_fn=<AddmmBackward0>) tensor(0.5776)\n",
      "tensor([[0.7700]], grad_fn=<AddmmBackward0>) tensor(0.9608)\n",
      "tensor([[-0.5552]], grad_fn=<AddmmBackward0>) tensor(-0.4384)\n",
      "tensor([[-0.3588]], grad_fn=<AddmmBackward0>) tensor(-0.4208)\n",
      "tensor([[-0.6158]], grad_fn=<AddmmBackward0>) tensor(-0.5841)\n",
      "tensor([[-0.5265]], grad_fn=<AddmmBackward0>) tensor(-0.6512)\n",
      "tensor([[0.4412]], grad_fn=<AddmmBackward0>) tensor(0.1350)\n",
      "tensor([[-0.2326]], grad_fn=<AddmmBackward0>) tensor(-0.2450)\n",
      "tensor([[-0.5685]], grad_fn=<AddmmBackward0>) tensor(-0.7075)\n",
      "tensor([[-0.3418]], grad_fn=<AddmmBackward0>) tensor(-0.8387)\n",
      "tensor([[0.2839]], grad_fn=<AddmmBackward0>) tensor(0.1790)\n",
      "tensor([[0.5976]], grad_fn=<AddmmBackward0>) tensor(0.6435)\n",
      "tensor([[-0.4509]], grad_fn=<AddmmBackward0>) tensor(-0.2226)\n",
      "tensor([[-0.6614]], grad_fn=<AddmmBackward0>) tensor(-0.7537)\n",
      "tensor([[-0.4722]], grad_fn=<AddmmBackward0>) tensor(-0.6643)\n",
      "tensor([[-0.8476]], grad_fn=<AddmmBackward0>) tensor(-0.8211)\n",
      "tensor([[-0.0375]], grad_fn=<AddmmBackward0>) tensor(-0.1647)\n",
      "tensor([[-0.0495]], grad_fn=<AddmmBackward0>) tensor(-0.2942)\n",
      "tensor([[-0.6555]], grad_fn=<AddmmBackward0>) tensor(-0.7010)\n",
      "tensor([[-0.5278]], grad_fn=<AddmmBackward0>) tensor(-0.5183)\n",
      "tensor([[0.4795]], grad_fn=<AddmmBackward0>) tensor(0.5741)\n",
      "tensor([[-0.0331]], grad_fn=<AddmmBackward0>) tensor(-0.8594)\n",
      "tensor([[-0.6977]], grad_fn=<AddmmBackward0>) tensor(-0.7324)\n",
      "tensor([[-0.0847]], grad_fn=<AddmmBackward0>) tensor(0.0192)\n",
      "tensor([[-0.7204]], grad_fn=<AddmmBackward0>) tensor(-0.7941)\n",
      "tensor([[0.1433]], grad_fn=<AddmmBackward0>) tensor(0.8503)\n",
      "tensor([[-0.6960]], grad_fn=<AddmmBackward0>) tensor(-0.8825)\n",
      "tensor([[-0.7529]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.0388]], grad_fn=<AddmmBackward0>) tensor(0.3698)\n",
      "tensor([[-0.0927]], grad_fn=<AddmmBackward0>) tensor(-0.0604)\n",
      "tensor([[-0.1109]], grad_fn=<AddmmBackward0>) tensor(-0.3148)\n",
      "tensor([[0.8156]], grad_fn=<AddmmBackward0>) tensor(0.5147)\n",
      "tensor([[-0.8565]], grad_fn=<AddmmBackward0>) tensor(-0.5923)\n",
      "tensor([[0.4682]], grad_fn=<AddmmBackward0>) tensor(0.5677)\n",
      "tensor([[-0.9052]], grad_fn=<AddmmBackward0>) tensor(-0.8267)\n",
      "tensor([[0.0796]], grad_fn=<AddmmBackward0>) tensor(0.4289)\n",
      "tensor([[-1.0633]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.0971]], grad_fn=<AddmmBackward0>) tensor(0.3460)\n",
      "tensor([[-0.8406]], grad_fn=<AddmmBackward0>) tensor(-0.7892)\n",
      "tensor([[-0.5694]], grad_fn=<AddmmBackward0>) tensor(-0.2932)\n",
      "tensor([[-0.6254]], grad_fn=<AddmmBackward0>) tensor(-0.9611)\n",
      "tensor([[0.8638]], grad_fn=<AddmmBackward0>) tensor(0.8143)\n",
      "tensor([[-0.8019]], grad_fn=<AddmmBackward0>) tensor(-0.6960)\n",
      "tensor([[-0.6919]], grad_fn=<AddmmBackward0>) tensor(-0.9115)\n",
      "tensor([[-0.7787]], grad_fn=<AddmmBackward0>) tensor(-0.9295)\n",
      "tensor([[0.3592]], grad_fn=<AddmmBackward0>) tensor(0.5632)\n",
      "tensor([[0.4429]], grad_fn=<AddmmBackward0>) tensor(0.7084)\n",
      "tensor([[-0.4205]], grad_fn=<AddmmBackward0>) tensor(-0.3400)\n",
      "tensor([[-0.7300]], grad_fn=<AddmmBackward0>) tensor(-0.4678)\n",
      "tensor([[-0.6533]], grad_fn=<AddmmBackward0>) tensor(-0.9741)\n",
      "tensor([[-0.8143]], grad_fn=<AddmmBackward0>) tensor(-0.8962)\n",
      "tensor([[-0.6214]], grad_fn=<AddmmBackward0>) tensor(-0.2475)\n",
      "tensor([[-0.5909]], grad_fn=<AddmmBackward0>) tensor(-0.4684)\n",
      "tensor([[-0.0434]], grad_fn=<AddmmBackward0>) tensor(0.7825)\n",
      "tensor([[-0.0527]], grad_fn=<AddmmBackward0>) tensor(-0.3153)\n",
      "tensor([[-0.4139]], grad_fn=<AddmmBackward0>) tensor(-0.2763)\n",
      "tensor([[-0.7764]], grad_fn=<AddmmBackward0>) tensor(-0.6756)\n",
      "tensor([[-0.8072]], grad_fn=<AddmmBackward0>) tensor(-0.4377)\n",
      "tensor([[-0.0334]], grad_fn=<AddmmBackward0>) tensor(-0.1445)\n",
      "tensor([[-0.4950]], grad_fn=<AddmmBackward0>) tensor(-0.4740)\n",
      "tensor([[0.4847]], grad_fn=<AddmmBackward0>) tensor(0.5593)\n",
      "tensor([[-0.5866]], grad_fn=<AddmmBackward0>) tensor(-0.9237)\n",
      "tensor([[0.9317]], grad_fn=<AddmmBackward0>) tensor(0.7099)\n",
      "tensor([[-1.1249]], grad_fn=<AddmmBackward0>) tensor(-0.3687)\n",
      "tensor([[0.2618]], grad_fn=<AddmmBackward0>) tensor(0.5158)\n",
      "tensor([[-0.7830]], grad_fn=<AddmmBackward0>) tensor(-0.4960)\n",
      "tensor([[-0.1242]], grad_fn=<AddmmBackward0>) tensor(0.1142)\n",
      "tensor([[-0.5224]], grad_fn=<AddmmBackward0>) tensor(-0.4296)\n",
      "tensor([[-0.7008]], grad_fn=<AddmmBackward0>) tensor(-0.6412)\n",
      "tensor([[-0.6218]], grad_fn=<AddmmBackward0>) tensor(-0.4453)\n",
      "tensor([[-0.4636]], grad_fn=<AddmmBackward0>) tensor(-0.9708)\n",
      "tensor([[-0.5030]], grad_fn=<AddmmBackward0>) tensor(-0.0915)\n",
      "tensor([[-0.6038]], grad_fn=<AddmmBackward0>) tensor(-0.6397)\n",
      "tensor([[-0.6255]], grad_fn=<AddmmBackward0>) tensor(-0.5104)\n",
      "tensor([[-0.2200]], grad_fn=<AddmmBackward0>) tensor(0.1585)\n",
      "tensor([[0.4162]], grad_fn=<AddmmBackward0>) tensor(0.7335)\n",
      "tensor([[0.3923]], grad_fn=<AddmmBackward0>) tensor(0.7872)\n",
      "tensor([[0.0729]], grad_fn=<AddmmBackward0>) tensor(0.2860)\n",
      "tensor([[0.1547]], grad_fn=<AddmmBackward0>) tensor(0.6171)\n",
      "tensor([[-0.2496]], grad_fn=<AddmmBackward0>) tensor(0.1332)\n",
      "tensor([[0.5736]], grad_fn=<AddmmBackward0>) tensor(0.7929)\n",
      "tensor([[0.3675]], grad_fn=<AddmmBackward0>) tensor(0.5800)\n",
      "tensor([[-0.2484]], grad_fn=<AddmmBackward0>) tensor(0.1234)\n",
      "tensor([[-0.5031]], grad_fn=<AddmmBackward0>) tensor(-0.7316)\n",
      "tensor([[-0.6846]], grad_fn=<AddmmBackward0>) tensor(-0.7900)\n",
      "tensor([[-0.3804]], grad_fn=<AddmmBackward0>) tensor(-0.4792)\n",
      "tensor([[-0.5490]], grad_fn=<AddmmBackward0>) tensor(-0.4000)\n",
      "tensor([[-0.0015]], grad_fn=<AddmmBackward0>) tensor(0.7521)\n",
      "tensor([[-0.3928]], grad_fn=<AddmmBackward0>) tensor(-0.6968)\n",
      "tensor([[-0.4251]], grad_fn=<AddmmBackward0>) tensor(-0.6321)\n",
      "tensor([[0.8253]], grad_fn=<AddmmBackward0>) tensor(0.6922)\n",
      "tensor([[0.2519]], grad_fn=<AddmmBackward0>) tensor(0.6217)\n",
      "tensor([[-0.9724]], grad_fn=<AddmmBackward0>) tensor(-0.4394)\n",
      "tensor([[0.6401]], grad_fn=<AddmmBackward0>) tensor(0.6009)\n",
      "tensor([[-1.0231]], grad_fn=<AddmmBackward0>) tensor(-0.6873)\n",
      "tensor([[-0.6724]], grad_fn=<AddmmBackward0>) tensor(-0.4774)\n",
      "tensor([[-0.5394]], grad_fn=<AddmmBackward0>) tensor(-0.3799)\n",
      "tensor([[-0.5696]], grad_fn=<AddmmBackward0>) tensor(-0.5157)\n",
      "tensor([[0.0802]], grad_fn=<AddmmBackward0>) tensor(-0.1456)\n",
      "tensor([[-0.7387]], grad_fn=<AddmmBackward0>) tensor(-0.1253)\n",
      "tensor([[0.6855]], grad_fn=<AddmmBackward0>) tensor(0.8278)\n",
      "tensor([[-0.5093]], grad_fn=<AddmmBackward0>) tensor(-0.3282)\n",
      "tensor([[-0.6848]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.0041]], grad_fn=<AddmmBackward0>) tensor(-0.0114)\n",
      "tensor([[0.0823]], grad_fn=<AddmmBackward0>) tensor(0.2367)\n",
      "tensor([[0.3695]], grad_fn=<AddmmBackward0>) tensor(0.5548)\n",
      "tensor([[0.5117]], grad_fn=<AddmmBackward0>) tensor(0.8058)\n",
      "tensor([[0.4825]], grad_fn=<AddmmBackward0>) tensor(0.7261)\n",
      "tensor([[-0.2973]], grad_fn=<AddmmBackward0>) tensor(-0.4137)\n",
      "tensor([[-0.0191]], grad_fn=<AddmmBackward0>) tensor(0.2602)\n",
      "tensor([[0.0056]], grad_fn=<AddmmBackward0>) tensor(0.5954)\n",
      "tensor([[-0.4389]], grad_fn=<AddmmBackward0>) tensor(-0.4841)\n",
      "tensor([[-0.6136]], grad_fn=<AddmmBackward0>) tensor(-0.3652)\n",
      "tensor([[-0.6800]], grad_fn=<AddmmBackward0>) tensor(-0.3702)\n",
      "tensor([[-0.6457]], grad_fn=<AddmmBackward0>) tensor(-0.8904)\n",
      "tensor([[0.4318]], grad_fn=<AddmmBackward0>) tensor(0.6348)\n",
      "tensor([[-0.8705]], grad_fn=<AddmmBackward0>) tensor(-0.9106)\n",
      "tensor([[0.5881]], grad_fn=<AddmmBackward0>) tensor(0.5610)\n",
      "tensor([[0.0090]], grad_fn=<AddmmBackward0>) tensor(0.4140)\n",
      "tensor([[0.4326]], grad_fn=<AddmmBackward0>) tensor(0.5625)\n",
      "tensor([[-0.6708]], grad_fn=<AddmmBackward0>) tensor(-0.4612)\n",
      "tensor([[0.4778]], grad_fn=<AddmmBackward0>) tensor(0.6015)\n",
      "tensor([[-0.1271]], grad_fn=<AddmmBackward0>) tensor(-0.1714)\n",
      "tensor([[-0.0652]], grad_fn=<AddmmBackward0>) tensor(0.4748)\n",
      "tensor([[-0.9554]], grad_fn=<AddmmBackward0>) tensor(-0.8566)\n",
      "tensor([[0.6850]], grad_fn=<AddmmBackward0>) tensor(0.7629)\n",
      "tensor([[-0.5789]], grad_fn=<AddmmBackward0>) tensor(-0.8236)\n",
      "tensor([[-0.5853]], grad_fn=<AddmmBackward0>) tensor(-0.4161)\n",
      "tensor([[-0.7375]], grad_fn=<AddmmBackward0>) tensor(-0.9269)\n",
      "tensor([[0.1580]], grad_fn=<AddmmBackward0>) tensor(0.5553)\n",
      "tensor([[-0.6375]], grad_fn=<AddmmBackward0>) tensor(-0.3717)\n",
      "tensor([[-0.3205]], grad_fn=<AddmmBackward0>) tensor(-0.3789)\n",
      "tensor([[-0.7193]], grad_fn=<AddmmBackward0>) tensor(-0.8482)\n",
      "tensor([[-0.7039]], grad_fn=<AddmmBackward0>) tensor(-0.7348)\n",
      "tensor([[-0.5317]], grad_fn=<AddmmBackward0>) tensor(-0.4748)\n",
      "tensor([[0.4979]], grad_fn=<AddmmBackward0>) tensor(0.5695)\n",
      "tensor([[-0.6926]], grad_fn=<AddmmBackward0>) tensor(-0.6300)\n",
      "tensor([[-0.5921]], grad_fn=<AddmmBackward0>) tensor(-0.6615)\n",
      "tensor([[0.7859]], grad_fn=<AddmmBackward0>) tensor(0.5604)\n",
      "tensor([[-0.5402]], grad_fn=<AddmmBackward0>) tensor(-0.4989)\n",
      "tensor([[-0.7053]], grad_fn=<AddmmBackward0>) tensor(-0.5806)\n",
      "tensor([[-0.0728]], grad_fn=<AddmmBackward0>) tensor(-0.3465)\n",
      "tensor([[-0.6625]], grad_fn=<AddmmBackward0>) tensor(-0.8753)\n",
      "tensor([[-0.0906]], grad_fn=<AddmmBackward0>) tensor(0.4561)\n",
      "tensor([[0.3449]], grad_fn=<AddmmBackward0>) tensor(0.6324)\n",
      "tensor([[-0.4308]], grad_fn=<AddmmBackward0>) tensor(0.2446)\n",
      "tensor([[-0.3014]], grad_fn=<AddmmBackward0>) tensor(-0.3520)\n",
      "tensor([[-0.2525]], grad_fn=<AddmmBackward0>) tensor(0.0168)\n",
      "tensor([[-0.6662]], grad_fn=<AddmmBackward0>) tensor(-0.7265)\n",
      "tensor([[0.2147]], grad_fn=<AddmmBackward0>) tensor(0.1342)\n",
      "tensor([[-1.3694]], grad_fn=<AddmmBackward0>) tensor(-0.9601)\n",
      "tensor([[-0.6691]], grad_fn=<AddmmBackward0>) tensor(-0.8920)\n",
      "tensor([[-0.5988]], grad_fn=<AddmmBackward0>) tensor(-0.9229)\n",
      "tensor([[-0.0493]], grad_fn=<AddmmBackward0>) tensor(0.7240)\n",
      "tensor([[-0.2733]], grad_fn=<AddmmBackward0>) tensor(-0.1978)\n",
      "tensor([[0.2503]], grad_fn=<AddmmBackward0>) tensor(0.5484)\n",
      "tensor([[-0.7406]], grad_fn=<AddmmBackward0>) tensor(-0.7482)\n",
      "tensor([[-0.8026]], grad_fn=<AddmmBackward0>) tensor(-0.9179)\n",
      "tensor([[-0.2248]], grad_fn=<AddmmBackward0>) tensor(0.8472)\n",
      "tensor([[-0.2483]], grad_fn=<AddmmBackward0>) tensor(0.5477)\n",
      "tensor([[0.1907]], grad_fn=<AddmmBackward0>) tensor(0.3562)\n",
      "tensor([[0.2494]], grad_fn=<AddmmBackward0>) tensor(0.7303)\n",
      "tensor([[0.0775]], grad_fn=<AddmmBackward0>) tensor(-0.0226)\n",
      "tensor([[-0.5306]], grad_fn=<AddmmBackward0>) tensor(-0.4120)\n",
      "tensor([[0.8940]], grad_fn=<AddmmBackward0>) tensor(0.9579)\n",
      "tensor([[-0.5288]], grad_fn=<AddmmBackward0>) tensor(-0.6380)\n",
      "tensor([[-0.8385]], grad_fn=<AddmmBackward0>) tensor(-0.8869)\n",
      "tensor([[-0.7061]], grad_fn=<AddmmBackward0>) tensor(-0.9214)\n",
      "tensor([[-0.1090]], grad_fn=<AddmmBackward0>) tensor(0.0042)\n",
      "tensor([[-0.0167]], grad_fn=<AddmmBackward0>) tensor(-0.0258)\n",
      "tensor([[-0.0782]], grad_fn=<AddmmBackward0>) tensor(0.5330)\n",
      "tensor([[-1.0195]], grad_fn=<AddmmBackward0>) tensor(-0.9605)\n",
      "tensor([[-0.8307]], grad_fn=<AddmmBackward0>) tensor(-0.8439)\n",
      "tensor([[-1.1794]], grad_fn=<AddmmBackward0>) tensor(-0.3876)\n",
      "tensor([[0.2519]], grad_fn=<AddmmBackward0>) tensor(0.1773)\n",
      "tensor([[0.0210]], grad_fn=<AddmmBackward0>) tensor(0.2792)\n",
      "tensor([[-0.4305]], grad_fn=<AddmmBackward0>) tensor(-0.6186)\n",
      "tensor([[0.3003]], grad_fn=<AddmmBackward0>) tensor(0.3178)\n",
      "tensor([[-0.5431]], grad_fn=<AddmmBackward0>) tensor(-0.1064)\n",
      "tensor([[-0.5799]], grad_fn=<AddmmBackward0>) tensor(-0.7342)\n",
      "tensor([[-0.0331]], grad_fn=<AddmmBackward0>) tensor(-0.1777)\n",
      "tensor([[-0.4185]], grad_fn=<AddmmBackward0>) tensor(-0.4981)\n",
      "tensor([[0.4022]], grad_fn=<AddmmBackward0>) tensor(0.5583)\n",
      "tensor([[-0.6228]], grad_fn=<AddmmBackward0>) tensor(-0.4258)\n",
      "tensor([[-0.8776]], grad_fn=<AddmmBackward0>) tensor(-0.8752)\n",
      "tensor([[-0.6568]], grad_fn=<AddmmBackward0>) tensor(-0.4383)\n",
      "tensor([[0.6588]], grad_fn=<AddmmBackward0>) tensor(0.5859)\n",
      "tensor([[-0.1979]], grad_fn=<AddmmBackward0>) tensor(-0.9318)\n",
      "tensor([[-0.7850]], grad_fn=<AddmmBackward0>) tensor(-0.7844)\n",
      "tensor([[-0.5318]], grad_fn=<AddmmBackward0>) tensor(-0.8523)\n",
      "tensor([[-0.1362]], grad_fn=<AddmmBackward0>) tensor(-0.1734)\n",
      "tensor([[-0.8413]], grad_fn=<AddmmBackward0>) tensor(-0.4695)\n",
      "tensor([[-0.8860]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.7216]], grad_fn=<AddmmBackward0>) tensor(-0.5805)\n",
      "tensor([[-0.6433]], grad_fn=<AddmmBackward0>) tensor(-0.9373)\n",
      "tensor([[-0.7333]], grad_fn=<AddmmBackward0>) tensor(-0.7564)\n",
      "tensor([[-0.1183]], grad_fn=<AddmmBackward0>) tensor(-0.8457)\n",
      "tensor([[0.1718]], grad_fn=<AddmmBackward0>) tensor(0.2216)\n",
      "tensor([[-0.4227]], grad_fn=<AddmmBackward0>) tensor(-0.2265)\n",
      "tensor([[-0.7920]], grad_fn=<AddmmBackward0>) tensor(-0.8524)\n",
      "tensor([[-0.9008]], grad_fn=<AddmmBackward0>) tensor(-0.7279)\n",
      "tensor([[-0.4187]], grad_fn=<AddmmBackward0>) tensor(-0.5062)\n",
      "tensor([[0.1790]], grad_fn=<AddmmBackward0>) tensor(0.3652)\n",
      "tensor([[-0.5052]], grad_fn=<AddmmBackward0>) tensor(-0.0300)\n",
      "tensor([[0.0091]], grad_fn=<AddmmBackward0>) tensor(-0.0442)\n",
      "tensor([[-0.2171]], grad_fn=<AddmmBackward0>) tensor(-0.4468)\n",
      "tensor([[-0.1531]], grad_fn=<AddmmBackward0>) tensor(-0.3428)\n",
      "tensor([[-0.6602]], grad_fn=<AddmmBackward0>) tensor(-0.9662)\n",
      "tensor([[-0.7977]], grad_fn=<AddmmBackward0>) tensor(-0.8334)\n",
      "tensor([[0.3961]], grad_fn=<AddmmBackward0>) tensor(0.7004)\n",
      "tensor([[0.3729]], grad_fn=<AddmmBackward0>) tensor(0.8136)\n",
      "tensor([[-0.6063]], grad_fn=<AddmmBackward0>) tensor(-0.5085)\n",
      "tensor([[0.1520]], grad_fn=<AddmmBackward0>) tensor(0.2364)\n",
      "tensor([[-0.4967]], grad_fn=<AddmmBackward0>) tensor(-0.8563)\n",
      "tensor([[0.6952]], grad_fn=<AddmmBackward0>) tensor(0.7277)\n",
      "tensor([[-0.4462]], grad_fn=<AddmmBackward0>) tensor(0.1314)\n",
      "tensor([[-0.3788]], grad_fn=<AddmmBackward0>) tensor(0.3501)\n",
      "tensor([[0.0348]], grad_fn=<AddmmBackward0>) tensor(0.1646)\n",
      "tensor([[-0.6549]], grad_fn=<AddmmBackward0>) tensor(-0.2639)\n",
      "tensor([[-0.2938]], grad_fn=<AddmmBackward0>) tensor(-0.2812)\n",
      "tensor([[-0.2296]], grad_fn=<AddmmBackward0>) tensor(-0.2864)\n",
      "tensor([[-0.6378]], grad_fn=<AddmmBackward0>) tensor(-0.8709)\n",
      "tensor([[-0.3936]], grad_fn=<AddmmBackward0>) tensor(-0.1496)\n",
      "tensor([[-0.9927]], grad_fn=<AddmmBackward0>) tensor(-0.7893)\n",
      "tensor([[0.6858]], grad_fn=<AddmmBackward0>) tensor(0.6016)\n",
      "tensor([[-0.8524]], grad_fn=<AddmmBackward0>) tensor(-0.9550)\n",
      "tensor([[-0.3079]], grad_fn=<AddmmBackward0>) tensor(-0.2928)\n",
      "tensor([[0.3142]], grad_fn=<AddmmBackward0>) tensor(0.2548)\n",
      "tensor([[-0.6923]], grad_fn=<AddmmBackward0>) tensor(-0.2780)\n",
      "tensor([[0.2883]], grad_fn=<AddmmBackward0>) tensor(0.2362)\n",
      "tensor([[0.4686]], grad_fn=<AddmmBackward0>) tensor(0.6153)\n",
      "tensor([[0.4797]], grad_fn=<AddmmBackward0>) tensor(0.9187)\n",
      "tensor([[-0.6126]], grad_fn=<AddmmBackward0>) tensor(-0.6299)\n",
      "tensor([[-0.2415]], grad_fn=<AddmmBackward0>) tensor(0.1586)\n",
      "tensor([[0.2407]], grad_fn=<AddmmBackward0>) tensor(0.3626)\n",
      "tensor([[-0.7282]], grad_fn=<AddmmBackward0>) tensor(-0.8259)\n",
      "tensor([[0.0784]], grad_fn=<AddmmBackward0>) tensor(0.3316)\n",
      "tensor([[-0.5249]], grad_fn=<AddmmBackward0>) tensor(-0.9848)\n",
      "tensor([[-0.5624]], grad_fn=<AddmmBackward0>) tensor(-0.4561)\n",
      "tensor([[0.0768]], grad_fn=<AddmmBackward0>) tensor(0.5137)\n",
      "tensor([[-0.6943]], grad_fn=<AddmmBackward0>) tensor(-0.7545)\n",
      "tensor([[-0.8540]], grad_fn=<AddmmBackward0>) tensor(-0.8136)\n",
      "tensor([[0.0988]], grad_fn=<AddmmBackward0>) tensor(0.6316)\n",
      "tensor([[-0.7365]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.5442]], grad_fn=<AddmmBackward0>) tensor(-0.6837)\n",
      "tensor([[-0.1175]], grad_fn=<AddmmBackward0>) tensor(-0.1840)\n",
      "tensor([[0.5406]], grad_fn=<AddmmBackward0>) tensor(0.6874)\n",
      "tensor([[-0.4795]], grad_fn=<AddmmBackward0>) tensor(-0.3465)\n",
      "tensor([[-0.4101]], grad_fn=<AddmmBackward0>) tensor(-0.4801)\n",
      "tensor([[-0.2471]], grad_fn=<AddmmBackward0>) tensor(0.0216)\n",
      "tensor([[-0.0692]], grad_fn=<AddmmBackward0>) tensor(-0.4465)\n",
      "tensor([[0.4851]], grad_fn=<AddmmBackward0>) tensor(0.7517)\n",
      "tensor([[-0.4855]], grad_fn=<AddmmBackward0>) tensor(-0.4742)\n",
      "tensor([[0.1032]], grad_fn=<AddmmBackward0>) tensor(0.4401)\n",
      "tensor([[-0.7522]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.4855]], grad_fn=<AddmmBackward0>) tensor(-0.8217)\n",
      "tensor([[0.8455]], grad_fn=<AddmmBackward0>) tensor(0.7384)\n",
      "tensor([[-0.6401]], grad_fn=<AddmmBackward0>) tensor(-0.8095)\n",
      "tensor([[-1.2950]], grad_fn=<AddmmBackward0>) tensor(-0.9602)\n",
      "tensor([[-0.6199]], grad_fn=<AddmmBackward0>) tensor(-0.9197)\n",
      "tensor([[-0.5371]], grad_fn=<AddmmBackward0>) tensor(-0.4225)\n",
      "tensor([[0.6300]], grad_fn=<AddmmBackward0>) tensor(0.5713)\n",
      "tensor([[-0.2288]], grad_fn=<AddmmBackward0>) tensor(0.1991)\n",
      "tensor([[-0.5805]], grad_fn=<AddmmBackward0>) tensor(-0.9230)\n",
      "tensor([[-0.6484]], grad_fn=<AddmmBackward0>) tensor(-0.4691)\n",
      "tensor([[-0.2690]], grad_fn=<AddmmBackward0>) tensor(0.1755)\n",
      "tensor([[-0.5226]], grad_fn=<AddmmBackward0>) tensor(-0.6451)\n",
      "tensor([[-0.1351]], grad_fn=<AddmmBackward0>) tensor(0.0518)\n",
      "tensor([[-0.7717]], grad_fn=<AddmmBackward0>) tensor(-0.8665)\n",
      "tensor([[-0.8010]], grad_fn=<AddmmBackward0>) tensor(-0.8448)\n",
      "tensor([[0.5500]], grad_fn=<AddmmBackward0>) tensor(0.5997)\n",
      "tensor([[-0.4691]], grad_fn=<AddmmBackward0>) tensor(-0.3114)\n",
      "tensor([[-1.1332]], grad_fn=<AddmmBackward0>) tensor(-0.8857)\n",
      "tensor([[-0.7401]], grad_fn=<AddmmBackward0>) tensor(-0.8812)\n",
      "tensor([[0.0696]], grad_fn=<AddmmBackward0>) tensor(-0.3312)\n",
      "tensor([[0.6823]], grad_fn=<AddmmBackward0>) tensor(0.5613)\n",
      "tensor([[-0.7615]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.6415]], grad_fn=<AddmmBackward0>) tensor(0.7189)\n",
      "tensor([[-0.5980]], grad_fn=<AddmmBackward0>) tensor(-0.8712)\n",
      "tensor([[0.0865]], grad_fn=<AddmmBackward0>) tensor(0.3795)\n",
      "tensor([[-0.6232]], grad_fn=<AddmmBackward0>) tensor(-0.3645)\n",
      "tensor([[0.0823]], grad_fn=<AddmmBackward0>) tensor(0.6221)\n",
      "tensor([[-0.5012]], grad_fn=<AddmmBackward0>) tensor(-0.7580)\n",
      "tensor([[0.4200]], grad_fn=<AddmmBackward0>) tensor(0.5080)\n",
      "tensor([[-0.4799]], grad_fn=<AddmmBackward0>) tensor(-0.5406)\n",
      "tensor([[0.3915]], grad_fn=<AddmmBackward0>) tensor(0.7280)\n",
      "tensor([[-0.4930]], grad_fn=<AddmmBackward0>) tensor(-0.3092)\n",
      "tensor([[0.9722]], grad_fn=<AddmmBackward0>) tensor(0.7439)\n",
      "tensor([[0.3686]], grad_fn=<AddmmBackward0>) tensor(0.7046)\n",
      "tensor([[-0.5039]], grad_fn=<AddmmBackward0>) tensor(-0.6358)\n",
      "tensor([[0.0784]], grad_fn=<AddmmBackward0>) tensor(0.2217)\n",
      "tensor([[0.2832]], grad_fn=<AddmmBackward0>) tensor(0.5428)\n",
      "tensor([[0.5960]], grad_fn=<AddmmBackward0>) tensor(0.6687)\n",
      "tensor([[0.4580]], grad_fn=<AddmmBackward0>) tensor(0.8412)\n",
      "tensor([[-0.4070]], grad_fn=<AddmmBackward0>) tensor(0.3262)\n",
      "tensor([[0.1720]], grad_fn=<AddmmBackward0>) tensor(0.7421)\n",
      "tensor([[-1.1421]], grad_fn=<AddmmBackward0>) tensor(-0.7262)\n",
      "tensor([[-0.3365]], grad_fn=<AddmmBackward0>) tensor(-0.3430)\n",
      "tensor([[0.3816]], grad_fn=<AddmmBackward0>) tensor(0.5069)\n",
      "tensor([[-0.6157]], grad_fn=<AddmmBackward0>) tensor(-0.4391)\n",
      "tensor([[-0.2291]], grad_fn=<AddmmBackward0>) tensor(-0.2667)\n",
      "tensor([[0.2891]], grad_fn=<AddmmBackward0>) tensor(0.2400)\n",
      "tensor([[-0.6972]], grad_fn=<AddmmBackward0>) tensor(-0.9639)\n",
      "tensor([[-0.5452]], grad_fn=<AddmmBackward0>) tensor(-0.3272)\n",
      "tensor([[-0.0771]], grad_fn=<AddmmBackward0>) tensor(0.1344)\n",
      "tensor([[-0.1535]], grad_fn=<AddmmBackward0>) tensor(-0.0042)\n",
      "tensor([[0.3668]], grad_fn=<AddmmBackward0>) tensor(0.5568)\n",
      "tensor([[-0.1068]], grad_fn=<AddmmBackward0>) tensor(0.0062)\n",
      "tensor([[-0.5408]], grad_fn=<AddmmBackward0>) tensor(-0.3334)\n",
      "tensor([[-0.4537]], grad_fn=<AddmmBackward0>) tensor(-0.0592)\n",
      "tensor([[0.2552]], grad_fn=<AddmmBackward0>) tensor(0.5803)\n",
      "tensor([[-0.3910]], grad_fn=<AddmmBackward0>) tensor(-0.3449)\n",
      "tensor([[-0.7429]], grad_fn=<AddmmBackward0>) tensor(-0.6826)\n",
      "tensor([[-0.5149]], grad_fn=<AddmmBackward0>) tensor(-0.6367)\n",
      "tensor([[-0.0260]], grad_fn=<AddmmBackward0>) tensor(-0.1841)\n",
      "tensor([[-0.7429]], grad_fn=<AddmmBackward0>) tensor(-0.6603)\n",
      "tensor([[-0.7073]], grad_fn=<AddmmBackward0>) tensor(-0.5729)\n",
      "tensor([[0.2373]], grad_fn=<AddmmBackward0>) tensor(0.2119)\n",
      "tensor([[-0.4026]], grad_fn=<AddmmBackward0>) tensor(-0.5015)\n",
      "tensor([[0.0190]], grad_fn=<AddmmBackward0>) tensor(-0.3611)\n",
      "tensor([[-0.6438]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.3380]], grad_fn=<AddmmBackward0>) tensor(-0.5041)\n",
      "tensor([[-0.2799]], grad_fn=<AddmmBackward0>) tensor(-0.0217)\n",
      "tensor([[0.3884]], grad_fn=<AddmmBackward0>) tensor(0.5676)\n",
      "tensor([[-0.7776]], grad_fn=<AddmmBackward0>) tensor(-0.8749)\n",
      "tensor([[-0.5030]], grad_fn=<AddmmBackward0>) tensor(-0.8665)\n",
      "tensor([[-0.5886]], grad_fn=<AddmmBackward0>) tensor(-0.4406)\n",
      "tensor([[-1.1225]], grad_fn=<AddmmBackward0>) tensor(-0.8922)\n",
      "tensor([[-0.7364]], grad_fn=<AddmmBackward0>) tensor(-0.5232)\n",
      "tensor([[-0.7409]], grad_fn=<AddmmBackward0>) tensor(0.3388)\n",
      "tensor([[-0.4760]], grad_fn=<AddmmBackward0>) tensor(-0.1093)\n",
      "tensor([[-0.0870]], grad_fn=<AddmmBackward0>) tensor(-0.1682)\n",
      "tensor([[0.2867]], grad_fn=<AddmmBackward0>) tensor(0.6869)\n",
      "tensor([[-0.5619]], grad_fn=<AddmmBackward0>) tensor(-0.6510)\n",
      "tensor([[0.1941]], grad_fn=<AddmmBackward0>) tensor(0.5514)\n",
      "tensor([[0.2560]], grad_fn=<AddmmBackward0>) tensor(0.5364)\n",
      "tensor([[0.3248]], grad_fn=<AddmmBackward0>) tensor(0.5639)\n",
      "tensor([[-0.7012]], grad_fn=<AddmmBackward0>) tensor(-0.7156)\n",
      "tensor([[-0.6515]], grad_fn=<AddmmBackward0>) tensor(-0.9746)\n",
      "tensor([[-0.0523]], grad_fn=<AddmmBackward0>) tensor(-0.0464)\n",
      "tensor([[-0.6314]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.6541]], grad_fn=<AddmmBackward0>) tensor(-0.4270)\n",
      "tensor([[-0.5357]], grad_fn=<AddmmBackward0>) tensor(-0.6203)\n",
      "tensor([[-0.0577]], grad_fn=<AddmmBackward0>) tensor(0.6713)\n",
      "tensor([[0.1230]], grad_fn=<AddmmBackward0>) tensor(0.5496)\n",
      "tensor([[0.0655]], grad_fn=<AddmmBackward0>) tensor(0.2525)\n",
      "tensor([[0.5741]], grad_fn=<AddmmBackward0>) tensor(0.6270)\n",
      "tensor([[0.7373]], grad_fn=<AddmmBackward0>) tensor(0.6023)\n",
      "tensor([[0.5650]], grad_fn=<AddmmBackward0>) tensor(0.7476)\n",
      "tensor([[-0.4186]], grad_fn=<AddmmBackward0>) tensor(-0.9198)\n",
      "tensor([[-0.4954]], grad_fn=<AddmmBackward0>) tensor(-0.6354)\n",
      "tensor([[-0.9141]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.6492]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.4519]], grad_fn=<AddmmBackward0>) tensor(-0.9454)\n",
      "tensor([[-0.4133]], grad_fn=<AddmmBackward0>) tensor(-0.4764)\n",
      "tensor([[0.8795]], grad_fn=<AddmmBackward0>) tensor(0.7525)\n",
      "tensor([[-0.6364]], grad_fn=<AddmmBackward0>) tensor(-0.3530)\n",
      "tensor([[0.3982]], grad_fn=<AddmmBackward0>) tensor(0.5328)\n",
      "tensor([[-0.4649]], grad_fn=<AddmmBackward0>) tensor(-0.5130)\n",
      "tensor([[-0.7582]], grad_fn=<AddmmBackward0>) tensor(-0.5120)\n",
      "tensor([[0.3228]], grad_fn=<AddmmBackward0>) tensor(0.5792)\n",
      "tensor([[0.4147]], grad_fn=<AddmmBackward0>) tensor(0.2374)\n",
      "tensor([[-0.6222]], grad_fn=<AddmmBackward0>) tensor(-0.6245)\n",
      "tensor([[-1.0633]], grad_fn=<AddmmBackward0>) tensor(-0.9331)\n",
      "tensor([[0.5914]], grad_fn=<AddmmBackward0>) tensor(0.7558)\n",
      "tensor([[-0.8095]], grad_fn=<AddmmBackward0>) tensor(-0.9623)\n",
      "tensor([[-0.4303]], grad_fn=<AddmmBackward0>) tensor(-0.9663)\n",
      "tensor([[0.3801]], grad_fn=<AddmmBackward0>) tensor(0.6594)\n",
      "tensor([[-0.0347]], grad_fn=<AddmmBackward0>) tensor(0.7694)\n",
      "tensor([[0.6860]], grad_fn=<AddmmBackward0>) tensor(0.5419)\n",
      "tensor([[0.2254]], grad_fn=<AddmmBackward0>) tensor(0.2565)\n",
      "tensor([[0.1910]], grad_fn=<AddmmBackward0>) tensor(0.3651)\n",
      "tensor([[-0.7639]], grad_fn=<AddmmBackward0>) tensor(-0.5209)\n",
      "tensor([[-0.2203]], grad_fn=<AddmmBackward0>) tensor(-0.0338)\n",
      "tensor([[-0.2975]], grad_fn=<AddmmBackward0>) tensor(-0.9164)\n",
      "tensor([[-0.6731]], grad_fn=<AddmmBackward0>) tensor(-0.6702)\n",
      "tensor([[0.1669]], grad_fn=<AddmmBackward0>) tensor(0.8120)\n",
      "tensor([[0.2504]], grad_fn=<AddmmBackward0>) tensor(0.4891)\n",
      "tensor([[0.6133]], grad_fn=<AddmmBackward0>) tensor(0.7438)\n",
      "tensor([[0.3207]], grad_fn=<AddmmBackward0>) tensor(0.7330)\n",
      "tensor([[-0.5600]], grad_fn=<AddmmBackward0>) tensor(-0.8670)\n",
      "tensor([[-0.5185]], grad_fn=<AddmmBackward0>) tensor(0.4596)\n",
      "tensor([[0.3280]], grad_fn=<AddmmBackward0>) tensor(0.7390)\n",
      "tensor([[-0.5227]], grad_fn=<AddmmBackward0>) tensor(-0.5313)\n",
      "tensor([[0.0255]], grad_fn=<AddmmBackward0>) tensor(0.3914)\n",
      "tensor([[-0.5988]], grad_fn=<AddmmBackward0>) tensor(-0.4414)\n",
      "tensor([[-0.7757]], grad_fn=<AddmmBackward0>) tensor(-0.8032)\n",
      "tensor([[0.0999]], grad_fn=<AddmmBackward0>) tensor(0.7330)\n",
      "tensor([[0.5882]], grad_fn=<AddmmBackward0>) tensor(0.5406)\n",
      "tensor([[-0.5377]], grad_fn=<AddmmBackward0>) tensor(-0.6455)\n",
      "tensor([[0.1410]], grad_fn=<AddmmBackward0>) tensor(0.1132)\n",
      "tensor([[0.5624]], grad_fn=<AddmmBackward0>) tensor(0.5877)\n",
      "tensor([[0.3456]], grad_fn=<AddmmBackward0>) tensor(0.6847)\n",
      "tensor([[0.7679]], grad_fn=<AddmmBackward0>) tensor(0.6996)\n",
      "tensor([[-0.7038]], grad_fn=<AddmmBackward0>) tensor(-0.9331)\n",
      "tensor([[-0.1806]], grad_fn=<AddmmBackward0>) tensor(-0.1795)\n",
      "tensor([[0.4133]], grad_fn=<AddmmBackward0>) tensor(0.5518)\n",
      "tensor([[0.5296]], grad_fn=<AddmmBackward0>) tensor(0.5358)\n",
      "tensor([[-0.6853]], grad_fn=<AddmmBackward0>) tensor(-0.5102)\n",
      "tensor([[0.0442]], grad_fn=<AddmmBackward0>) tensor(0.2143)\n",
      "tensor([[0.4993]], grad_fn=<AddmmBackward0>) tensor(0.7058)\n",
      "tensor([[-0.8272]], grad_fn=<AddmmBackward0>) tensor(-0.5697)\n",
      "tensor([[0.2800]], grad_fn=<AddmmBackward0>) tensor(0.9991)\n",
      "tensor([[-0.8569]], grad_fn=<AddmmBackward0>) tensor(-0.9566)\n",
      "tensor([[-0.7245]], grad_fn=<AddmmBackward0>) tensor(-0.3528)\n",
      "tensor([[0.1043]], grad_fn=<AddmmBackward0>) tensor(0.2891)\n",
      "tensor([[0.3821]], grad_fn=<AddmmBackward0>) tensor(0.6210)\n",
      "tensor([[-0.7299]], grad_fn=<AddmmBackward0>) tensor(-0.8776)\n",
      "tensor([[-0.7274]], grad_fn=<AddmmBackward0>) tensor(-0.8791)\n",
      "tensor([[0.7524]], grad_fn=<AddmmBackward0>) tensor(0.5629)\n",
      "tensor([[-0.5630]], grad_fn=<AddmmBackward0>) tensor(-0.3319)\n",
      "tensor([[-0.7723]], grad_fn=<AddmmBackward0>) tensor(-0.8296)\n",
      "tensor([[-0.4496]], grad_fn=<AddmmBackward0>) tensor(-0.2844)\n",
      "tensor([[0.3815]], grad_fn=<AddmmBackward0>) tensor(0.7222)\n",
      "tensor([[0.3759]], grad_fn=<AddmmBackward0>) tensor(0.5211)\n",
      "tensor([[-0.0474]], grad_fn=<AddmmBackward0>) tensor(0.2977)\n",
      "tensor([[-0.9126]], grad_fn=<AddmmBackward0>) tensor(-0.7834)\n",
      "tensor([[-0.0952]], grad_fn=<AddmmBackward0>) tensor(0.7755)\n",
      "tensor([[-0.5009]], grad_fn=<AddmmBackward0>) tensor(-0.1399)\n",
      "tensor([[-0.6979]], grad_fn=<AddmmBackward0>) tensor(-0.7478)\n",
      "tensor([[0.1755]], grad_fn=<AddmmBackward0>) tensor(0.6866)\n",
      "tensor([[-1.1238]], grad_fn=<AddmmBackward0>) tensor(-0.7865)\n",
      "tensor([[0.7713]], grad_fn=<AddmmBackward0>) tensor(0.6137)\n",
      "tensor([[-0.7622]], grad_fn=<AddmmBackward0>) tensor(-0.9916)\n",
      "tensor([[-0.2781]], grad_fn=<AddmmBackward0>) tensor(-0.2535)\n",
      "tensor([[0.6754]], grad_fn=<AddmmBackward0>) tensor(0.5766)\n",
      "tensor([[0.3122]], grad_fn=<AddmmBackward0>) tensor(0.7680)\n",
      "tensor([[-0.8915]], grad_fn=<AddmmBackward0>) tensor(-0.7341)\n",
      "tensor([[-0.1650]], grad_fn=<AddmmBackward0>) tensor(-0.4503)\n",
      "tensor([[-0.7672]], grad_fn=<AddmmBackward0>) tensor(-0.4181)\n",
      "tensor([[0.8692]], grad_fn=<AddmmBackward0>) tensor(0.8401)\n",
      "tensor([[-0.4078]], grad_fn=<AddmmBackward0>) tensor(-0.9843)\n",
      "tensor([[-0.3116]], grad_fn=<AddmmBackward0>) tensor(-0.8044)\n",
      "tensor([[0.2672]], grad_fn=<AddmmBackward0>) tensor(0.6426)\n",
      "tensor([[0.4026]], grad_fn=<AddmmBackward0>) tensor(0.7971)\n",
      "tensor([[-0.5965]], grad_fn=<AddmmBackward0>) tensor(-0.5780)\n",
      "tensor([[-0.2845]], grad_fn=<AddmmBackward0>) tensor(-0.1530)\n",
      "tensor([[-0.6063]], grad_fn=<AddmmBackward0>) tensor(-0.4374)\n",
      "tensor([[-0.4100]], grad_fn=<AddmmBackward0>) tensor(0.2090)\n",
      "tensor([[0.1536]], grad_fn=<AddmmBackward0>) tensor(0.4449)\n",
      "tensor([[0.2966]], grad_fn=<AddmmBackward0>) tensor(0.7626)\n",
      "tensor([[-0.2035]], grad_fn=<AddmmBackward0>) tensor(0.0011)\n",
      "tensor([[-0.6873]], grad_fn=<AddmmBackward0>) tensor(-0.4610)\n",
      "tensor([[-0.8099]], grad_fn=<AddmmBackward0>) tensor(-0.7684)\n",
      "tensor([[0.6916]], grad_fn=<AddmmBackward0>) tensor(0.6292)\n",
      "tensor([[-0.6360]], grad_fn=<AddmmBackward0>) tensor(-0.6352)\n",
      "tensor([[-0.2781]], grad_fn=<AddmmBackward0>) tensor(-0.0723)\n",
      "tensor([[-0.2394]], grad_fn=<AddmmBackward0>) tensor(-0.0696)\n",
      "tensor([[0.3181]], grad_fn=<AddmmBackward0>) tensor(0.5725)\n",
      "tensor([[0.0878]], grad_fn=<AddmmBackward0>) tensor(0.3466)\n",
      "tensor([[0.3921]], grad_fn=<AddmmBackward0>) tensor(0.6676)\n",
      "tensor([[0.2351]], grad_fn=<AddmmBackward0>) tensor(0.7176)\n",
      "tensor([[-0.6127]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.4664]], grad_fn=<AddmmBackward0>) tensor(0.5415)\n",
      "tensor([[-0.5736]], grad_fn=<AddmmBackward0>) tensor(-0.3897)\n",
      "tensor([[0.3393]], grad_fn=<AddmmBackward0>) tensor(0.5450)\n",
      "tensor([[0.3303]], grad_fn=<AddmmBackward0>) tensor(0.5282)\n",
      "tensor([[-0.7274]], grad_fn=<AddmmBackward0>) tensor(-0.7823)\n",
      "tensor([[-0.6704]], grad_fn=<AddmmBackward0>) tensor(-0.8695)\n",
      "tensor([[-0.0724]], grad_fn=<AddmmBackward0>) tensor(-0.1540)\n",
      "tensor([[-0.8330]], grad_fn=<AddmmBackward0>) tensor(-0.9484)\n",
      "tensor([[0.1123]], grad_fn=<AddmmBackward0>) tensor(0.2647)\n",
      "tensor([[0.5131]], grad_fn=<AddmmBackward0>) tensor(0.5517)\n",
      "tensor([[-0.4102]], grad_fn=<AddmmBackward0>) tensor(0.0802)\n",
      "tensor([[-0.8029]], grad_fn=<AddmmBackward0>) tensor(-0.9437)\n",
      "tensor([[-0.4161]], grad_fn=<AddmmBackward0>) tensor(-0.1052)\n",
      "tensor([[-0.5014]], grad_fn=<AddmmBackward0>) tensor(-0.2699)\n",
      "tensor([[-0.9006]], grad_fn=<AddmmBackward0>) tensor(-0.7060)\n",
      "tensor([[0.7035]], grad_fn=<AddmmBackward0>) tensor(0.8288)\n",
      "tensor([[-0.4242]], grad_fn=<AddmmBackward0>) tensor(-0.2520)\n",
      "tensor([[0.4275]], grad_fn=<AddmmBackward0>) tensor(0.5643)\n",
      "tensor([[-0.6083]], grad_fn=<AddmmBackward0>) tensor(-0.6806)\n",
      "tensor([[-0.2727]], grad_fn=<AddmmBackward0>) tensor(-0.2495)\n",
      "tensor([[0.3443]], grad_fn=<AddmmBackward0>) tensor(0.7510)\n",
      "tensor([[-0.6758]], grad_fn=<AddmmBackward0>) tensor(-0.7630)\n",
      "tensor([[0.2512]], grad_fn=<AddmmBackward0>) tensor(0.5887)\n",
      "tensor([[-0.3580]], grad_fn=<AddmmBackward0>) tensor(-0.3832)\n",
      "tensor([[-0.2174]], grad_fn=<AddmmBackward0>) tensor(0.7220)\n",
      "tensor([[-0.0565]], grad_fn=<AddmmBackward0>) tensor(-0.1396)\n",
      "tensor([[-0.5411]], grad_fn=<AddmmBackward0>) tensor(-0.3134)\n",
      "tensor([[0.6191]], grad_fn=<AddmmBackward0>) tensor(0.5496)\n",
      "tensor([[-0.2077]], grad_fn=<AddmmBackward0>) tensor(-0.2255)\n",
      "tensor([[0.0964]], grad_fn=<AddmmBackward0>) tensor(0.5504)\n",
      "tensor([[-0.6124]], grad_fn=<AddmmBackward0>) tensor(-0.9677)\n",
      "tensor([[0.0477]], grad_fn=<AddmmBackward0>) tensor(0.5511)\n",
      "tensor([[0.4440]], grad_fn=<AddmmBackward0>) tensor(0.6830)\n",
      "tensor([[-0.4998]], grad_fn=<AddmmBackward0>) tensor(-0.5242)\n",
      "tensor([[0.1751]], grad_fn=<AddmmBackward0>) tensor(0.5105)\n",
      "tensor([[0.2304]], grad_fn=<AddmmBackward0>) tensor(0.6137)\n",
      "tensor([[-0.1904]], grad_fn=<AddmmBackward0>) tensor(-0.1509)\n",
      "tensor([[-0.3558]], grad_fn=<AddmmBackward0>) tensor(-0.2497)\n",
      "tensor([[-0.8146]], grad_fn=<AddmmBackward0>) tensor(-0.9179)\n",
      "tensor([[-0.8457]], grad_fn=<AddmmBackward0>) tensor(-0.6025)\n",
      "tensor([[0.9146]], grad_fn=<AddmmBackward0>) tensor(0.8468)\n",
      "tensor([[-0.5110]], grad_fn=<AddmmBackward0>) tensor(-0.0289)\n",
      "tensor([[-0.3124]], grad_fn=<AddmmBackward0>) tensor(-0.1099)\n",
      "tensor([[0.4897]], grad_fn=<AddmmBackward0>) tensor(0.6328)\n",
      "tensor([[0.1728]], grad_fn=<AddmmBackward0>) tensor(0.5335)\n",
      "tensor([[-0.5331]], grad_fn=<AddmmBackward0>) tensor(-0.7271)\n",
      "tensor([[-0.7444]], grad_fn=<AddmmBackward0>) tensor(-0.4373)\n",
      "tensor([[-0.0738]], grad_fn=<AddmmBackward0>) tensor(-0.2450)\n",
      "tensor([[-0.5148]], grad_fn=<AddmmBackward0>) tensor(-0.2843)\n",
      "tensor([[0.2364]], grad_fn=<AddmmBackward0>) tensor(0.7750)\n",
      "tensor([[-0.5500]], grad_fn=<AddmmBackward0>) tensor(-0.1099)\n",
      "tensor([[0.0639]], grad_fn=<AddmmBackward0>) tensor(-0.2170)\n",
      "tensor([[0.6309]], grad_fn=<AddmmBackward0>) tensor(0.7063)\n",
      "tensor([[-0.8457]], grad_fn=<AddmmBackward0>) tensor(-0.4663)\n",
      "tensor([[-0.2572]], grad_fn=<AddmmBackward0>) tensor(0.2776)\n",
      "tensor([[-0.3626]], grad_fn=<AddmmBackward0>) tensor(-0.3021)\n",
      "tensor([[-0.2631]], grad_fn=<AddmmBackward0>) tensor(-0.1565)\n",
      "tensor([[0.0142]], grad_fn=<AddmmBackward0>) tensor(0.1349)\n",
      "tensor([[0.4208]], grad_fn=<AddmmBackward0>) tensor(0.5835)\n",
      "tensor([[-0.0234]], grad_fn=<AddmmBackward0>) tensor(0.3802)\n",
      "tensor([[-0.7097]], grad_fn=<AddmmBackward0>) tensor(-0.8499)\n",
      "tensor([[-0.0308]], grad_fn=<AddmmBackward0>) tensor(0.7981)\n",
      "tensor([[-0.5310]], grad_fn=<AddmmBackward0>) tensor(-0.5570)\n",
      "tensor([[0.8517]], grad_fn=<AddmmBackward0>) tensor(0.7149)\n",
      "tensor([[-0.7342]], grad_fn=<AddmmBackward0>) tensor(-0.7162)\n",
      "tensor([[-1.1230]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.9440]], grad_fn=<AddmmBackward0>) tensor(-0.3444)\n",
      "tensor([[-0.3376]], grad_fn=<AddmmBackward0>) tensor(-0.3445)\n",
      "tensor([[0.0047]], grad_fn=<AddmmBackward0>) tensor(0.0160)\n",
      "tensor([[-0.4083]], grad_fn=<AddmmBackward0>) tensor(-0.4457)\n",
      "tensor([[0.6492]], grad_fn=<AddmmBackward0>) tensor(0.7941)\n",
      "tensor([[-0.8167]], grad_fn=<AddmmBackward0>) tensor(-0.3428)\n",
      "tensor([[0.2461]], grad_fn=<AddmmBackward0>) tensor(0.3640)\n",
      "tensor([[0.4257]], grad_fn=<AddmmBackward0>) tensor(0.7600)\n",
      "tensor([[-0.2504]], grad_fn=<AddmmBackward0>) tensor(0.2251)\n",
      "tensor([[-0.5205]], grad_fn=<AddmmBackward0>) tensor(-0.4199)\n",
      "tensor([[-0.6046]], grad_fn=<AddmmBackward0>) tensor(-0.4758)\n",
      "tensor([[-0.8271]], grad_fn=<AddmmBackward0>) tensor(-0.5908)\n",
      "tensor([[0.0811]], grad_fn=<AddmmBackward0>) tensor(0.3750)\n",
      "tensor([[-0.5492]], grad_fn=<AddmmBackward0>) tensor(-0.4454)\n",
      "tensor([[-0.5821]], grad_fn=<AddmmBackward0>) tensor(-0.5391)\n",
      "tensor([[0.1905]], grad_fn=<AddmmBackward0>) tensor(0.0118)\n",
      "tensor([[-0.1473]], grad_fn=<AddmmBackward0>) tensor(-0.1350)\n",
      "tensor([[-0.7113]], grad_fn=<AddmmBackward0>) tensor(-0.2962)\n",
      "tensor([[-0.4215]], grad_fn=<AddmmBackward0>) tensor(0.0059)\n",
      "tensor([[0.6014]], grad_fn=<AddmmBackward0>) tensor(0.5320)\n",
      "tensor([[-0.8989]], grad_fn=<AddmmBackward0>) tensor(-0.8481)\n",
      "tensor([[-0.9218]], grad_fn=<AddmmBackward0>) tensor(-0.8140)\n",
      "tensor([[0.3230]], grad_fn=<AddmmBackward0>) tensor(0.5344)\n",
      "tensor([[-0.2160]], grad_fn=<AddmmBackward0>) tensor(0.0190)\n",
      "tensor([[0.4352]], grad_fn=<AddmmBackward0>) tensor(0.5787)\n",
      "tensor([[-0.3195]], grad_fn=<AddmmBackward0>) tensor(0.0233)\n",
      "tensor([[-0.8825]], grad_fn=<AddmmBackward0>) tensor(-0.5824)\n",
      "tensor([[0.1085]], grad_fn=<AddmmBackward0>) tensor(-0.0510)\n",
      "tensor([[0.4072]], grad_fn=<AddmmBackward0>) tensor(0.5704)\n",
      "tensor([[0.2901]], grad_fn=<AddmmBackward0>) tensor(0.4951)\n",
      "tensor([[-0.1449]], grad_fn=<AddmmBackward0>) tensor(0.1455)\n",
      "tensor([[-0.7145]], grad_fn=<AddmmBackward0>) tensor(-0.9301)\n",
      "tensor([[-0.6496]], grad_fn=<AddmmBackward0>) tensor(-0.8942)\n",
      "tensor([[-0.2386]], grad_fn=<AddmmBackward0>) tensor(-0.2644)\n",
      "tensor([[0.2489]], grad_fn=<AddmmBackward0>) tensor(0.9458)\n",
      "tensor([[0.0294]], grad_fn=<AddmmBackward0>) tensor(0.5571)\n",
      "tensor([[-0.0185]], grad_fn=<AddmmBackward0>) tensor(-0.1417)\n",
      "tensor([[-0.4165]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.3705]], grad_fn=<AddmmBackward0>) tensor(0.7420)\n",
      "tensor([[0.7758]], grad_fn=<AddmmBackward0>) tensor(0.7357)\n",
      "tensor([[0.5849]], grad_fn=<AddmmBackward0>) tensor(0.7170)\n",
      "tensor([[-0.7623]], grad_fn=<AddmmBackward0>) tensor(-0.5310)\n",
      "tensor([[-0.4433]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.1257]], grad_fn=<AddmmBackward0>) tensor(-0.2510)\n",
      "tensor([[0.6896]], grad_fn=<AddmmBackward0>) tensor(0.7568)\n",
      "tensor([[0.1562]], grad_fn=<AddmmBackward0>) tensor(0.6716)\n",
      "tensor([[-0.5565]], grad_fn=<AddmmBackward0>) tensor(-0.2376)\n",
      "tensor([[-0.0903]], grad_fn=<AddmmBackward0>) tensor(0.3383)\n",
      "tensor([[-0.6328]], grad_fn=<AddmmBackward0>) tensor(-0.8590)\n",
      "tensor([[-0.6222]], grad_fn=<AddmmBackward0>) tensor(-0.5844)\n",
      "tensor([[-0.8010]], grad_fn=<AddmmBackward0>) tensor(-0.4089)\n",
      "tensor([[-0.6648]], grad_fn=<AddmmBackward0>) tensor(-0.4410)\n",
      "tensor([[-0.5989]], grad_fn=<AddmmBackward0>) tensor(-0.2636)\n",
      "tensor([[0.2745]], grad_fn=<AddmmBackward0>) tensor(0.3724)\n",
      "tensor([[-1.0103]], grad_fn=<AddmmBackward0>) tensor(-0.1817)\n",
      "tensor([[0.5573]], grad_fn=<AddmmBackward0>) tensor(0.8197)\n",
      "tensor([[-0.0554]], grad_fn=<AddmmBackward0>) tensor(0.0416)\n",
      "tensor([[-0.6416]], grad_fn=<AddmmBackward0>) tensor(-0.5732)\n",
      "tensor([[0.3673]], grad_fn=<AddmmBackward0>) tensor(0.6169)\n",
      "tensor([[-0.0721]], grad_fn=<AddmmBackward0>) tensor(0.3968)\n",
      "tensor([[-0.4208]], grad_fn=<AddmmBackward0>) tensor(-0.3153)\n",
      "tensor([[-0.6237]], grad_fn=<AddmmBackward0>) tensor(-0.3729)\n",
      "tensor([[0.5714]], grad_fn=<AddmmBackward0>) tensor(0.6411)\n",
      "tensor([[-0.0886]], grad_fn=<AddmmBackward0>) tensor(0.1219)\n",
      "tensor([[-0.7186]], grad_fn=<AddmmBackward0>) tensor(-0.8767)\n",
      "tensor([[-0.2800]], grad_fn=<AddmmBackward0>) tensor(-0.7132)\n",
      "tensor([[-0.2637]], grad_fn=<AddmmBackward0>) tensor(-0.0949)\n",
      "tensor([[-0.6474]], grad_fn=<AddmmBackward0>) tensor(-0.8574)\n",
      "tensor([[-1.1066]], grad_fn=<AddmmBackward0>) tensor(-0.9424)\n",
      "tensor([[0.5503]], grad_fn=<AddmmBackward0>) tensor(0.5659)\n",
      "tensor([[-0.9600]], grad_fn=<AddmmBackward0>) tensor(-0.8635)\n",
      "tensor([[-0.1012]], grad_fn=<AddmmBackward0>) tensor(0.6681)\n",
      "tensor([[-1.0541]], grad_fn=<AddmmBackward0>) tensor(-0.9682)\n",
      "tensor([[-0.9550]], grad_fn=<AddmmBackward0>) tensor(-0.7914)\n",
      "tensor([[-0.0375]], grad_fn=<AddmmBackward0>) tensor(-0.4559)\n",
      "tensor([[-0.6605]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.2531]], grad_fn=<AddmmBackward0>) tensor(0.7006)\n",
      "tensor([[0.3590]], grad_fn=<AddmmBackward0>) tensor(0.7140)\n",
      "tensor([[-0.3101]], grad_fn=<AddmmBackward0>) tensor(0.6776)\n",
      "tensor([[-0.7656]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.0526]], grad_fn=<AddmmBackward0>) tensor(-0.8572)\n",
      "tensor([[-0.6836]], grad_fn=<AddmmBackward0>) tensor(-0.7388)\n",
      "tensor([[0.6769]], grad_fn=<AddmmBackward0>) tensor(0.7851)\n",
      "tensor([[0.0950]], grad_fn=<AddmmBackward0>) tensor(0.5336)\n",
      "tensor([[-0.8876]], grad_fn=<AddmmBackward0>) tensor(-0.4416)\n",
      "tensor([[-1.0259]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.0354]], grad_fn=<AddmmBackward0>) tensor(0.0258)\n",
      "tensor([[-0.5368]], grad_fn=<AddmmBackward0>) tensor(-0.9061)\n",
      "tensor([[0.3293]], grad_fn=<AddmmBackward0>) tensor(0.7367)\n",
      "tensor([[-0.0866]], grad_fn=<AddmmBackward0>) tensor(0.0005)\n",
      "tensor([[0.7166]], grad_fn=<AddmmBackward0>) tensor(0.6064)\n",
      "tensor([[-0.5551]], grad_fn=<AddmmBackward0>) tensor(-0.6477)\n",
      "tensor([[0.0392]], grad_fn=<AddmmBackward0>) tensor(0.2526)\n",
      "tensor([[-0.7740]], grad_fn=<AddmmBackward0>) tensor(-0.6371)\n",
      "tensor([[0.5042]], grad_fn=<AddmmBackward0>) tensor(0.5228)\n",
      "tensor([[-0.8489]], grad_fn=<AddmmBackward0>) tensor(-0.7286)\n",
      "tensor([[-0.4561]], grad_fn=<AddmmBackward0>) tensor(-0.8668)\n",
      "tensor([[0.0111]], grad_fn=<AddmmBackward0>) tensor(0.2299)\n",
      "tensor([[-0.5132]], grad_fn=<AddmmBackward0>) tensor(0.0608)\n",
      "tensor([[-0.6543]], grad_fn=<AddmmBackward0>) tensor(-0.9427)\n",
      "tensor([[-0.5469]], grad_fn=<AddmmBackward0>) tensor(-0.6244)\n",
      "tensor([[0.8727]], grad_fn=<AddmmBackward0>) tensor(0.5532)\n",
      "tensor([[0.3294]], grad_fn=<AddmmBackward0>) tensor(0.6906)\n",
      "tensor([[-0.0080]], grad_fn=<AddmmBackward0>) tensor(-0.1050)\n",
      "tensor([[0.5961]], grad_fn=<AddmmBackward0>) tensor(0.5846)\n",
      "tensor([[0.2846]], grad_fn=<AddmmBackward0>) tensor(0.2916)\n",
      "tensor([[-0.7894]], grad_fn=<AddmmBackward0>) tensor(-0.8236)\n",
      "tensor([[-0.4524]], grad_fn=<AddmmBackward0>) tensor(-0.2852)\n",
      "tensor([[-0.5977]], grad_fn=<AddmmBackward0>) tensor(-0.3118)\n",
      "tensor([[-0.6453]], grad_fn=<AddmmBackward0>) tensor(-0.5106)\n",
      "tensor([[-0.3832]], grad_fn=<AddmmBackward0>) tensor(-0.4272)\n",
      "tensor([[-0.1045]], grad_fn=<AddmmBackward0>) tensor(-0.1184)\n",
      "tensor([[0.1628]], grad_fn=<AddmmBackward0>) tensor(0.6908)\n",
      "tensor([[-0.7231]], grad_fn=<AddmmBackward0>) tensor(-0.6594)\n",
      "tensor([[-0.1002]], grad_fn=<AddmmBackward0>) tensor(-0.2964)\n",
      "tensor([[-0.5691]], grad_fn=<AddmmBackward0>) tensor(-0.5968)\n",
      "tensor([[0.5327]], grad_fn=<AddmmBackward0>) tensor(0.6943)\n",
      "tensor([[-0.0737]], grad_fn=<AddmmBackward0>) tensor(-0.2356)\n",
      "tensor([[0.7152]], grad_fn=<AddmmBackward0>) tensor(0.8408)\n",
      "tensor([[-0.3289]], grad_fn=<AddmmBackward0>) tensor(0.0234)\n",
      "tensor([[-0.5242]], grad_fn=<AddmmBackward0>) tensor(-0.7076)\n",
      "tensor([[-0.4028]], grad_fn=<AddmmBackward0>) tensor(-0.2924)\n",
      "tensor([[-0.8840]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.1377]], grad_fn=<AddmmBackward0>) tensor(0.1715)\n",
      "tensor([[0.0069]], grad_fn=<AddmmBackward0>) tensor(0.3126)\n",
      "tensor([[-0.0298]], grad_fn=<AddmmBackward0>) tensor(0.2663)\n",
      "tensor([[0.3558]], grad_fn=<AddmmBackward0>) tensor(0.6932)\n",
      "tensor([[-0.0493]], grad_fn=<AddmmBackward0>) tensor(0.0238)\n",
      "tensor([[-0.3533]], grad_fn=<AddmmBackward0>) tensor(0.0117)\n",
      "tensor([[-0.4345]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.8080]], grad_fn=<AddmmBackward0>) tensor(-0.9111)\n",
      "tensor([[0.0262]], grad_fn=<AddmmBackward0>) tensor(0.1539)\n",
      "tensor([[-0.1400]], grad_fn=<AddmmBackward0>) tensor(0.3262)\n",
      "tensor([[0.0908]], grad_fn=<AddmmBackward0>) tensor(-0.1159)\n",
      "tensor([[-0.4603]], grad_fn=<AddmmBackward0>) tensor(-0.1176)\n",
      "tensor([[-0.0902]], grad_fn=<AddmmBackward0>) tensor(0.5334)\n",
      "tensor([[-0.5522]], grad_fn=<AddmmBackward0>) tensor(0.9847)\n",
      "tensor([[-0.6728]], grad_fn=<AddmmBackward0>) tensor(-0.7474)\n",
      "tensor([[-0.5490]], grad_fn=<AddmmBackward0>) tensor(-0.9197)\n",
      "tensor([[-0.4428]], grad_fn=<AddmmBackward0>) tensor(-0.6530)\n",
      "tensor([[-0.8120]], grad_fn=<AddmmBackward0>) tensor(-0.9223)\n",
      "tensor([[-0.4542]], grad_fn=<AddmmBackward0>) tensor(-0.5777)\n",
      "tensor([[0.7879]], grad_fn=<AddmmBackward0>) tensor(0.8305)\n",
      "tensor([[-1.0193]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.7969]], grad_fn=<AddmmBackward0>) tensor(0.6159)\n",
      "tensor([[0.0998]], grad_fn=<AddmmBackward0>) tensor(0.3739)\n",
      "tensor([[-0.8348]], grad_fn=<AddmmBackward0>) tensor(-0.5909)\n",
      "tensor([[-0.5798]], grad_fn=<AddmmBackward0>) tensor(-0.4865)\n",
      "tensor([[0.7887]], grad_fn=<AddmmBackward0>) tensor(0.7525)\n",
      "tensor([[0.2474]], grad_fn=<AddmmBackward0>) tensor(0.6610)\n",
      "tensor([[-0.0280]], grad_fn=<AddmmBackward0>) tensor(0.0480)\n",
      "tensor([[-0.4114]], grad_fn=<AddmmBackward0>) tensor(-0.9579)\n",
      "tensor([[0.4992]], grad_fn=<AddmmBackward0>) tensor(0.5854)\n",
      "tensor([[0.3853]], grad_fn=<AddmmBackward0>) tensor(0.5508)\n",
      "tensor([[-0.9062]], grad_fn=<AddmmBackward0>) tensor(-0.7612)\n",
      "tensor([[0.7660]], grad_fn=<AddmmBackward0>) tensor(0.5819)\n",
      "tensor([[-0.5625]], grad_fn=<AddmmBackward0>) tensor(-0.5182)\n",
      "tensor([[-0.4422]], grad_fn=<AddmmBackward0>) tensor(-0.2848)\n",
      "tensor([[-0.6408]], grad_fn=<AddmmBackward0>) tensor(-0.8811)\n",
      "tensor([[0.0959]], grad_fn=<AddmmBackward0>) tensor(0.1607)\n",
      "tensor([[-0.5134]], grad_fn=<AddmmBackward0>) tensor(-0.4879)\n",
      "tensor([[-0.3275]], grad_fn=<AddmmBackward0>) tensor(-0.3713)\n",
      "tensor([[-0.0144]], grad_fn=<AddmmBackward0>) tensor(0.0151)\n",
      "tensor([[0.4584]], grad_fn=<AddmmBackward0>) tensor(0.5848)\n",
      "tensor([[-0.5079]], grad_fn=<AddmmBackward0>) tensor(-0.5081)\n",
      "tensor([[0.0991]], grad_fn=<AddmmBackward0>) tensor(0.1765)\n",
      "tensor([[-0.0858]], grad_fn=<AddmmBackward0>) tensor(0.0160)\n",
      "tensor([[0.2057]], grad_fn=<AddmmBackward0>) tensor(-0.1721)\n",
      "tensor([[-0.5589]], grad_fn=<AddmmBackward0>) tensor(-0.5181)\n",
      "tensor([[-0.7339]], grad_fn=<AddmmBackward0>) tensor(-0.5696)\n",
      "tensor([[0.5377]], grad_fn=<AddmmBackward0>) tensor(0.7284)\n",
      "tensor([[-0.5957]], grad_fn=<AddmmBackward0>) tensor(-0.4601)\n",
      "tensor([[-0.9442]], grad_fn=<AddmmBackward0>) tensor(-0.3985)\n",
      "tensor([[-0.0783]], grad_fn=<AddmmBackward0>) tensor(0.3854)\n",
      "tensor([[-0.6740]], grad_fn=<AddmmBackward0>) tensor(-0.4223)\n",
      "tensor([[-0.9797]], grad_fn=<AddmmBackward0>) tensor(-0.9078)\n",
      "tensor([[0.2435]], grad_fn=<AddmmBackward0>) tensor(0.3657)\n",
      "tensor([[-0.1966]], grad_fn=<AddmmBackward0>) tensor(0.5162)\n",
      "tensor([[-0.3443]], grad_fn=<AddmmBackward0>) tensor(-0.2661)\n",
      "tensor([[-0.6754]], grad_fn=<AddmmBackward0>) tensor(-0.5539)\n",
      "tensor([[-0.7335]], grad_fn=<AddmmBackward0>) tensor(-0.4513)\n",
      "tensor([[-0.3494]], grad_fn=<AddmmBackward0>) tensor(-0.4651)\n",
      "tensor([[-0.7109]], grad_fn=<AddmmBackward0>) tensor(-0.6708)\n",
      "tensor([[0.2498]], grad_fn=<AddmmBackward0>) tensor(0.3512)\n",
      "tensor([[0.4504]], grad_fn=<AddmmBackward0>) tensor(0.7271)\n",
      "tensor([[0.3713]], grad_fn=<AddmmBackward0>) tensor(0.7985)\n",
      "tensor([[-0.5507]], grad_fn=<AddmmBackward0>) tensor(-0.8383)\n",
      "tensor([[-0.6520]], grad_fn=<AddmmBackward0>) tensor(-0.4964)\n",
      "tensor([[-0.2151]], grad_fn=<AddmmBackward0>) tensor(-0.0220)\n",
      "tensor([[-0.8099]], grad_fn=<AddmmBackward0>) tensor(-0.5532)\n",
      "tensor([[-1.2834]], grad_fn=<AddmmBackward0>) tensor(-0.9593)\n",
      "tensor([[-0.6942]], grad_fn=<AddmmBackward0>) tensor(-0.4736)\n",
      "tensor([[-0.1915]], grad_fn=<AddmmBackward0>) tensor(-0.3865)\n",
      "tensor([[-0.6318]], grad_fn=<AddmmBackward0>) tensor(-0.4634)\n",
      "tensor([[0.1220]], grad_fn=<AddmmBackward0>) tensor(0.0085)\n",
      "tensor([[-0.0918]], grad_fn=<AddmmBackward0>) tensor(0.1126)\n",
      "tensor([[-0.6202]], grad_fn=<AddmmBackward0>) tensor(-0.8898)\n",
      "tensor([[-0.3875]], grad_fn=<AddmmBackward0>) tensor(-0.3681)\n",
      "tensor([[-0.9663]], grad_fn=<AddmmBackward0>) tensor(-0.4095)\n",
      "tensor([[0.3972]], grad_fn=<AddmmBackward0>) tensor(0.5729)\n",
      "tensor([[0.3918]], grad_fn=<AddmmBackward0>) tensor(0.5709)\n",
      "tensor([[0.0029]], grad_fn=<AddmmBackward0>) tensor(0.7257)\n",
      "tensor([[-0.7033]], grad_fn=<AddmmBackward0>) tensor(-0.0728)\n",
      "tensor([[-0.4103]], grad_fn=<AddmmBackward0>) tensor(-0.2949)\n",
      "tensor([[-0.6553]], grad_fn=<AddmmBackward0>) tensor(-0.5061)\n",
      "tensor([[-0.3055]], grad_fn=<AddmmBackward0>) tensor(-0.1458)\n",
      "tensor([[-0.6409]], grad_fn=<AddmmBackward0>) tensor(-0.3465)\n",
      "tensor([[-0.7939]], grad_fn=<AddmmBackward0>) tensor(-0.6393)\n",
      "tensor([[-0.0460]], grad_fn=<AddmmBackward0>) tensor(-0.2703)\n",
      "tensor([[0.0515]], grad_fn=<AddmmBackward0>) tensor(0.1748)\n",
      "tensor([[0.2192]], grad_fn=<AddmmBackward0>) tensor(0.5347)\n",
      "tensor([[0.4453]], grad_fn=<AddmmBackward0>) tensor(0.8056)\n",
      "tensor([[0.3208]], grad_fn=<AddmmBackward0>) tensor(0.5733)\n",
      "tensor([[-0.4449]], grad_fn=<AddmmBackward0>) tensor(-0.3368)\n",
      "tensor([[0.3702]], grad_fn=<AddmmBackward0>) tensor(0.3826)\n",
      "tensor([[-0.5815]], grad_fn=<AddmmBackward0>) tensor(-0.0692)\n",
      "tensor([[0.4042]], grad_fn=<AddmmBackward0>) tensor(0.6592)\n",
      "tensor([[-0.5436]], grad_fn=<AddmmBackward0>) tensor(-0.2805)\n",
      "tensor([[0.2251]], grad_fn=<AddmmBackward0>) tensor(0.8325)\n",
      "tensor([[-0.6456]], grad_fn=<AddmmBackward0>) tensor(-0.4396)\n",
      "tensor([[-0.6260]], grad_fn=<AddmmBackward0>) tensor(-0.7751)\n",
      "tensor([[0.7401]], grad_fn=<AddmmBackward0>) tensor(0.5986)\n",
      "tensor([[0.3930]], grad_fn=<AddmmBackward0>) tensor(0.4394)\n",
      "tensor([[-0.4601]], grad_fn=<AddmmBackward0>) tensor(-0.4410)\n",
      "tensor([[-0.3909]], grad_fn=<AddmmBackward0>) tensor(-0.3397)\n",
      "tensor([[0.2546]], grad_fn=<AddmmBackward0>) tensor(0.7999)\n",
      "tensor([[0.1286]], grad_fn=<AddmmBackward0>) tensor(0.1873)\n",
      "tensor([[0.2275]], grad_fn=<AddmmBackward0>) tensor(0.7941)\n",
      "tensor([[-0.8025]], grad_fn=<AddmmBackward0>) tensor(-0.8886)\n",
      "tensor([[-0.1061]], grad_fn=<AddmmBackward0>) tensor(0.6207)\n",
      "tensor([[-0.7133]], grad_fn=<AddmmBackward0>) tensor(-0.4952)\n",
      "tensor([[-0.5280]], grad_fn=<AddmmBackward0>) tensor(0.5711)\n",
      "tensor([[0.5036]], grad_fn=<AddmmBackward0>) tensor(0.5300)\n",
      "tensor([[-0.4489]], grad_fn=<AddmmBackward0>) tensor(-0.3999)\n",
      "tensor([[0.5002]], grad_fn=<AddmmBackward0>) tensor(0.5989)\n",
      "tensor([[-0.5552]], grad_fn=<AddmmBackward0>) tensor(-0.6727)\n",
      "tensor([[-0.6623]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.2870]], grad_fn=<AddmmBackward0>) tensor(-0.0171)\n",
      "tensor([[-0.6878]], grad_fn=<AddmmBackward0>) tensor(-0.4597)\n",
      "tensor([[-0.2599]], grad_fn=<AddmmBackward0>) tensor(-0.3879)\n",
      "tensor([[0.3535]], grad_fn=<AddmmBackward0>) tensor(0.5380)\n",
      "tensor([[-0.0915]], grad_fn=<AddmmBackward0>) tensor(-0.0598)\n",
      "tensor([[-0.5426]], grad_fn=<AddmmBackward0>) tensor(-0.8064)\n",
      "tensor([[-0.2941]], grad_fn=<AddmmBackward0>) tensor(0.0868)\n",
      "tensor([[-0.8967]], grad_fn=<AddmmBackward0>) tensor(-0.6176)\n",
      "tensor([[0.1329]], grad_fn=<AddmmBackward0>) tensor(0.0369)\n",
      "tensor([[-0.6977]], grad_fn=<AddmmBackward0>) tensor(-0.5816)\n",
      "tensor([[-0.0592]], grad_fn=<AddmmBackward0>) tensor(0.0128)\n",
      "tensor([[0.2618]], grad_fn=<AddmmBackward0>) tensor(0.5898)\n",
      "tensor([[0.6145]], grad_fn=<AddmmBackward0>) tensor(0.6915)\n",
      "tensor([[0.1582]], grad_fn=<AddmmBackward0>) tensor(0.8023)\n",
      "tensor([[0.8053]], grad_fn=<AddmmBackward0>) tensor(0.7443)\n",
      "tensor([[-0.3954]], grad_fn=<AddmmBackward0>) tensor(-0.4941)\n",
      "tensor([[0.1991]], grad_fn=<AddmmBackward0>) tensor(0.5471)\n",
      "tensor([[-0.1388]], grad_fn=<AddmmBackward0>) tensor(-0.3210)\n",
      "tensor([[-0.8275]], grad_fn=<AddmmBackward0>) tensor(-0.9588)\n",
      "tensor([[-0.6920]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.6971]], grad_fn=<AddmmBackward0>) tensor(0.8229)\n",
      "tensor([[-0.1613]], grad_fn=<AddmmBackward0>) tensor(-0.1203)\n",
      "tensor([[-0.2164]], grad_fn=<AddmmBackward0>) tensor(-0.2510)\n",
      "tensor([[-0.5787]], grad_fn=<AddmmBackward0>) tensor(-0.4509)\n",
      "tensor([[0.0871]], grad_fn=<AddmmBackward0>) tensor(0.5893)\n",
      "tensor([[-0.1731]], grad_fn=<AddmmBackward0>) tensor(0.0051)\n",
      "tensor([[-0.1293]], grad_fn=<AddmmBackward0>) tensor(0.3285)\n",
      "tensor([[-0.0888]], grad_fn=<AddmmBackward0>) tensor(0.3886)\n",
      "tensor([[0.1469]], grad_fn=<AddmmBackward0>) tensor(0.3752)\n",
      "tensor([[0.6573]], grad_fn=<AddmmBackward0>) tensor(0.5888)\n",
      "tensor([[-0.1182]], grad_fn=<AddmmBackward0>) tensor(-0.3277)\n",
      "tensor([[-0.0786]], grad_fn=<AddmmBackward0>) tensor(0.3186)\n",
      "tensor([[-0.4607]], grad_fn=<AddmmBackward0>) tensor(-0.4127)\n",
      "tensor([[-0.7716]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.7698]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.2015]], grad_fn=<AddmmBackward0>) tensor(-0.2528)\n",
      "tensor([[-0.7600]], grad_fn=<AddmmBackward0>) tensor(-0.7383)\n",
      "tensor([[0.0402]], grad_fn=<AddmmBackward0>) tensor(0.7727)\n",
      "tensor([[-0.4297]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.5875]], grad_fn=<AddmmBackward0>) tensor(-0.5153)\n",
      "tensor([[-0.7738]], grad_fn=<AddmmBackward0>) tensor(-0.7864)\n",
      "tensor([[-0.5358]], grad_fn=<AddmmBackward0>) tensor(-0.3996)\n",
      "tensor([[0.2789]], grad_fn=<AddmmBackward0>) tensor(0.1351)\n",
      "tensor([[-0.2139]], grad_fn=<AddmmBackward0>) tensor(0.8969)\n",
      "tensor([[0.5716]], grad_fn=<AddmmBackward0>) tensor(0.7351)\n",
      "tensor([[-0.3418]], grad_fn=<AddmmBackward0>) tensor(0.8122)\n",
      "tensor([[-0.3837]], grad_fn=<AddmmBackward0>) tensor(-0.3513)\n",
      "tensor([[-0.5579]], grad_fn=<AddmmBackward0>) tensor(-0.9327)\n",
      "tensor([[0.6787]], grad_fn=<AddmmBackward0>) tensor(0.6203)\n",
      "tensor([[-0.6133]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.0476]], grad_fn=<AddmmBackward0>) tensor(-0.0570)\n",
      "tensor([[-0.6848]], grad_fn=<AddmmBackward0>) tensor(-0.4863)\n",
      "tensor([[-0.2724]], grad_fn=<AddmmBackward0>) tensor(-0.2827)\n",
      "tensor([[-0.7276]], grad_fn=<AddmmBackward0>) tensor(-0.5812)\n",
      "tensor([[0.5695]], grad_fn=<AddmmBackward0>) tensor(0.5296)\n",
      "tensor([[-0.8526]], grad_fn=<AddmmBackward0>) tensor(-0.4505)\n",
      "tensor([[0.3935]], grad_fn=<AddmmBackward0>) tensor(0.5709)\n",
      "tensor([[-0.6889]], grad_fn=<AddmmBackward0>) tensor(-0.6822)\n",
      "tensor([[-0.5249]], grad_fn=<AddmmBackward0>) tensor(-0.5379)\n",
      "tensor([[-0.6885]], grad_fn=<AddmmBackward0>) tensor(-0.5653)\n",
      "tensor([[-0.2456]], grad_fn=<AddmmBackward0>) tensor(-0.3470)\n",
      "tensor([[-0.5237]], grad_fn=<AddmmBackward0>) tensor(-0.2336)\n",
      "tensor([[0.0233]], grad_fn=<AddmmBackward0>) tensor(0.2377)\n",
      "tensor([[0.3501]], grad_fn=<AddmmBackward0>) tensor(0.7351)\n",
      "tensor([[0.3000]], grad_fn=<AddmmBackward0>) tensor(0.8001)\n",
      "tensor([[-0.2023]], grad_fn=<AddmmBackward0>) tensor(0.0183)\n",
      "tensor([[-0.6620]], grad_fn=<AddmmBackward0>) tensor(-0.5962)\n",
      "tensor([[-0.5035]], grad_fn=<AddmmBackward0>) tensor(0.2895)\n",
      "tensor([[-0.3603]], grad_fn=<AddmmBackward0>) tensor(-0.4790)\n",
      "tensor([[0.0727]], grad_fn=<AddmmBackward0>) tensor(1.)\n",
      "tensor([[0.4543]], grad_fn=<AddmmBackward0>) tensor(0.5626)\n",
      "tensor([[-0.2985]], grad_fn=<AddmmBackward0>) tensor(-0.6308)\n",
      "tensor([[0.4274]], grad_fn=<AddmmBackward0>) tensor(0.3709)\n",
      "tensor([[0.2004]], grad_fn=<AddmmBackward0>) tensor(0.5559)\n",
      "tensor([[-0.3725]], grad_fn=<AddmmBackward0>) tensor(-0.9182)\n",
      "tensor([[0.2484]], grad_fn=<AddmmBackward0>) tensor(0.1068)\n",
      "tensor([[0.4473]], grad_fn=<AddmmBackward0>) tensor(0.2373)\n",
      "tensor([[0.3405]], grad_fn=<AddmmBackward0>) tensor(0.6936)\n",
      "tensor([[-0.4497]], grad_fn=<AddmmBackward0>) tensor(-0.5259)\n",
      "tensor([[-0.9622]], grad_fn=<AddmmBackward0>) tensor(-0.9161)\n",
      "tensor([[-0.4790]], grad_fn=<AddmmBackward0>) tensor(-0.9633)\n",
      "tensor([[-0.1311]], grad_fn=<AddmmBackward0>) tensor(0.7379)\n",
      "tensor([[-0.5416]], grad_fn=<AddmmBackward0>) tensor(-0.4189)\n",
      "tensor([[-0.7863]], grad_fn=<AddmmBackward0>) tensor(-0.8455)\n",
      "tensor([[-0.6073]], grad_fn=<AddmmBackward0>) tensor(-0.6693)\n",
      "tensor([[-0.8485]], grad_fn=<AddmmBackward0>) tensor(0.5298)\n",
      "tensor([[-0.5897]], grad_fn=<AddmmBackward0>) tensor(-0.6349)\n",
      "tensor([[-0.0278]], grad_fn=<AddmmBackward0>) tensor(-0.1671)\n",
      "tensor([[0.5669]], grad_fn=<AddmmBackward0>) tensor(0.5562)\n",
      "tensor([[0.4901]], grad_fn=<AddmmBackward0>) tensor(0.7782)\n",
      "tensor([[-0.0760]], grad_fn=<AddmmBackward0>) tensor(-0.3225)\n",
      "tensor([[0.4921]], grad_fn=<AddmmBackward0>) tensor(0.4722)\n",
      "tensor([[-0.7711]], grad_fn=<AddmmBackward0>) tensor(-0.8606)\n",
      "tensor([[-0.0941]], grad_fn=<AddmmBackward0>) tensor(0.5113)\n",
      "tensor([[-0.8610]], grad_fn=<AddmmBackward0>) tensor(-0.4015)\n",
      "tensor([[-0.4108]], grad_fn=<AddmmBackward0>) tensor(-0.6467)\n",
      "tensor([[-0.4460]], grad_fn=<AddmmBackward0>) tensor(-0.4482)\n",
      "tensor([[0.0687]], grad_fn=<AddmmBackward0>) tensor(0.2641)\n",
      "tensor([[0.2810]], grad_fn=<AddmmBackward0>) tensor(0.5441)\n",
      "tensor([[0.1562]], grad_fn=<AddmmBackward0>) tensor(0.2022)\n",
      "tensor([[-0.4887]], grad_fn=<AddmmBackward0>) tensor(0.2296)\n",
      "tensor([[0.4174]], grad_fn=<AddmmBackward0>) tensor(0.5030)\n",
      "tensor([[-0.5068]], grad_fn=<AddmmBackward0>) tensor(-0.6638)\n",
      "tensor([[-0.0210]], grad_fn=<AddmmBackward0>) tensor(0.0136)\n",
      "tensor([[0.6057]], grad_fn=<AddmmBackward0>) tensor(0.5394)\n",
      "tensor([[0.1110]], grad_fn=<AddmmBackward0>) tensor(0.6223)\n",
      "tensor([[-0.0480]], grad_fn=<AddmmBackward0>) tensor(0.0331)\n",
      "tensor([[-0.2196]], grad_fn=<AddmmBackward0>) tensor(-0.3032)\n",
      "tensor([[-0.7806]], grad_fn=<AddmmBackward0>) tensor(-0.8307)\n",
      "tensor([[0.3000]], grad_fn=<AddmmBackward0>) tensor(0.2203)\n",
      "tensor([[0.2897]], grad_fn=<AddmmBackward0>) tensor(0.5705)\n",
      "tensor([[0.3223]], grad_fn=<AddmmBackward0>) tensor(0.7870)\n",
      "tensor([[-1.0853]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.3768]], grad_fn=<AddmmBackward0>) tensor(-0.6235)\n",
      "tensor([[0.6856]], grad_fn=<AddmmBackward0>) tensor(0.7771)\n",
      "tensor([[-0.6755]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.3403]], grad_fn=<AddmmBackward0>) tensor(0.5325)\n",
      "tensor([[0.1041]], grad_fn=<AddmmBackward0>) tensor(0.2530)\n",
      "tensor([[0.4232]], grad_fn=<AddmmBackward0>) tensor(0.5724)\n",
      "tensor([[-0.4557]], grad_fn=<AddmmBackward0>) tensor(-0.2998)\n",
      "tensor([[-0.0910]], grad_fn=<AddmmBackward0>) tensor(0.2973)\n",
      "tensor([[-0.5912]], grad_fn=<AddmmBackward0>) tensor(-0.8576)\n",
      "tensor([[-0.5872]], grad_fn=<AddmmBackward0>) tensor(-0.3149)\n",
      "tensor([[-0.9245]], grad_fn=<AddmmBackward0>) tensor(-0.7334)\n",
      "tensor([[0.7201]], grad_fn=<AddmmBackward0>) tensor(0.7044)\n",
      "tensor([[-0.7890]], grad_fn=<AddmmBackward0>) tensor(-0.6873)\n",
      "tensor([[0.4060]], grad_fn=<AddmmBackward0>) tensor(0.6818)\n",
      "tensor([[-0.5315]], grad_fn=<AddmmBackward0>) tensor(-0.4854)\n",
      "tensor([[-0.6145]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.6884]], grad_fn=<AddmmBackward0>) tensor(-0.6511)\n",
      "tensor([[0.5256]], grad_fn=<AddmmBackward0>) tensor(0.5395)\n",
      "tensor([[-0.5693]], grad_fn=<AddmmBackward0>) tensor(-0.4306)\n",
      "tensor([[-0.6673]], grad_fn=<AddmmBackward0>) tensor(-0.5589)\n",
      "tensor([[-0.0742]], grad_fn=<AddmmBackward0>) tensor(0.6963)\n",
      "tensor([[0.4721]], grad_fn=<AddmmBackward0>) tensor(0.6905)\n",
      "tensor([[0.3517]], grad_fn=<AddmmBackward0>) tensor(0.7383)\n",
      "tensor([[-0.0598]], grad_fn=<AddmmBackward0>) tensor(-0.1833)\n",
      "tensor([[-0.7324]], grad_fn=<AddmmBackward0>) tensor(-0.6723)\n",
      "tensor([[-0.0360]], grad_fn=<AddmmBackward0>) tensor(0.0165)\n",
      "tensor([[-0.6079]], grad_fn=<AddmmBackward0>) tensor(-0.9561)\n",
      "tensor([[-0.6868]], grad_fn=<AddmmBackward0>) tensor(-0.6514)\n",
      "tensor([[0.8113]], grad_fn=<AddmmBackward0>) tensor(0.6029)\n",
      "tensor([[-0.5190]], grad_fn=<AddmmBackward0>) tensor(-0.1080)\n",
      "tensor([[-0.6265]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.0452]], grad_fn=<AddmmBackward0>) tensor(0.5607)\n",
      "tensor([[-0.1998]], grad_fn=<AddmmBackward0>) tensor(-0.0038)\n",
      "tensor([[0.3128]], grad_fn=<AddmmBackward0>) tensor(0.5796)\n",
      "tensor([[-0.4744]], grad_fn=<AddmmBackward0>) tensor(-0.4177)\n",
      "tensor([[-0.7537]], grad_fn=<AddmmBackward0>) tensor(-0.6925)\n",
      "tensor([[-0.4995]], grad_fn=<AddmmBackward0>) tensor(-0.4039)\n",
      "tensor([[-0.6708]], grad_fn=<AddmmBackward0>) tensor(-0.8915)\n",
      "tensor([[-0.5924]], grad_fn=<AddmmBackward0>) tensor(-0.4582)\n",
      "tensor([[-0.0778]], grad_fn=<AddmmBackward0>) tensor(0.0036)\n",
      "tensor([[-0.8270]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.6934]], grad_fn=<AddmmBackward0>) tensor(-0.9170)\n",
      "tensor([[-0.0503]], grad_fn=<AddmmBackward0>) tensor(-0.1206)\n",
      "tensor([[-0.8181]], grad_fn=<AddmmBackward0>) tensor(-0.6410)\n",
      "tensor([[-0.2494]], grad_fn=<AddmmBackward0>) tensor(-0.2505)\n",
      "tensor([[-0.7177]], grad_fn=<AddmmBackward0>) tensor(-0.5991)\n",
      "tensor([[-0.4443]], grad_fn=<AddmmBackward0>) tensor(0.0759)\n",
      "tensor([[-0.7559]], grad_fn=<AddmmBackward0>) tensor(-0.9444)\n",
      "tensor([[-0.5916]], grad_fn=<AddmmBackward0>) tensor(-0.3460)\n",
      "tensor([[-0.6089]], grad_fn=<AddmmBackward0>) tensor(-0.4593)\n",
      "tensor([[-0.1887]], grad_fn=<AddmmBackward0>) tensor(0.1181)\n",
      "tensor([[-0.6461]], grad_fn=<AddmmBackward0>) tensor(-0.4859)\n",
      "tensor([[0.2900]], grad_fn=<AddmmBackward0>) tensor(0.6888)\n",
      "tensor([[-0.9269]], grad_fn=<AddmmBackward0>) tensor(-0.7725)\n",
      "tensor([[-0.5099]], grad_fn=<AddmmBackward0>) tensor(-0.5078)\n",
      "tensor([[-0.2093]], grad_fn=<AddmmBackward0>) tensor(-0.2130)\n",
      "tensor([[-0.6500]], grad_fn=<AddmmBackward0>) tensor(-0.9787)\n",
      "tensor([[0.0170]], grad_fn=<AddmmBackward0>) tensor(0.0051)\n",
      "tensor([[-0.6196]], grad_fn=<AddmmBackward0>) tensor(-0.5005)\n",
      "tensor([[-0.6689]], grad_fn=<AddmmBackward0>) tensor(-0.8168)\n",
      "tensor([[-0.5693]], grad_fn=<AddmmBackward0>) tensor(-0.4971)\n",
      "tensor([[0.4188]], grad_fn=<AddmmBackward0>) tensor(0.5404)\n",
      "tensor([[-0.0203]], grad_fn=<AddmmBackward0>) tensor(0.0256)\n",
      "tensor([[-0.8315]], grad_fn=<AddmmBackward0>) tensor(-0.7145)\n",
      "tensor([[-0.7093]], grad_fn=<AddmmBackward0>) tensor(-0.8929)\n",
      "tensor([[-0.6066]], grad_fn=<AddmmBackward0>) tensor(-0.6980)\n",
      "tensor([[-0.5960]], grad_fn=<AddmmBackward0>) tensor(-0.6045)\n",
      "tensor([[-0.6831]], grad_fn=<AddmmBackward0>) tensor(-0.5364)\n",
      "tensor([[-0.0055]], grad_fn=<AddmmBackward0>) tensor(-0.0076)\n",
      "tensor([[-0.7697]], grad_fn=<AddmmBackward0>) tensor(0.0294)\n",
      "tensor([[-0.3886]], grad_fn=<AddmmBackward0>) tensor(-0.1556)\n",
      "tensor([[0.3182]], grad_fn=<AddmmBackward0>) tensor(0.5814)\n",
      "tensor([[-0.6008]], grad_fn=<AddmmBackward0>) tensor(-0.6683)\n",
      "tensor([[-0.8049]], grad_fn=<AddmmBackward0>) tensor(-0.8035)\n",
      "tensor([[-0.3339]], grad_fn=<AddmmBackward0>) tensor(-0.2525)\n",
      "tensor([[-0.5519]], grad_fn=<AddmmBackward0>) tensor(-0.8390)\n",
      "tensor([[0.5948]], grad_fn=<AddmmBackward0>) tensor(0.5843)\n",
      "tensor([[0.1339]], grad_fn=<AddmmBackward0>) tensor(0.0620)\n",
      "tensor([[-0.2102]], grad_fn=<AddmmBackward0>) tensor(-0.8479)\n",
      "tensor([[0.5562]], grad_fn=<AddmmBackward0>) tensor(0.7087)\n",
      "tensor([[-0.5912]], grad_fn=<AddmmBackward0>) tensor(-0.6603)\n",
      "tensor([[0.7752]], grad_fn=<AddmmBackward0>) tensor(0.6704)\n",
      "tensor([[-0.5458]], grad_fn=<AddmmBackward0>) tensor(-0.6474)\n",
      "tensor([[0.3819]], grad_fn=<AddmmBackward0>) tensor(0.7857)\n",
      "tensor([[-0.3286]], grad_fn=<AddmmBackward0>) tensor(0.0806)\n",
      "tensor([[-0.3422]], grad_fn=<AddmmBackward0>) tensor(-0.4541)\n",
      "tensor([[0.0255]], grad_fn=<AddmmBackward0>) tensor(0.7650)\n",
      "tensor([[-0.7598]], grad_fn=<AddmmBackward0>) tensor(-0.0069)\n",
      "tensor([[-0.1242]], grad_fn=<AddmmBackward0>) tensor(0.1417)\n",
      "tensor([[-0.1980]], grad_fn=<AddmmBackward0>) tensor(0.7568)\n",
      "tensor([[-0.6309]], grad_fn=<AddmmBackward0>) tensor(-0.5296)\n",
      "tensor([[-0.0553]], grad_fn=<AddmmBackward0>) tensor(-0.0698)\n",
      "tensor([[-0.3604]], grad_fn=<AddmmBackward0>) tensor(-0.8739)\n",
      "tensor([[0.0423]], grad_fn=<AddmmBackward0>) tensor(0.0084)\n",
      "tensor([[0.6539]], grad_fn=<AddmmBackward0>) tensor(0.7275)\n",
      "tensor([[-0.5035]], grad_fn=<AddmmBackward0>) tensor(-0.5006)\n",
      "tensor([[-0.5912]], grad_fn=<AddmmBackward0>) tensor(-0.0034)\n",
      "tensor([[0.2650]], grad_fn=<AddmmBackward0>) tensor(0.8341)\n",
      "tensor([[0.5131]], grad_fn=<AddmmBackward0>) tensor(0.7381)\n",
      "tensor([[-0.0422]], grad_fn=<AddmmBackward0>) tensor(-0.4447)\n",
      "tensor([[-0.3814]], grad_fn=<AddmmBackward0>) tensor(-0.1602)\n",
      "tensor([[0.4289]], grad_fn=<AddmmBackward0>) tensor(0.5577)\n",
      "tensor([[-0.4976]], grad_fn=<AddmmBackward0>) tensor(-0.1834)\n",
      "tensor([[0.1139]], grad_fn=<AddmmBackward0>) tensor(0.1911)\n",
      "tensor([[0.2997]], grad_fn=<AddmmBackward0>) tensor(0.6460)\n",
      "tensor([[0.0475]], grad_fn=<AddmmBackward0>) tensor(0.7380)\n",
      "tensor([[-0.5774]], grad_fn=<AddmmBackward0>) tensor(-0.9177)\n",
      "tensor([[0.0947]], grad_fn=<AddmmBackward0>) tensor(0.3694)\n",
      "tensor([[-0.7582]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.4270]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.8892]], grad_fn=<AddmmBackward0>) tensor(-0.4233)\n",
      "tensor([[0.2692]], grad_fn=<AddmmBackward0>) tensor(0.4322)\n",
      "tensor([[0.7942]], grad_fn=<AddmmBackward0>) tensor(0.7439)\n",
      "tensor([[-0.4752]], grad_fn=<AddmmBackward0>) tensor(-0.1047)\n",
      "tensor([[-1.0153]], grad_fn=<AddmmBackward0>) tensor(-0.8562)\n",
      "tensor([[-0.4706]], grad_fn=<AddmmBackward0>) tensor(-0.4826)\n",
      "tensor([[-0.5736]], grad_fn=<AddmmBackward0>) tensor(-0.4422)\n",
      "tensor([[0.4254]], grad_fn=<AddmmBackward0>) tensor(0.4050)\n",
      "tensor([[0.6250]], grad_fn=<AddmmBackward0>) tensor(0.6876)\n",
      "tensor([[-0.5217]], grad_fn=<AddmmBackward0>) tensor(-0.9753)\n",
      "tensor([[0.0855]], grad_fn=<AddmmBackward0>) tensor(-0.0702)\n",
      "tensor([[-0.0385]], grad_fn=<AddmmBackward0>) tensor(-0.1628)\n",
      "tensor([[-0.1093]], grad_fn=<AddmmBackward0>) tensor(0.0328)\n",
      "tensor([[-0.6446]], grad_fn=<AddmmBackward0>) tensor(-0.5394)\n",
      "tensor([[-0.5085]], grad_fn=<AddmmBackward0>) tensor(-0.4594)\n",
      "tensor([[-0.0639]], grad_fn=<AddmmBackward0>) tensor(-0.3162)\n",
      "tensor([[0.3315]], grad_fn=<AddmmBackward0>) tensor(0.1255)\n",
      "tensor([[-0.5987]], grad_fn=<AddmmBackward0>) tensor(-0.7486)\n",
      "tensor([[0.2513]], grad_fn=<AddmmBackward0>) tensor(0.5613)\n",
      "tensor([[-0.6411]], grad_fn=<AddmmBackward0>) tensor(-0.4892)\n",
      "tensor([[0.3815]], grad_fn=<AddmmBackward0>) tensor(0.5755)\n",
      "tensor([[0.0290]], grad_fn=<AddmmBackward0>) tensor(0.0358)\n",
      "tensor([[0.9774]], grad_fn=<AddmmBackward0>) tensor(0.6052)\n",
      "tensor([[-0.3532]], grad_fn=<AddmmBackward0>) tensor(-0.3812)\n",
      "tensor([[0.5539]], grad_fn=<AddmmBackward0>) tensor(0.5822)\n",
      "tensor([[-0.3927]], grad_fn=<AddmmBackward0>) tensor(-0.1698)\n",
      "tensor([[-0.5304]], grad_fn=<AddmmBackward0>) tensor(-0.6571)\n",
      "tensor([[-0.9397]], grad_fn=<AddmmBackward0>) tensor(-0.3756)\n",
      "tensor([[-0.6480]], grad_fn=<AddmmBackward0>) tensor(-0.7295)\n",
      "tensor([[0.3730]], grad_fn=<AddmmBackward0>) tensor(0.5395)\n",
      "tensor([[-0.5763]], grad_fn=<AddmmBackward0>) tensor(-0.4533)\n",
      "tensor([[-0.8203]], grad_fn=<AddmmBackward0>) tensor(-0.4745)\n",
      "tensor([[-0.6847]], grad_fn=<AddmmBackward0>) tensor(0.4786)\n",
      "tensor([[-1.3843]], grad_fn=<AddmmBackward0>) tensor(-0.9517)\n",
      "tensor([[-0.0601]], grad_fn=<AddmmBackward0>) tensor(0.1184)\n",
      "tensor([[-0.7232]], grad_fn=<AddmmBackward0>) tensor(-0.7129)\n",
      "tensor([[0.4971]], grad_fn=<AddmmBackward0>) tensor(0.5699)\n",
      "tensor([[-0.8099]], grad_fn=<AddmmBackward0>) tensor(-0.7179)\n",
      "tensor([[-0.6597]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.5711]], grad_fn=<AddmmBackward0>) tensor(-0.9521)\n",
      "tensor([[0.0341]], grad_fn=<AddmmBackward0>) tensor(-0.8065)\n",
      "tensor([[-0.7447]], grad_fn=<AddmmBackward0>) tensor(-0.4575)\n",
      "tensor([[-0.7092]], grad_fn=<AddmmBackward0>) tensor(-0.8563)\n",
      "tensor([[-0.6125]], grad_fn=<AddmmBackward0>) tensor(-0.4465)\n",
      "tensor([[-0.0089]], grad_fn=<AddmmBackward0>) tensor(-0.1895)\n",
      "tensor([[-0.6141]], grad_fn=<AddmmBackward0>) tensor(-0.8583)\n",
      "tensor([[-0.6287]], grad_fn=<AddmmBackward0>) tensor(-0.6247)\n",
      "tensor([[0.3301]], grad_fn=<AddmmBackward0>) tensor(0.5725)\n",
      "tensor([[0.4876]], grad_fn=<AddmmBackward0>) tensor(0.5405)\n",
      "tensor([[-0.1505]], grad_fn=<AddmmBackward0>) tensor(0.0189)\n",
      "tensor([[-0.0182]], grad_fn=<AddmmBackward0>) tensor(0.0476)\n",
      "tensor([[0.4970]], grad_fn=<AddmmBackward0>) tensor(0.8291)\n",
      "tensor([[0.2348]], grad_fn=<AddmmBackward0>) tensor(0.5418)\n",
      "tensor([[0.0022]], grad_fn=<AddmmBackward0>) tensor(0.0125)\n",
      "tensor([[-0.6577]], grad_fn=<AddmmBackward0>) tensor(-0.6109)\n",
      "tensor([[0.2224]], grad_fn=<AddmmBackward0>) tensor(0.5713)\n",
      "tensor([[-0.2297]], grad_fn=<AddmmBackward0>) tensor(0.6202)\n",
      "tensor([[-0.3652]], grad_fn=<AddmmBackward0>) tensor(-0.1665)\n",
      "tensor([[0.4580]], grad_fn=<AddmmBackward0>) tensor(0.7384)\n",
      "tensor([[0.4470]], grad_fn=<AddmmBackward0>) tensor(0.2358)\n",
      "tensor([[-0.3080]], grad_fn=<AddmmBackward0>) tensor(0.3027)\n",
      "tensor([[0.5648]], grad_fn=<AddmmBackward0>) tensor(0.5284)\n",
      "tensor([[0.0357]], grad_fn=<AddmmBackward0>) tensor(0.3374)\n",
      "tensor([[0.4486]], grad_fn=<AddmmBackward0>) tensor(0.5164)\n",
      "tensor([[0.3761]], grad_fn=<AddmmBackward0>) tensor(0.3869)\n",
      "tensor([[0.1086]], grad_fn=<AddmmBackward0>) tensor(0.3277)\n",
      "tensor([[-0.0267]], grad_fn=<AddmmBackward0>) tensor(-0.1536)\n",
      "tensor([[0.5696]], grad_fn=<AddmmBackward0>) tensor(0.3776)\n",
      "tensor([[-0.4861]], grad_fn=<AddmmBackward0>) tensor(-0.2450)\n",
      "tensor([[0.0641]], grad_fn=<AddmmBackward0>) tensor(0.2862)\n",
      "tensor([[-0.0215]], grad_fn=<AddmmBackward0>) tensor(0.0141)\n",
      "tensor([[-0.0133]], grad_fn=<AddmmBackward0>) tensor(-0.0091)\n",
      "tensor([[0.1113]], grad_fn=<AddmmBackward0>) tensor(0.1817)\n",
      "tensor([[0.3622]], grad_fn=<AddmmBackward0>) tensor(0.5376)\n",
      "tensor([[0.1504]], grad_fn=<AddmmBackward0>) tensor(0.6615)\n",
      "tensor([[0.8859]], grad_fn=<AddmmBackward0>) tensor(0.9162)\n",
      "tensor([[-0.5322]], grad_fn=<AddmmBackward0>) tensor(-0.2962)\n",
      "tensor([[-0.1836]], grad_fn=<AddmmBackward0>) tensor(-0.5312)\n",
      "tensor([[-0.1764]], grad_fn=<AddmmBackward0>) tensor(-0.1472)\n",
      "tensor([[-0.3559]], grad_fn=<AddmmBackward0>) tensor(0.6738)\n",
      "tensor([[0.2664]], grad_fn=<AddmmBackward0>) tensor(0.3649)\n",
      "tensor([[-0.4020]], grad_fn=<AddmmBackward0>) tensor(0.2386)\n",
      "tensor([[0.2861]], grad_fn=<AddmmBackward0>) tensor(0.5593)\n",
      "tensor([[0.6099]], grad_fn=<AddmmBackward0>) tensor(0.6017)\n",
      "tensor([[-0.6264]], grad_fn=<AddmmBackward0>) tensor(-0.5097)\n",
      "tensor([[0.3713]], grad_fn=<AddmmBackward0>) tensor(0.2587)\n",
      "tensor([[0.6351]], grad_fn=<AddmmBackward0>) tensor(0.8019)\n",
      "tensor([[-0.6028]], grad_fn=<AddmmBackward0>) tensor(-0.6207)\n",
      "tensor([[0.2537]], grad_fn=<AddmmBackward0>) tensor(0.5040)\n",
      "tensor([[-0.5346]], grad_fn=<AddmmBackward0>) tensor(-0.5008)\n",
      "tensor([[0.0626]], grad_fn=<AddmmBackward0>) tensor(0.0501)\n",
      "tensor([[0.2637]], grad_fn=<AddmmBackward0>) tensor(0.8062)\n",
      "tensor([[-0.1033]], grad_fn=<AddmmBackward0>) tensor(-0.1674)\n",
      "tensor([[-0.1099]], grad_fn=<AddmmBackward0>) tensor(-0.0013)\n",
      "tensor([[-0.9595]], grad_fn=<AddmmBackward0>) tensor(-0.8625)\n",
      "tensor([[0.0736]], grad_fn=<AddmmBackward0>) tensor(0.5868)\n",
      "tensor([[-0.3240]], grad_fn=<AddmmBackward0>) tensor(-0.3958)\n",
      "tensor([[-0.3284]], grad_fn=<AddmmBackward0>) tensor(0.2071)\n",
      "tensor([[0.1943]], grad_fn=<AddmmBackward0>) tensor(0.6516)\n",
      "tensor([[0.5573]], grad_fn=<AddmmBackward0>) tensor(0.5280)\n",
      "tensor([[-0.0072]], grad_fn=<AddmmBackward0>) tensor(0.6749)\n",
      "tensor([[-0.1253]], grad_fn=<AddmmBackward0>) tensor(0.0344)\n",
      "tensor([[-0.3749]], grad_fn=<AddmmBackward0>) tensor(-0.3762)\n",
      "tensor([[-0.6980]], grad_fn=<AddmmBackward0>) tensor(-0.7605)\n",
      "tensor([[-0.3590]], grad_fn=<AddmmBackward0>) tensor(-0.3680)\n",
      "tensor([[0.0938]], grad_fn=<AddmmBackward0>) tensor(0.6990)\n",
      "tensor([[0.3723]], grad_fn=<AddmmBackward0>) tensor(0.7129)\n",
      "tensor([[0.0309]], grad_fn=<AddmmBackward0>) tensor(0.3552)\n",
      "tensor([[0.2348]], grad_fn=<AddmmBackward0>) tensor(0.5417)\n",
      "tensor([[-0.7418]], grad_fn=<AddmmBackward0>) tensor(-0.0928)\n",
      "tensor([[0.3979]], grad_fn=<AddmmBackward0>) tensor(0.5782)\n",
      "tensor([[-0.6728]], grad_fn=<AddmmBackward0>) tensor(-0.2894)\n",
      "tensor([[-0.2850]], grad_fn=<AddmmBackward0>) tensor(-0.4510)\n",
      "tensor([[-0.4538]], grad_fn=<AddmmBackward0>) tensor(-0.6701)\n",
      "tensor([[-0.7890]], grad_fn=<AddmmBackward0>) tensor(-0.9301)\n",
      "tensor([[0.6445]], grad_fn=<AddmmBackward0>) tensor(0.7149)\n",
      "tensor([[-0.0493]], grad_fn=<AddmmBackward0>) tensor(-0.0684)\n",
      "tensor([[0.5895]], grad_fn=<AddmmBackward0>) tensor(0.5737)\n",
      "tensor([[0.3321]], grad_fn=<AddmmBackward0>) tensor(0.8061)\n",
      "tensor([[-0.5322]], grad_fn=<AddmmBackward0>) tensor(-0.4888)\n",
      "tensor([[0.3193]], grad_fn=<AddmmBackward0>) tensor(0.5801)\n",
      "tensor([[-0.5392]], grad_fn=<AddmmBackward0>) tensor(-0.5463)\n",
      "tensor([[0.0256]], grad_fn=<AddmmBackward0>) tensor(-0.0172)\n",
      "tensor([[-0.7355]], grad_fn=<AddmmBackward0>) tensor(-0.8515)\n",
      "tensor([[-0.5902]], grad_fn=<AddmmBackward0>) tensor(-0.4917)\n",
      "tensor([[0.5601]], grad_fn=<AddmmBackward0>) tensor(0.6896)\n",
      "tensor([[-0.0163]], grad_fn=<AddmmBackward0>) tensor(-0.8646)\n",
      "tensor([[-0.2049]], grad_fn=<AddmmBackward0>) tensor(0.7637)\n",
      "tensor([[-0.6746]], grad_fn=<AddmmBackward0>) tensor(-0.7654)\n",
      "tensor([[-0.8068]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.2320]], grad_fn=<AddmmBackward0>) tensor(-0.8833)\n",
      "tensor([[0.4318]], grad_fn=<AddmmBackward0>) tensor(0.6694)\n",
      "tensor([[-0.4270]], grad_fn=<AddmmBackward0>) tensor(-0.1672)\n",
      "tensor([[-0.7577]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.2998]], grad_fn=<AddmmBackward0>) tensor(0.2302)\n",
      "tensor([[0.1405]], grad_fn=<AddmmBackward0>) tensor(0.5717)\n",
      "tensor([[0.3771]], grad_fn=<AddmmBackward0>) tensor(0.2361)\n",
      "tensor([[-0.8872]], grad_fn=<AddmmBackward0>) tensor(-0.3767)\n",
      "tensor([[-0.6748]], grad_fn=<AddmmBackward0>) tensor(-0.9581)\n",
      "tensor([[-0.9443]], grad_fn=<AddmmBackward0>) tensor(-0.8615)\n",
      "tensor([[-0.8777]], grad_fn=<AddmmBackward0>) tensor(-0.4775)\n",
      "tensor([[-0.6583]], grad_fn=<AddmmBackward0>) tensor(-0.6436)\n",
      "tensor([[-0.7654]], grad_fn=<AddmmBackward0>) tensor(-0.8425)\n",
      "tensor([[-0.8448]], grad_fn=<AddmmBackward0>) tensor(-0.9162)\n",
      "tensor([[-0.7167]], grad_fn=<AddmmBackward0>) tensor(-0.9478)\n",
      "tensor([[-0.3077]], grad_fn=<AddmmBackward0>) tensor(0.4505)\n",
      "tensor([[-0.1052]], grad_fn=<AddmmBackward0>) tensor(-0.0967)\n",
      "tensor([[-0.2440]], grad_fn=<AddmmBackward0>) tensor(-0.1505)\n",
      "tensor([[0.3029]], grad_fn=<AddmmBackward0>) tensor(0.5758)\n",
      "tensor([[0.2918]], grad_fn=<AddmmBackward0>) tensor(0.7152)\n",
      "tensor([[-0.2621]], grad_fn=<AddmmBackward0>) tensor(-0.2821)\n",
      "tensor([[0.5719]], grad_fn=<AddmmBackward0>) tensor(0.7240)\n",
      "tensor([[-0.8899]], grad_fn=<AddmmBackward0>) tensor(-0.4664)\n",
      "tensor([[0.5936]], grad_fn=<AddmmBackward0>) tensor(0.7945)\n",
      "tensor([[0.0117]], grad_fn=<AddmmBackward0>) tensor(-0.0603)\n",
      "tensor([[0.6607]], grad_fn=<AddmmBackward0>) tensor(0.5443)\n",
      "tensor([[0.5509]], grad_fn=<AddmmBackward0>) tensor(0.6008)\n",
      "tensor([[0.2894]], grad_fn=<AddmmBackward0>) tensor(0.5345)\n",
      "tensor([[-0.7346]], grad_fn=<AddmmBackward0>) tensor(-0.7831)\n",
      "tensor([[-0.1720]], grad_fn=<AddmmBackward0>) tensor(-0.1140)\n",
      "tensor([[0.9608]], grad_fn=<AddmmBackward0>) tensor(0.7335)\n",
      "tensor([[0.5508]], grad_fn=<AddmmBackward0>) tensor(0.5871)\n",
      "tensor([[0.0574]], grad_fn=<AddmmBackward0>) tensor(0.6899)\n",
      "tensor([[-0.6682]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.6895]], grad_fn=<AddmmBackward0>) tensor(-0.4470)\n",
      "tensor([[-0.2617]], grad_fn=<AddmmBackward0>) tensor(0.7782)\n",
      "tensor([[-0.5214]], grad_fn=<AddmmBackward0>) tensor(-0.5688)\n",
      "tensor([[0.3924]], grad_fn=<AddmmBackward0>) tensor(0.7855)\n",
      "tensor([[0.0851]], grad_fn=<AddmmBackward0>) tensor(0.5965)\n",
      "tensor([[-1.0486]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.4135]], grad_fn=<AddmmBackward0>) tensor(0.5508)\n",
      "tensor([[-0.2784]], grad_fn=<AddmmBackward0>) tensor(-0.2531)\n",
      "tensor([[-0.5913]], grad_fn=<AddmmBackward0>) tensor(-0.6746)\n",
      "tensor([[-0.0435]], grad_fn=<AddmmBackward0>) tensor(-0.0340)\n",
      "tensor([[-0.7004]], grad_fn=<AddmmBackward0>) tensor(-0.8647)\n",
      "tensor([[0.0052]], grad_fn=<AddmmBackward0>) tensor(0.7160)\n",
      "tensor([[0.0824]], grad_fn=<AddmmBackward0>) tensor(0.6736)\n",
      "tensor([[0.0990]], grad_fn=<AddmmBackward0>) tensor(-0.1380)\n",
      "tensor([[0.0495]], grad_fn=<AddmmBackward0>) tensor(-0.0623)\n",
      "tensor([[-0.6789]], grad_fn=<AddmmBackward0>) tensor(-0.3347)\n",
      "tensor([[-0.1193]], grad_fn=<AddmmBackward0>) tensor(-0.4461)\n",
      "tensor([[-0.6333]], grad_fn=<AddmmBackward0>) tensor(-0.6764)\n",
      "tensor([[-0.0019]], grad_fn=<AddmmBackward0>) tensor(0.2007)\n",
      "tensor([[-0.6736]], grad_fn=<AddmmBackward0>) tensor(-0.4823)\n",
      "tensor([[-0.6953]], grad_fn=<AddmmBackward0>) tensor(-0.4633)\n",
      "tensor([[-0.0723]], grad_fn=<AddmmBackward0>) tensor(0.6673)\n",
      "tensor([[-0.8746]], grad_fn=<AddmmBackward0>) tensor(-0.6863)\n",
      "tensor([[-0.6338]], grad_fn=<AddmmBackward0>) tensor(-0.3054)\n",
      "tensor([[-0.7909]], grad_fn=<AddmmBackward0>) tensor(-0.7100)\n",
      "tensor([[0.2201]], grad_fn=<AddmmBackward0>) tensor(0.6707)\n",
      "tensor([[-0.7126]], grad_fn=<AddmmBackward0>) tensor(-0.7677)\n",
      "tensor([[-0.5555]], grad_fn=<AddmmBackward0>) tensor(-0.4677)\n",
      "tensor([[-0.2765]], grad_fn=<AddmmBackward0>) tensor(-0.2686)\n",
      "tensor([[-0.1898]], grad_fn=<AddmmBackward0>) tensor(0.0249)\n",
      "tensor([[-0.6414]], grad_fn=<AddmmBackward0>) tensor(-0.6377)\n",
      "tensor([[-0.9393]], grad_fn=<AddmmBackward0>) tensor(-0.9647)\n",
      "tensor([[-0.5836]], grad_fn=<AddmmBackward0>) tensor(-0.6740)\n",
      "tensor([[-0.8143]], grad_fn=<AddmmBackward0>) tensor(-0.7260)\n",
      "tensor([[-0.2010]], grad_fn=<AddmmBackward0>) tensor(-0.0653)\n",
      "tensor([[0.1752]], grad_fn=<AddmmBackward0>) tensor(0.3910)\n",
      "tensor([[0.4881]], grad_fn=<AddmmBackward0>) tensor(0.5690)\n",
      "tensor([[0.4382]], grad_fn=<AddmmBackward0>) tensor(0.5157)\n",
      "tensor([[-0.6276]], grad_fn=<AddmmBackward0>) tensor(-0.7688)\n",
      "tensor([[-0.7791]], grad_fn=<AddmmBackward0>) tensor(-0.8282)\n",
      "tensor([[-0.5212]], grad_fn=<AddmmBackward0>) tensor(-0.5008)\n",
      "tensor([[-0.1182]], grad_fn=<AddmmBackward0>) tensor(0.4534)\n",
      "tensor([[-0.4963]], grad_fn=<AddmmBackward0>) tensor(-0.3512)\n",
      "tensor([[-0.4996]], grad_fn=<AddmmBackward0>) tensor(-0.2392)\n",
      "tensor([[-0.7963]], grad_fn=<AddmmBackward0>) tensor(-0.9582)\n",
      "tensor([[0.2401]], grad_fn=<AddmmBackward0>) tensor(0.7299)\n",
      "tensor([[-0.9445]], grad_fn=<AddmmBackward0>) tensor(-0.9103)\n",
      "tensor([[-0.8657]], grad_fn=<AddmmBackward0>) tensor(-0.8247)\n",
      "tensor([[0.7651]], grad_fn=<AddmmBackward0>) tensor(0.7258)\n",
      "tensor([[-1.2162]], grad_fn=<AddmmBackward0>) tensor(-0.9008)\n",
      "tensor([[-0.5825]], grad_fn=<AddmmBackward0>) tensor(-0.3954)\n",
      "tensor([[0.7338]], grad_fn=<AddmmBackward0>) tensor(0.6379)\n",
      "tensor([[-0.6710]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.4896]], grad_fn=<AddmmBackward0>) tensor(-0.8888)\n",
      "tensor([[-0.6075]], grad_fn=<AddmmBackward0>) tensor(-0.9716)\n",
      "tensor([[-0.2873]], grad_fn=<AddmmBackward0>) tensor(0.2911)\n",
      "tensor([[-0.4643]], grad_fn=<AddmmBackward0>) tensor(-0.5343)\n",
      "tensor([[-0.6739]], grad_fn=<AddmmBackward0>) tensor(-0.8773)\n",
      "tensor([[-1.0224]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.6084]], grad_fn=<AddmmBackward0>) tensor(-0.5742)\n",
      "tensor([[-0.7518]], grad_fn=<AddmmBackward0>) tensor(-0.7712)\n",
      "tensor([[-0.0753]], grad_fn=<AddmmBackward0>) tensor(0.4474)\n",
      "tensor([[-0.3933]], grad_fn=<AddmmBackward0>) tensor(0.0053)\n",
      "tensor([[-0.3842]], grad_fn=<AddmmBackward0>) tensor(-0.1113)\n",
      "tensor([[-0.3568]], grad_fn=<AddmmBackward0>) tensor(0.1668)\n",
      "tensor([[-0.7478]], grad_fn=<AddmmBackward0>) tensor(-0.7639)\n",
      "tensor([[0.8487]], grad_fn=<AddmmBackward0>) tensor(0.6803)\n",
      "tensor([[-0.0207]], grad_fn=<AddmmBackward0>) tensor(0.6394)\n",
      "tensor([[-0.1149]], grad_fn=<AddmmBackward0>) tensor(0.2664)\n",
      "tensor([[0.6436]], grad_fn=<AddmmBackward0>) tensor(0.7523)\n",
      "tensor([[-0.2754]], grad_fn=<AddmmBackward0>) tensor(-0.2160)\n",
      "tensor([[-0.8521]], grad_fn=<AddmmBackward0>) tensor(-0.7563)\n",
      "tensor([[0.5087]], grad_fn=<AddmmBackward0>) tensor(0.5776)\n",
      "tensor([[-0.7789]], grad_fn=<AddmmBackward0>) tensor(-0.9196)\n",
      "tensor([[0.3145]], grad_fn=<AddmmBackward0>) tensor(0.4807)\n",
      "tensor([[-0.5538]], grad_fn=<AddmmBackward0>) tensor(-0.7164)\n",
      "tensor([[0.5542]], grad_fn=<AddmmBackward0>) tensor(0.5537)\n",
      "tensor([[-0.4219]], grad_fn=<AddmmBackward0>) tensor(0.6279)\n",
      "tensor([[0.5027]], grad_fn=<AddmmBackward0>) tensor(0.3860)\n",
      "tensor([[0.1900]], grad_fn=<AddmmBackward0>) tensor(0.3870)\n",
      "tensor([[0.0135]], grad_fn=<AddmmBackward0>) tensor(0.2223)\n",
      "tensor([[-1.0212]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.1583]], grad_fn=<AddmmBackward0>) tensor(-0.4433)\n",
      "tensor([[0.7016]], grad_fn=<AddmmBackward0>) tensor(0.7552)\n",
      "tensor([[-0.9312]], grad_fn=<AddmmBackward0>) tensor(-0.6037)\n",
      "tensor([[-0.4911]], grad_fn=<AddmmBackward0>) tensor(-0.4885)\n",
      "tensor([[-0.0329]], grad_fn=<AddmmBackward0>) tensor(0.6202)\n",
      "tensor([[-0.0281]], grad_fn=<AddmmBackward0>) tensor(0.7660)\n",
      "tensor([[-0.3372]], grad_fn=<AddmmBackward0>) tensor(-0.4423)\n",
      "tensor([[-0.5775]], grad_fn=<AddmmBackward0>) tensor(0.5436)\n",
      "tensor([[-0.0820]], grad_fn=<AddmmBackward0>) tensor(0.0047)\n",
      "tensor([[0.0491]], grad_fn=<AddmmBackward0>) tensor(0.0945)\n",
      "tensor([[-0.0333]], grad_fn=<AddmmBackward0>) tensor(0.0169)\n",
      "tensor([[-1.1505]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.4303]], grad_fn=<AddmmBackward0>) tensor(0.0082)\n",
      "tensor([[-0.6234]], grad_fn=<AddmmBackward0>) tensor(-0.3673)\n",
      "tensor([[-0.5804]], grad_fn=<AddmmBackward0>) tensor(-0.5715)\n",
      "tensor([[-0.3114]], grad_fn=<AddmmBackward0>) tensor(-0.9472)\n",
      "tensor([[-0.3192]], grad_fn=<AddmmBackward0>) tensor(0.1477)\n",
      "tensor([[-0.4635]], grad_fn=<AddmmBackward0>) tensor(0.3418)\n",
      "tensor([[-0.0091]], grad_fn=<AddmmBackward0>) tensor(-0.0639)\n",
      "tensor([[-0.7672]], grad_fn=<AddmmBackward0>) tensor(-0.3216)\n",
      "tensor([[0.4862]], grad_fn=<AddmmBackward0>) tensor(0.8090)\n",
      "tensor([[-0.1130]], grad_fn=<AddmmBackward0>) tensor(-0.2578)\n",
      "tensor([[-0.4276]], grad_fn=<AddmmBackward0>) tensor(-0.2443)\n",
      "tensor([[0.6651]], grad_fn=<AddmmBackward0>) tensor(0.9494)\n",
      "tensor([[-0.0554]], grad_fn=<AddmmBackward0>) tensor(0.0466)\n",
      "tensor([[-0.6891]], grad_fn=<AddmmBackward0>) tensor(-0.6954)\n",
      "tensor([[0.2637]], grad_fn=<AddmmBackward0>) tensor(0.5296)\n",
      "tensor([[-0.1269]], grad_fn=<AddmmBackward0>) tensor(-0.1825)\n",
      "tensor([[-1.0465]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.4379]], grad_fn=<AddmmBackward0>) tensor(0.5124)\n",
      "tensor([[-0.6762]], grad_fn=<AddmmBackward0>) tensor(-0.4978)\n",
      "tensor([[-0.0626]], grad_fn=<AddmmBackward0>) tensor(-0.2028)\n",
      "tensor([[-0.6330]], grad_fn=<AddmmBackward0>) tensor(-0.9221)\n",
      "tensor([[0.0206]], grad_fn=<AddmmBackward0>) tensor(0.5174)\n",
      "tensor([[-0.2443]], grad_fn=<AddmmBackward0>) tensor(0.4013)\n",
      "tensor([[-0.6735]], grad_fn=<AddmmBackward0>) tensor(-0.8893)\n",
      "tensor([[-0.0172]], grad_fn=<AddmmBackward0>) tensor(-0.3275)\n",
      "tensor([[-0.6655]], grad_fn=<AddmmBackward0>) tensor(-0.5905)\n",
      "tensor([[-0.7334]], grad_fn=<AddmmBackward0>) tensor(-0.8372)\n",
      "tensor([[0.1090]], grad_fn=<AddmmBackward0>) tensor(0.6176)\n",
      "tensor([[-0.6748]], grad_fn=<AddmmBackward0>) tensor(-0.8599)\n",
      "tensor([[0.1931]], grad_fn=<AddmmBackward0>) tensor(0.5800)\n",
      "tensor([[-0.7673]], grad_fn=<AddmmBackward0>) tensor(-0.7756)\n",
      "tensor([[0.5982]], grad_fn=<AddmmBackward0>) tensor(0.7275)\n",
      "tensor([[0.4214]], grad_fn=<AddmmBackward0>) tensor(0.5299)\n",
      "tensor([[-0.6605]], grad_fn=<AddmmBackward0>) tensor(-0.5810)\n",
      "tensor([[-0.5920]], grad_fn=<AddmmBackward0>) tensor(-0.6820)\n",
      "tensor([[-0.5650]], grad_fn=<AddmmBackward0>) tensor(-0.4387)\n",
      "tensor([[-0.5130]], grad_fn=<AddmmBackward0>) tensor(-0.5084)\n",
      "tensor([[-0.2955]], grad_fn=<AddmmBackward0>) tensor(-0.1126)\n",
      "tensor([[0.0629]], grad_fn=<AddmmBackward0>) tensor(0.5495)\n",
      "tensor([[0.2910]], grad_fn=<AddmmBackward0>) tensor(-0.2610)\n",
      "tensor([[-0.3388]], grad_fn=<AddmmBackward0>) tensor(-0.4003)\n",
      "tensor([[-0.7640]], grad_fn=<AddmmBackward0>) tensor(-0.9199)\n",
      "tensor([[-0.6362]], grad_fn=<AddmmBackward0>) tensor(-0.4507)\n",
      "tensor([[-0.5469]], grad_fn=<AddmmBackward0>) tensor(-0.6450)\n",
      "tensor([[0.4719]], grad_fn=<AddmmBackward0>) tensor(0.7228)\n",
      "tensor([[-0.6428]], grad_fn=<AddmmBackward0>) tensor(-0.3797)\n",
      "tensor([[-0.9695]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.5944]], grad_fn=<AddmmBackward0>) tensor(-0.5686)\n",
      "tensor([[-1.1136]], grad_fn=<AddmmBackward0>) tensor(-0.6410)\n",
      "tensor([[-0.0075]], grad_fn=<AddmmBackward0>) tensor(0.3888)\n",
      "tensor([[-0.3237]], grad_fn=<AddmmBackward0>) tensor(-0.1399)\n",
      "tensor([[-0.5692]], grad_fn=<AddmmBackward0>) tensor(-0.6689)\n",
      "tensor([[-0.2349]], grad_fn=<AddmmBackward0>) tensor(-0.0409)\n",
      "tensor([[-0.5115]], grad_fn=<AddmmBackward0>) tensor(-0.7883)\n",
      "tensor([[0.4657]], grad_fn=<AddmmBackward0>) tensor(0.6962)\n",
      "tensor([[-0.6461]], grad_fn=<AddmmBackward0>) tensor(-0.6444)\n",
      "tensor([[-0.1933]], grad_fn=<AddmmBackward0>) tensor(-0.2415)\n",
      "tensor([[-0.5514]], grad_fn=<AddmmBackward0>) tensor(-0.8780)\n",
      "tensor([[-0.1320]], grad_fn=<AddmmBackward0>) tensor(-0.4570)\n",
      "tensor([[0.1691]], grad_fn=<AddmmBackward0>) tensor(0.2041)\n",
      "tensor([[-0.3388]], grad_fn=<AddmmBackward0>) tensor(-0.8725)\n",
      "tensor([[-0.5312]], grad_fn=<AddmmBackward0>) tensor(-0.1388)\n",
      "tensor([[0.3465]], grad_fn=<AddmmBackward0>) tensor(0.5857)\n",
      "tensor([[-0.3834]], grad_fn=<AddmmBackward0>) tensor(-0.3005)\n",
      "tensor([[-0.6553]], grad_fn=<AddmmBackward0>) tensor(-0.6269)\n",
      "tensor([[-0.4953]], grad_fn=<AddmmBackward0>) tensor(-0.6683)\n",
      "tensor([[-0.3171]], grad_fn=<AddmmBackward0>) tensor(-0.6981)\n",
      "tensor([[-0.5720]], grad_fn=<AddmmBackward0>) tensor(-0.7074)\n",
      "tensor([[-0.0420]], grad_fn=<AddmmBackward0>) tensor(0.0647)\n",
      "tensor([[-0.6006]], grad_fn=<AddmmBackward0>) tensor(-0.0773)\n",
      "tensor([[-0.4842]], grad_fn=<AddmmBackward0>) tensor(-0.4315)\n",
      "tensor([[0.7358]], grad_fn=<AddmmBackward0>) tensor(0.5755)\n",
      "tensor([[-0.6002]], grad_fn=<AddmmBackward0>) tensor(-0.4790)\n",
      "tensor([[0.0303]], grad_fn=<AddmmBackward0>) tensor(0.3843)\n",
      "tensor([[-0.6236]], grad_fn=<AddmmBackward0>) tensor(-0.7054)\n",
      "tensor([[-0.9280]], grad_fn=<AddmmBackward0>) tensor(-0.4488)\n",
      "tensor([[-0.6236]], grad_fn=<AddmmBackward0>) tensor(-0.6538)\n",
      "tensor([[-0.5951]], grad_fn=<AddmmBackward0>) tensor(-0.4456)\n",
      "tensor([[0.2739]], grad_fn=<AddmmBackward0>) tensor(0.5314)\n",
      "tensor([[-0.5214]], grad_fn=<AddmmBackward0>) tensor(0.6141)\n",
      "tensor([[-0.5278]], grad_fn=<AddmmBackward0>) tensor(-0.4259)\n",
      "tensor([[-0.6075]], grad_fn=<AddmmBackward0>) tensor(-0.7287)\n",
      "tensor([[-0.5383]], grad_fn=<AddmmBackward0>) tensor(-0.6418)\n",
      "tensor([[-0.8830]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.5742]], grad_fn=<AddmmBackward0>) tensor(-0.3955)\n",
      "tensor([[0.6127]], grad_fn=<AddmmBackward0>) tensor(0.5287)\n",
      "tensor([[-0.0433]], grad_fn=<AddmmBackward0>) tensor(-0.4165)\n",
      "tensor([[0.9837]], grad_fn=<AddmmBackward0>) tensor(0.9631)\n",
      "tensor([[-0.1544]], grad_fn=<AddmmBackward0>) tensor(-0.1571)\n",
      "tensor([[-0.7381]], grad_fn=<AddmmBackward0>) tensor(-0.8444)\n",
      "tensor([[-0.5039]], grad_fn=<AddmmBackward0>) tensor(-0.5439)\n",
      "tensor([[-1.0051]], grad_fn=<AddmmBackward0>) tensor(-0.6908)\n",
      "tensor([[-0.6916]], grad_fn=<AddmmBackward0>) tensor(-0.9151)\n",
      "tensor([[0.6140]], grad_fn=<AddmmBackward0>) tensor(0.5214)\n",
      "tensor([[0.0998]], grad_fn=<AddmmBackward0>) tensor(0.6130)\n",
      "tensor([[0.5759]], grad_fn=<AddmmBackward0>) tensor(0.5599)\n",
      "tensor([[-0.4370]], grad_fn=<AddmmBackward0>) tensor(-0.6790)\n",
      "tensor([[-0.4377]], grad_fn=<AddmmBackward0>) tensor(-0.2404)\n",
      "tensor([[-0.5737]], grad_fn=<AddmmBackward0>) tensor(-0.5286)\n",
      "tensor([[0.6360]], grad_fn=<AddmmBackward0>) tensor(0.7998)\n",
      "tensor([[-0.4621]], grad_fn=<AddmmBackward0>) tensor(-0.6860)\n",
      "tensor([[-0.1399]], grad_fn=<AddmmBackward0>) tensor(-0.2744)\n",
      "tensor([[-0.7320]], grad_fn=<AddmmBackward0>) tensor(-0.7035)\n",
      "tensor([[-0.8471]], grad_fn=<AddmmBackward0>) tensor(-0.6082)\n",
      "tensor([[-0.5409]], grad_fn=<AddmmBackward0>) tensor(-0.5337)\n",
      "tensor([[0.1119]], grad_fn=<AddmmBackward0>) tensor(0.3822)\n",
      "tensor([[-0.5988]], grad_fn=<AddmmBackward0>) tensor(-0.3662)\n",
      "tensor([[0.4908]], grad_fn=<AddmmBackward0>) tensor(0.5780)\n",
      "tensor([[-0.7685]], grad_fn=<AddmmBackward0>) tensor(-0.7742)\n",
      "tensor([[-0.9319]], grad_fn=<AddmmBackward0>) tensor(-0.4511)\n",
      "tensor([[0.3294]], grad_fn=<AddmmBackward0>) tensor(0.3707)\n",
      "tensor([[0.1800]], grad_fn=<AddmmBackward0>) tensor(0.5926)\n",
      "tensor([[-0.6217]], grad_fn=<AddmmBackward0>) tensor(-0.4283)\n",
      "tensor([[-0.8119]], grad_fn=<AddmmBackward0>) tensor(-0.8224)\n",
      "tensor([[0.7858]], grad_fn=<AddmmBackward0>) tensor(0.6647)\n",
      "tensor([[0.0024]], grad_fn=<AddmmBackward0>) tensor(0.6960)\n",
      "tensor([[0.4272]], grad_fn=<AddmmBackward0>) tensor(0.2369)\n",
      "tensor([[-0.7696]], grad_fn=<AddmmBackward0>) tensor(-0.7757)\n",
      "tensor([[0.3897]], grad_fn=<AddmmBackward0>) tensor(0.5941)\n",
      "tensor([[0.2474]], grad_fn=<AddmmBackward0>) tensor(0.3947)\n",
      "tensor([[-0.2098]], grad_fn=<AddmmBackward0>) tensor(-0.0223)\n",
      "tensor([[-1.0751]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.7665]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.5612]], grad_fn=<AddmmBackward0>) tensor(-0.6514)\n",
      "tensor([[-0.5480]], grad_fn=<AddmmBackward0>) tensor(-0.8998)\n",
      "tensor([[-0.8780]], grad_fn=<AddmmBackward0>) tensor(-0.8470)\n",
      "tensor([[-0.8360]], grad_fn=<AddmmBackward0>) tensor(-0.8488)\n",
      "tensor([[-0.9312]], grad_fn=<AddmmBackward0>) tensor(-0.8407)\n",
      "tensor([[-0.6815]], grad_fn=<AddmmBackward0>) tensor(-0.3250)\n",
      "tensor([[-0.0776]], grad_fn=<AddmmBackward0>) tensor(-0.8490)\n",
      "tensor([[-0.9009]], grad_fn=<AddmmBackward0>) tensor(-0.8471)\n",
      "tensor([[-0.6596]], grad_fn=<AddmmBackward0>) tensor(-0.6629)\n",
      "tensor([[-0.5375]], grad_fn=<AddmmBackward0>) tensor(-0.9708)\n",
      "tensor([[0.1913]], grad_fn=<AddmmBackward0>) tensor(0.3120)\n",
      "tensor([[-0.0579]], grad_fn=<AddmmBackward0>) tensor(0.1401)\n",
      "tensor([[0.1663]], grad_fn=<AddmmBackward0>) tensor(0.6182)\n",
      "tensor([[-0.8267]], grad_fn=<AddmmBackward0>) tensor(-0.8694)\n",
      "tensor([[-0.6850]], grad_fn=<AddmmBackward0>) tensor(-0.6445)\n",
      "tensor([[-0.4518]], grad_fn=<AddmmBackward0>) tensor(-0.4978)\n",
      "tensor([[-0.7908]], grad_fn=<AddmmBackward0>) tensor(-0.8698)\n",
      "tensor([[-0.2648]], grad_fn=<AddmmBackward0>) tensor(-0.2567)\n",
      "tensor([[-0.3782]], grad_fn=<AddmmBackward0>) tensor(-0.1106)\n",
      "tensor([[-0.6654]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.5580]], grad_fn=<AddmmBackward0>) tensor(-0.4114)\n",
      "tensor([[-0.4339]], grad_fn=<AddmmBackward0>) tensor(-0.5069)\n",
      "tensor([[-0.5340]], grad_fn=<AddmmBackward0>) tensor(-0.0780)\n",
      "tensor([[-0.4290]], grad_fn=<AddmmBackward0>) tensor(-0.3085)\n",
      "tensor([[-0.4318]], grad_fn=<AddmmBackward0>) tensor(-0.1495)\n",
      "tensor([[-0.4685]], grad_fn=<AddmmBackward0>) tensor(-0.7386)\n",
      "tensor([[-0.5296]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.1809]], grad_fn=<AddmmBackward0>) tensor(-0.2390)\n",
      "tensor([[-0.4524]], grad_fn=<AddmmBackward0>) tensor(-0.4293)\n",
      "tensor([[0.2763]], grad_fn=<AddmmBackward0>) tensor(0.6048)\n",
      "tensor([[0.5681]], grad_fn=<AddmmBackward0>) tensor(0.5847)\n",
      "tensor([[-0.8285]], grad_fn=<AddmmBackward0>) tensor(-0.8131)\n",
      "tensor([[-0.5007]], grad_fn=<AddmmBackward0>) tensor(-0.3134)\n",
      "tensor([[0.4858]], grad_fn=<AddmmBackward0>) tensor(0.5403)\n",
      "tensor([[0.6475]], grad_fn=<AddmmBackward0>) tensor(0.8436)\n",
      "tensor([[-0.2804]], grad_fn=<AddmmBackward0>) tensor(-0.0886)\n",
      "tensor([[-0.4492]], grad_fn=<AddmmBackward0>) tensor(-0.4069)\n",
      "tensor([[-0.2761]], grad_fn=<AddmmBackward0>) tensor(0.4969)\n",
      "tensor([[-0.7832]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.2235]], grad_fn=<AddmmBackward0>) tensor(0.0314)\n",
      "tensor([[-1.0947]], grad_fn=<AddmmBackward0>) tensor(-0.8934)\n",
      "tensor([[-0.5348]], grad_fn=<AddmmBackward0>) tensor(-0.4548)\n",
      "tensor([[0.4255]], grad_fn=<AddmmBackward0>) tensor(0.7503)\n",
      "tensor([[0.3058]], grad_fn=<AddmmBackward0>) tensor(0.5854)\n",
      "tensor([[-0.1277]], grad_fn=<AddmmBackward0>) tensor(-0.2673)\n",
      "tensor([[-0.1185]], grad_fn=<AddmmBackward0>) tensor(0.6892)\n",
      "tensor([[0.1047]], grad_fn=<AddmmBackward0>) tensor(-0.0003)\n",
      "tensor([[0.4744]], grad_fn=<AddmmBackward0>) tensor(0.5375)\n",
      "tensor([[0.8437]], grad_fn=<AddmmBackward0>) tensor(0.8044)\n",
      "tensor([[-1.4005]], grad_fn=<AddmmBackward0>) tensor(-0.9588)\n",
      "tensor([[0.5339]], grad_fn=<AddmmBackward0>) tensor(0.7394)\n",
      "tensor([[0.4162]], grad_fn=<AddmmBackward0>) tensor(0.7076)\n",
      "tensor([[-0.5464]], grad_fn=<AddmmBackward0>) tensor(-0.4337)\n",
      "tensor([[0.0729]], grad_fn=<AddmmBackward0>) tensor(0.5822)\n",
      "tensor([[-0.0366]], grad_fn=<AddmmBackward0>) tensor(-0.1566)\n",
      "tensor([[-0.8504]], grad_fn=<AddmmBackward0>) tensor(-0.4894)\n",
      "tensor([[-0.7745]], grad_fn=<AddmmBackward0>) tensor(-0.8165)\n",
      "tensor([[-0.0925]], grad_fn=<AddmmBackward0>) tensor(-0.2564)\n",
      "tensor([[-0.5449]], grad_fn=<AddmmBackward0>) tensor(-0.8440)\n",
      "tensor([[-0.7911]], grad_fn=<AddmmBackward0>) tensor(-0.9094)\n",
      "tensor([[-0.1393]], grad_fn=<AddmmBackward0>) tensor(-0.1261)\n",
      "tensor([[-0.8465]], grad_fn=<AddmmBackward0>) tensor(-0.9706)\n",
      "tensor([[-0.1742]], grad_fn=<AddmmBackward0>) tensor(-0.1667)\n",
      "tensor([[-0.3504]], grad_fn=<AddmmBackward0>) tensor(-0.9276)\n",
      "tensor([[-0.7819]], grad_fn=<AddmmBackward0>) tensor(-0.8750)\n",
      "tensor([[-0.6637]], grad_fn=<AddmmBackward0>) tensor(-0.7014)\n",
      "tensor([[-0.0251]], grad_fn=<AddmmBackward0>) tensor(-0.0593)\n",
      "tensor([[-0.4575]], grad_fn=<AddmmBackward0>) tensor(-0.1264)\n",
      "tensor([[-0.3685]], grad_fn=<AddmmBackward0>) tensor(-0.2652)\n",
      "tensor([[0.4886]], grad_fn=<AddmmBackward0>) tensor(0.6031)\n",
      "tensor([[0.1002]], grad_fn=<AddmmBackward0>) tensor(0.1940)\n",
      "tensor([[-1.1829]], grad_fn=<AddmmBackward0>) tensor(-0.9537)\n",
      "tensor([[-0.3342]], grad_fn=<AddmmBackward0>) tensor(-0.3216)\n",
      "tensor([[0.4963]], grad_fn=<AddmmBackward0>) tensor(0.7233)\n",
      "tensor([[-0.4101]], grad_fn=<AddmmBackward0>) tensor(-0.3807)\n",
      "tensor([[-0.6048]], grad_fn=<AddmmBackward0>) tensor(-0.9090)\n",
      "tensor([[-0.5084]], grad_fn=<AddmmBackward0>) tensor(-0.6459)\n",
      "tensor([[-0.7882]], grad_fn=<AddmmBackward0>) tensor(-0.6317)\n",
      "tensor([[0.2045]], grad_fn=<AddmmBackward0>) tensor(0.5317)\n",
      "tensor([[-0.9078]], grad_fn=<AddmmBackward0>) tensor(-0.9327)\n",
      "tensor([[-0.1408]], grad_fn=<AddmmBackward0>) tensor(-0.2378)\n",
      "tensor([[0.5901]], grad_fn=<AddmmBackward0>) tensor(0.5262)\n",
      "tensor([[-0.8331]], grad_fn=<AddmmBackward0>) tensor(-0.8914)\n",
      "tensor([[0.4774]], grad_fn=<AddmmBackward0>) tensor(0.7363)\n",
      "tensor([[-0.6296]], grad_fn=<AddmmBackward0>) tensor(-0.4551)\n",
      "tensor([[-0.8017]], grad_fn=<AddmmBackward0>) tensor(-0.8701)\n",
      "tensor([[0.1947]], grad_fn=<AddmmBackward0>) tensor(0.1327)\n",
      "tensor([[-0.8561]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.7712]], grad_fn=<AddmmBackward0>) tensor(-0.8636)\n",
      "tensor([[-0.4413]], grad_fn=<AddmmBackward0>) tensor(-0.5946)\n",
      "tensor([[-0.6407]], grad_fn=<AddmmBackward0>) tensor(-0.6194)\n",
      "tensor([[0.5382]], grad_fn=<AddmmBackward0>) tensor(0.7866)\n",
      "tensor([[-0.4323]], grad_fn=<AddmmBackward0>) tensor(-0.5972)\n",
      "tensor([[-0.7644]], grad_fn=<AddmmBackward0>) tensor(-0.7054)\n",
      "tensor([[-0.0163]], grad_fn=<AddmmBackward0>) tensor(-0.3090)\n",
      "tensor([[-0.3614]], grad_fn=<AddmmBackward0>) tensor(0.2330)\n",
      "tensor([[0.2410]], grad_fn=<AddmmBackward0>) tensor(-0.1732)\n",
      "tensor([[-0.7621]], grad_fn=<AddmmBackward0>) tensor(-0.7240)\n",
      "tensor([[0.0301]], grad_fn=<AddmmBackward0>) tensor(-0.1841)\n",
      "tensor([[-0.5811]], grad_fn=<AddmmBackward0>) tensor(-0.6817)\n",
      "tensor([[-0.5323]], grad_fn=<AddmmBackward0>) tensor(-0.5376)\n",
      "tensor([[0.0736]], grad_fn=<AddmmBackward0>) tensor(-0.1549)\n",
      "tensor([[0.1637]], grad_fn=<AddmmBackward0>) tensor(0.5073)\n",
      "tensor([[-0.0385]], grad_fn=<AddmmBackward0>) tensor(0.3586)\n",
      "tensor([[-0.5899]], grad_fn=<AddmmBackward0>) tensor(-0.5920)\n",
      "tensor([[-0.5894]], grad_fn=<AddmmBackward0>) tensor(-0.5950)\n",
      "tensor([[0.3773]], grad_fn=<AddmmBackward0>) tensor(0.5795)\n",
      "tensor([[-0.7342]], grad_fn=<AddmmBackward0>) tensor(-0.8698)\n",
      "tensor([[-0.5930]], grad_fn=<AddmmBackward0>) tensor(-0.8346)\n",
      "tensor([[-0.8854]], grad_fn=<AddmmBackward0>) tensor(-0.6839)\n",
      "tensor([[0.0218]], grad_fn=<AddmmBackward0>) tensor(-0.0142)\n",
      "tensor([[-0.6812]], grad_fn=<AddmmBackward0>) tensor(-0.4059)\n",
      "tensor([[0.0581]], grad_fn=<AddmmBackward0>) tensor(0.1050)\n",
      "tensor([[-0.3023]], grad_fn=<AddmmBackward0>) tensor(-0.5824)\n",
      "tensor([[-0.6568]], grad_fn=<AddmmBackward0>) tensor(-0.9470)\n",
      "tensor([[-0.2098]], grad_fn=<AddmmBackward0>) tensor(0.5838)\n",
      "tensor([[-0.4724]], grad_fn=<AddmmBackward0>) tensor(-0.7257)\n",
      "tensor([[-0.5715]], grad_fn=<AddmmBackward0>) tensor(-0.6664)\n",
      "tensor([[-0.5326]], grad_fn=<AddmmBackward0>) tensor(-0.2822)\n",
      "tensor([[-0.6622]], grad_fn=<AddmmBackward0>) tensor(-0.5679)\n",
      "tensor([[0.1408]], grad_fn=<AddmmBackward0>) tensor(0.4901)\n",
      "tensor([[-0.7585]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.8332]], grad_fn=<AddmmBackward0>) tensor(-0.5736)\n",
      "tensor([[-0.5303]], grad_fn=<AddmmBackward0>) tensor(-0.3314)\n",
      "tensor([[-0.9404]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.1831]], grad_fn=<AddmmBackward0>) tensor(-0.2713)\n",
      "tensor([[0.2073]], grad_fn=<AddmmBackward0>) tensor(0.5528)\n",
      "tensor([[-0.5117]], grad_fn=<AddmmBackward0>) tensor(0.5789)\n",
      "tensor([[0.2758]], grad_fn=<AddmmBackward0>) tensor(0.3227)\n",
      "tensor([[-0.7555]], grad_fn=<AddmmBackward0>) tensor(-0.6492)\n",
      "tensor([[0.0421]], grad_fn=<AddmmBackward0>) tensor(0.2887)\n",
      "tensor([[-0.4913]], grad_fn=<AddmmBackward0>) tensor(-0.1043)\n",
      "tensor([[-0.2593]], grad_fn=<AddmmBackward0>) tensor(0.0471)\n",
      "tensor([[-0.1578]], grad_fn=<AddmmBackward0>) tensor(-0.0451)\n",
      "tensor([[-0.7373]], grad_fn=<AddmmBackward0>) tensor(-0.8480)\n",
      "tensor([[0.2934]], grad_fn=<AddmmBackward0>) tensor(0.5538)\n",
      "tensor([[-0.5724]], grad_fn=<AddmmBackward0>) tensor(-0.6096)\n",
      "tensor([[-0.7542]], grad_fn=<AddmmBackward0>) tensor(-0.8479)\n",
      "tensor([[-0.0588]], grad_fn=<AddmmBackward0>) tensor(0.2059)\n",
      "tensor([[-0.6236]], grad_fn=<AddmmBackward0>) tensor(-0.4447)\n",
      "tensor([[0.7330]], grad_fn=<AddmmBackward0>) tensor(0.5501)\n",
      "tensor([[-0.9438]], grad_fn=<AddmmBackward0>) tensor(-0.4422)\n",
      "tensor([[0.6494]], grad_fn=<AddmmBackward0>) tensor(0.6963)\n",
      "tensor([[-0.4262]], grad_fn=<AddmmBackward0>) tensor(-0.5573)\n",
      "tensor([[-0.6682]], grad_fn=<AddmmBackward0>) tensor(-0.6754)\n",
      "tensor([[-0.0436]], grad_fn=<AddmmBackward0>) tensor(0.6339)\n",
      "tensor([[0.2169]], grad_fn=<AddmmBackward0>) tensor(0.8030)\n",
      "tensor([[-0.6630]], grad_fn=<AddmmBackward0>) tensor(-0.7257)\n",
      "tensor([[-0.4854]], grad_fn=<AddmmBackward0>) tensor(-0.6386)\n",
      "tensor([[-0.2499]], grad_fn=<AddmmBackward0>) tensor(0.5842)\n",
      "tensor([[0.4387]], grad_fn=<AddmmBackward0>) tensor(0.5774)\n",
      "tensor([[0.4913]], grad_fn=<AddmmBackward0>) tensor(0.7709)\n",
      "tensor([[-0.6390]], grad_fn=<AddmmBackward0>) tensor(-0.3672)\n",
      "tensor([[-0.2685]], grad_fn=<AddmmBackward0>) tensor(-0.1458)\n",
      "tensor([[0.4223]], grad_fn=<AddmmBackward0>) tensor(0.5773)\n",
      "tensor([[-0.1460]], grad_fn=<AddmmBackward0>) tensor(0.1333)\n",
      "tensor([[-0.4109]], grad_fn=<AddmmBackward0>) tensor(-0.3950)\n",
      "tensor([[0.2346]], grad_fn=<AddmmBackward0>) tensor(0.5647)\n",
      "tensor([[-0.3500]], grad_fn=<AddmmBackward0>) tensor(-0.2524)\n",
      "tensor([[-0.0591]], grad_fn=<AddmmBackward0>) tensor(0.0147)\n",
      "tensor([[-0.7142]], grad_fn=<AddmmBackward0>) tensor(-0.4295)\n",
      "tensor([[-0.9152]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.6700]], grad_fn=<AddmmBackward0>) tensor(-0.8700)\n",
      "tensor([[0.1325]], grad_fn=<AddmmBackward0>) tensor(0.5621)\n",
      "tensor([[-0.1918]], grad_fn=<AddmmBackward0>) tensor(-0.1081)\n",
      "tensor([[-0.5361]], grad_fn=<AddmmBackward0>) tensor(-0.9025)\n",
      "tensor([[-0.5463]], grad_fn=<AddmmBackward0>) tensor(-0.9596)\n",
      "tensor([[-1.0195]], grad_fn=<AddmmBackward0>) tensor(-0.7685)\n",
      "tensor([[-0.6328]], grad_fn=<AddmmBackward0>) tensor(-0.6340)\n",
      "tensor([[-0.4864]], grad_fn=<AddmmBackward0>) tensor(-0.3948)\n",
      "tensor([[-0.3089]], grad_fn=<AddmmBackward0>) tensor(-0.2457)\n",
      "tensor([[0.0268]], grad_fn=<AddmmBackward0>) tensor(0.1062)\n",
      "tensor([[-0.0148]], grad_fn=<AddmmBackward0>) tensor(0.3686)\n",
      "tensor([[-0.1955]], grad_fn=<AddmmBackward0>) tensor(0.2845)\n",
      "tensor([[-0.4789]], grad_fn=<AddmmBackward0>) tensor(-0.8756)\n",
      "tensor([[-0.7143]], grad_fn=<AddmmBackward0>) tensor(-0.5236)\n",
      "tensor([[-0.3177]], grad_fn=<AddmmBackward0>) tensor(-0.2485)\n",
      "tensor([[-0.5155]], grad_fn=<AddmmBackward0>) tensor(-0.3267)\n",
      "tensor([[-0.8199]], grad_fn=<AddmmBackward0>) tensor(-0.6614)\n",
      "tensor([[0.5852]], grad_fn=<AddmmBackward0>) tensor(0.5802)\n",
      "tensor([[-0.6671]], grad_fn=<AddmmBackward0>) tensor(-0.2700)\n",
      "tensor([[-0.5069]], grad_fn=<AddmmBackward0>) tensor(-0.4232)\n",
      "tensor([[-0.2760]], grad_fn=<AddmmBackward0>) tensor(-0.7987)\n",
      "tensor([[-0.0281]], grad_fn=<AddmmBackward0>) tensor(0.5476)\n",
      "tensor([[-0.8161]], grad_fn=<AddmmBackward0>) tensor(-0.9407)\n",
      "tensor([[0.2705]], grad_fn=<AddmmBackward0>) tensor(0.2210)\n",
      "tensor([[-0.7556]], grad_fn=<AddmmBackward0>) tensor(-0.7118)\n",
      "tensor([[-0.3674]], grad_fn=<AddmmBackward0>) tensor(-0.5283)\n",
      "tensor([[-0.1792]], grad_fn=<AddmmBackward0>) tensor(0.1530)\n",
      "tensor([[-0.4872]], grad_fn=<AddmmBackward0>) tensor(-0.5247)\n",
      "tensor([[-0.4038]], grad_fn=<AddmmBackward0>) tensor(-0.8468)\n",
      "tensor([[-0.1846]], grad_fn=<AddmmBackward0>) tensor(-0.7288)\n",
      "tensor([[0.8434]], grad_fn=<AddmmBackward0>) tensor(0.8426)\n",
      "tensor([[-0.2729]], grad_fn=<AddmmBackward0>) tensor(0.1762)\n",
      "tensor([[0.4172]], grad_fn=<AddmmBackward0>) tensor(0.5346)\n",
      "tensor([[0.4076]], grad_fn=<AddmmBackward0>) tensor(0.5523)\n",
      "tensor([[0.7497]], grad_fn=<AddmmBackward0>) tensor(0.6322)\n",
      "tensor([[0.7665]], grad_fn=<AddmmBackward0>) tensor(0.6008)\n",
      "tensor([[-0.5341]], grad_fn=<AddmmBackward0>) tensor(-0.3902)\n",
      "tensor([[0.4315]], grad_fn=<AddmmBackward0>) tensor(0.8066)\n",
      "tensor([[-0.2495]], grad_fn=<AddmmBackward0>) tensor(-0.2652)\n",
      "tensor([[-0.5456]], grad_fn=<AddmmBackward0>) tensor(-0.9662)\n",
      "tensor([[-0.7857]], grad_fn=<AddmmBackward0>) tensor(-0.6388)\n",
      "tensor([[-0.3836]], grad_fn=<AddmmBackward0>) tensor(-0.2503)\n",
      "tensor([[-0.1057]], grad_fn=<AddmmBackward0>) tensor(-0.2535)\n",
      "tensor([[-0.2708]], grad_fn=<AddmmBackward0>) tensor(0.2992)\n",
      "tensor([[-0.6362]], grad_fn=<AddmmBackward0>) tensor(-0.8574)\n",
      "tensor([[-0.1332]], grad_fn=<AddmmBackward0>) tensor(-0.1724)\n",
      "tensor([[0.0661]], grad_fn=<AddmmBackward0>) tensor(0.5887)\n",
      "tensor([[-0.7505]], grad_fn=<AddmmBackward0>) tensor(-0.8911)\n",
      "tensor([[-1.0228]], grad_fn=<AddmmBackward0>) tensor(-0.3921)\n",
      "tensor([[-0.2415]], grad_fn=<AddmmBackward0>) tensor(0.0006)\n",
      "tensor([[-0.4166]], grad_fn=<AddmmBackward0>) tensor(-0.1542)\n",
      "tensor([[0.1396]], grad_fn=<AddmmBackward0>) tensor(0.2366)\n",
      "tensor([[-0.0595]], grad_fn=<AddmmBackward0>) tensor(0.2572)\n",
      "tensor([[0.5385]], grad_fn=<AddmmBackward0>) tensor(0.5579)\n",
      "tensor([[0.3769]], grad_fn=<AddmmBackward0>) tensor(0.6844)\n",
      "tensor([[-0.7798]], grad_fn=<AddmmBackward0>) tensor(-0.6980)\n",
      "tensor([[-0.4578]], grad_fn=<AddmmBackward0>) tensor(-0.4746)\n",
      "tensor([[0.4025]], grad_fn=<AddmmBackward0>) tensor(0.2366)\n",
      "tensor([[0.7680]], grad_fn=<AddmmBackward0>) tensor(0.5288)\n",
      "tensor([[0.0238]], grad_fn=<AddmmBackward0>) tensor(-0.0209)\n",
      "tensor([[-0.3810]], grad_fn=<AddmmBackward0>) tensor(-0.4345)\n",
      "tensor([[0.2722]], grad_fn=<AddmmBackward0>) tensor(0.7253)\n",
      "tensor([[-0.9326]], grad_fn=<AddmmBackward0>) tensor(-0.8531)\n",
      "tensor([[-0.4138]], grad_fn=<AddmmBackward0>) tensor(-0.7566)\n",
      "tensor([[-0.4688]], grad_fn=<AddmmBackward0>) tensor(-0.4973)\n",
      "tensor([[-0.9814]], grad_fn=<AddmmBackward0>) tensor(-0.8964)\n",
      "tensor([[-0.7382]], grad_fn=<AddmmBackward0>) tensor(-0.8627)\n",
      "tensor([[-0.1854]], grad_fn=<AddmmBackward0>) tensor(-0.1350)\n",
      "tensor([[-0.7535]], grad_fn=<AddmmBackward0>) tensor(-0.4877)\n",
      "tensor([[-0.4582]], grad_fn=<AddmmBackward0>) tensor(-0.8808)\n",
      "tensor([[-0.1705]], grad_fn=<AddmmBackward0>) tensor(0.4880)\n",
      "tensor([[0.3276]], grad_fn=<AddmmBackward0>) tensor(0.3410)\n",
      "tensor([[-0.5823]], grad_fn=<AddmmBackward0>) tensor(-0.2565)\n",
      "tensor([[0.3306]], grad_fn=<AddmmBackward0>) tensor(0.7985)\n",
      "tensor([[0.1511]], grad_fn=<AddmmBackward0>) tensor(0.8336)\n",
      "tensor([[0.8306]], grad_fn=<AddmmBackward0>) tensor(0.7478)\n",
      "tensor([[-0.7178]], grad_fn=<AddmmBackward0>) tensor(-0.4060)\n",
      "tensor([[0.5985]], grad_fn=<AddmmBackward0>) tensor(0.6023)\n",
      "tensor([[0.4137]], grad_fn=<AddmmBackward0>) tensor(0.6438)\n",
      "tensor([[0.5038]], grad_fn=<AddmmBackward0>) tensor(0.5371)\n",
      "tensor([[-0.4263]], grad_fn=<AddmmBackward0>) tensor(0.6001)\n",
      "tensor([[0.3965]], grad_fn=<AddmmBackward0>) tensor(0.5766)\n",
      "tensor([[-0.5082]], grad_fn=<AddmmBackward0>) tensor(-0.4101)\n",
      "tensor([[0.7283]], grad_fn=<AddmmBackward0>) tensor(0.5953)\n",
      "tensor([[-0.3578]], grad_fn=<AddmmBackward0>) tensor(0.0347)\n",
      "tensor([[-0.6543]], grad_fn=<AddmmBackward0>) tensor(-0.4996)\n",
      "tensor([[-0.3057]], grad_fn=<AddmmBackward0>) tensor(-0.3831)\n",
      "tensor([[0.0472]], grad_fn=<AddmmBackward0>) tensor(0.1111)\n",
      "tensor([[-1.0613]], grad_fn=<AddmmBackward0>) tensor(-0.7416)\n",
      "tensor([[0.1400]], grad_fn=<AddmmBackward0>) tensor(0.5646)\n",
      "tensor([[-0.7310]], grad_fn=<AddmmBackward0>) tensor(-0.5899)\n",
      "tensor([[0.4426]], grad_fn=<AddmmBackward0>) tensor(0.1023)\n",
      "tensor([[-0.5635]], grad_fn=<AddmmBackward0>) tensor(-0.4922)\n",
      "tensor([[-0.9227]], grad_fn=<AddmmBackward0>) tensor(-0.3556)\n",
      "tensor([[0.2325]], grad_fn=<AddmmBackward0>) tensor(0.5755)\n",
      "tensor([[-0.2870]], grad_fn=<AddmmBackward0>) tensor(-0.1210)\n",
      "tensor([[-0.6588]], grad_fn=<AddmmBackward0>) tensor(-0.3835)\n",
      "tensor([[-0.0715]], grad_fn=<AddmmBackward0>) tensor(0.1142)\n",
      "tensor([[-0.7168]], grad_fn=<AddmmBackward0>) tensor(-0.7788)\n",
      "tensor([[-0.5595]], grad_fn=<AddmmBackward0>) tensor(-0.5913)\n",
      "tensor([[-0.0803]], grad_fn=<AddmmBackward0>) tensor(-0.8658)\n",
      "tensor([[-0.0446]], grad_fn=<AddmmBackward0>) tensor(-0.3247)\n",
      "tensor([[-0.6568]], grad_fn=<AddmmBackward0>) tensor(-0.6012)\n",
      "tensor([[-0.0137]], grad_fn=<AddmmBackward0>) tensor(0.0354)\n",
      "tensor([[-0.6594]], grad_fn=<AddmmBackward0>) tensor(-0.9024)\n",
      "tensor([[-0.3551]], grad_fn=<AddmmBackward0>) tensor(-0.2535)\n",
      "tensor([[-0.3518]], grad_fn=<AddmmBackward0>) tensor(-0.2743)\n",
      "tensor([[-0.1553]], grad_fn=<AddmmBackward0>) tensor(-0.1962)\n",
      "tensor([[-0.1831]], grad_fn=<AddmmBackward0>) tensor(-0.5026)\n",
      "tensor([[-0.3546]], grad_fn=<AddmmBackward0>) tensor(0.1380)\n",
      "tensor([[-0.8290]], grad_fn=<AddmmBackward0>) tensor(-0.8362)\n",
      "tensor([[0.5494]], grad_fn=<AddmmBackward0>) tensor(0.3698)\n",
      "tensor([[-0.4973]], grad_fn=<AddmmBackward0>) tensor(-0.4103)\n",
      "tensor([[-0.5232]], grad_fn=<AddmmBackward0>) tensor(-0.1798)\n",
      "tensor([[0.2807]], grad_fn=<AddmmBackward0>) tensor(0.3687)\n",
      "tensor([[0.4391]], grad_fn=<AddmmBackward0>) tensor(0.7914)\n",
      "tensor([[-0.7690]], grad_fn=<AddmmBackward0>) tensor(-0.9503)\n",
      "tensor([[-0.5430]], grad_fn=<AddmmBackward0>) tensor(-0.5733)\n",
      "tensor([[-0.6182]], grad_fn=<AddmmBackward0>) tensor(-0.6505)\n",
      "tensor([[0.4568]], grad_fn=<AddmmBackward0>) tensor(0.6662)\n",
      "tensor([[0.4257]], grad_fn=<AddmmBackward0>) tensor(-0.0980)\n",
      "tensor([[-0.7236]], grad_fn=<AddmmBackward0>) tensor(-0.7677)\n",
      "tensor([[-1.4080]], grad_fn=<AddmmBackward0>) tensor(-0.9414)\n",
      "tensor([[-0.2197]], grad_fn=<AddmmBackward0>) tensor(-0.4574)\n",
      "tensor([[-0.7170]], grad_fn=<AddmmBackward0>) tensor(-0.6521)\n",
      "tensor([[0.6641]], grad_fn=<AddmmBackward0>) tensor(0.5743)\n",
      "tensor([[-1.1545]], grad_fn=<AddmmBackward0>) tensor(-0.8787)\n",
      "tensor([[-0.7638]], grad_fn=<AddmmBackward0>) tensor(-0.4344)\n",
      "tensor([[-0.4302]], grad_fn=<AddmmBackward0>) tensor(-0.7609)\n",
      "tensor([[-0.5002]], grad_fn=<AddmmBackward0>) tensor(-0.5445)\n",
      "tensor([[0.4408]], grad_fn=<AddmmBackward0>) tensor(0.8471)\n",
      "tensor([[-0.9897]], grad_fn=<AddmmBackward0>) tensor(-0.8795)\n",
      "tensor([[-0.4391]], grad_fn=<AddmmBackward0>) tensor(-0.9739)\n",
      "tensor([[0.0309]], grad_fn=<AddmmBackward0>) tensor(0.2288)\n",
      "tensor([[0.4064]], grad_fn=<AddmmBackward0>) tensor(0.7727)\n",
      "tensor([[-1.1248]], grad_fn=<AddmmBackward0>) tensor(-0.8874)\n",
      "tensor([[-0.4250]], grad_fn=<AddmmBackward0>) tensor(-0.8970)\n",
      "tensor([[-0.5600]], grad_fn=<AddmmBackward0>) tensor(0.9768)\n",
      "tensor([[0.3910]], grad_fn=<AddmmBackward0>) tensor(0.5565)\n",
      "tensor([[-0.4499]], grad_fn=<AddmmBackward0>) tensor(-0.0415)\n",
      "tensor([[-0.1298]], grad_fn=<AddmmBackward0>) tensor(-0.0965)\n",
      "tensor([[-0.1714]], grad_fn=<AddmmBackward0>) tensor(-0.1478)\n",
      "tensor([[-0.0033]], grad_fn=<AddmmBackward0>) tensor(0.6779)\n",
      "tensor([[-0.3310]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.5507]], grad_fn=<AddmmBackward0>) tensor(0.5135)\n",
      "tensor([[-0.2077]], grad_fn=<AddmmBackward0>) tensor(-0.0147)\n",
      "tensor([[-0.6541]], grad_fn=<AddmmBackward0>) tensor(-0.6332)\n",
      "tensor([[0.4835]], grad_fn=<AddmmBackward0>) tensor(0.5434)\n",
      "tensor([[1.1317]], grad_fn=<AddmmBackward0>) tensor(0.6911)\n",
      "tensor([[-0.2551]], grad_fn=<AddmmBackward0>) tensor(-0.3861)\n",
      "tensor([[-0.1614]], grad_fn=<AddmmBackward0>) tensor(0.2882)\n",
      "tensor([[-0.2824]], grad_fn=<AddmmBackward0>) tensor(-0.1552)\n",
      "tensor([[0.3047]], grad_fn=<AddmmBackward0>) tensor(0.5952)\n",
      "tensor([[-0.6977]], grad_fn=<AddmmBackward0>) tensor(-0.8895)\n",
      "tensor([[-0.5873]], grad_fn=<AddmmBackward0>) tensor(-0.7977)\n",
      "tensor([[-0.1979]], grad_fn=<AddmmBackward0>) tensor(0.3250)\n",
      "tensor([[0.0471]], grad_fn=<AddmmBackward0>) tensor(0.8275)\n",
      "tensor([[-0.6169]], grad_fn=<AddmmBackward0>) tensor(-0.6144)\n",
      "tensor([[0.1824]], grad_fn=<AddmmBackward0>) tensor(0.2458)\n",
      "tensor([[-0.5839]], grad_fn=<AddmmBackward0>) tensor(-0.5079)\n",
      "tensor([[0.3977]], grad_fn=<AddmmBackward0>) tensor(0.6698)\n",
      "tensor([[-0.0158]], grad_fn=<AddmmBackward0>) tensor(0.5139)\n",
      "tensor([[-0.1526]], grad_fn=<AddmmBackward0>) tensor(-0.1284)\n",
      "tensor([[-0.0802]], grad_fn=<AddmmBackward0>) tensor(0.0096)\n",
      "tensor([[-0.3936]], grad_fn=<AddmmBackward0>) tensor(-0.3337)\n",
      "tensor([[0.0100]], grad_fn=<AddmmBackward0>) tensor(0.0087)\n",
      "tensor([[0.2413]], grad_fn=<AddmmBackward0>) tensor(0.6633)\n",
      "tensor([[-0.2010]], grad_fn=<AddmmBackward0>) tensor(0.0334)\n",
      "tensor([[0.8180]], grad_fn=<AddmmBackward0>) tensor(0.6560)\n",
      "tensor([[-0.8466]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.0580]], grad_fn=<AddmmBackward0>) tensor(-0.0021)\n",
      "tensor([[0.1553]], grad_fn=<AddmmBackward0>) tensor(0.8291)\n",
      "tensor([[0.4991]], grad_fn=<AddmmBackward0>) tensor(0.5511)\n",
      "tensor([[-0.3801]], grad_fn=<AddmmBackward0>) tensor(-0.4436)\n",
      "tensor([[-0.6929]], grad_fn=<AddmmBackward0>) tensor(-0.8231)\n",
      "tensor([[0.2535]], grad_fn=<AddmmBackward0>) tensor(-0.1011)\n",
      "tensor([[0.3755]], grad_fn=<AddmmBackward0>) tensor(0.6973)\n",
      "tensor([[0.3718]], grad_fn=<AddmmBackward0>) tensor(0.8261)\n",
      "tensor([[-0.7069]], grad_fn=<AddmmBackward0>) tensor(-0.8820)\n",
      "tensor([[-0.1031]], grad_fn=<AddmmBackward0>) tensor(-0.0581)\n",
      "tensor([[-0.7410]], grad_fn=<AddmmBackward0>) tensor(-0.4604)\n",
      "tensor([[-0.0553]], grad_fn=<AddmmBackward0>) tensor(-0.0017)\n",
      "tensor([[0.2784]], grad_fn=<AddmmBackward0>) tensor(0.3641)\n",
      "tensor([[-0.2419]], grad_fn=<AddmmBackward0>) tensor(-0.1225)\n",
      "tensor([[-0.3485]], grad_fn=<AddmmBackward0>) tensor(0.0196)\n",
      "tensor([[0.7420]], grad_fn=<AddmmBackward0>) tensor(0.6024)\n",
      "tensor([[-0.4131]], grad_fn=<AddmmBackward0>) tensor(-0.9373)\n",
      "tensor([[0.3676]], grad_fn=<AddmmBackward0>) tensor(0.5760)\n",
      "tensor([[0.0409]], grad_fn=<AddmmBackward0>) tensor(0.2487)\n",
      "tensor([[0.4323]], grad_fn=<AddmmBackward0>) tensor(0.5729)\n",
      "tensor([[-0.4887]], grad_fn=<AddmmBackward0>) tensor(-0.3941)\n",
      "tensor([[0.4746]], grad_fn=<AddmmBackward0>) tensor(0.5850)\n",
      "tensor([[-0.4323]], grad_fn=<AddmmBackward0>) tensor(-0.7035)\n",
      "tensor([[-0.2238]], grad_fn=<AddmmBackward0>) tensor(0.0143)\n",
      "tensor([[-0.6340]], grad_fn=<AddmmBackward0>) tensor(-0.7482)\n",
      "tensor([[0.3169]], grad_fn=<AddmmBackward0>) tensor(0.5763)\n",
      "tensor([[-0.1082]], grad_fn=<AddmmBackward0>) tensor(0.0131)\n",
      "tensor([[0.4030]], grad_fn=<AddmmBackward0>) tensor(0.7398)\n",
      "tensor([[-0.5512]], grad_fn=<AddmmBackward0>) tensor(-0.5348)\n",
      "tensor([[0.4852]], grad_fn=<AddmmBackward0>) tensor(0.5453)\n",
      "tensor([[-0.4967]], grad_fn=<AddmmBackward0>) tensor(-0.6025)\n",
      "tensor([[0.4604]], grad_fn=<AddmmBackward0>) tensor(0.5703)\n",
      "tensor([[0.4034]], grad_fn=<AddmmBackward0>) tensor(0.4931)\n",
      "tensor([[-0.6955]], grad_fn=<AddmmBackward0>) tensor(-0.9311)\n",
      "tensor([[0.3410]], grad_fn=<AddmmBackward0>) tensor(0.0740)\n",
      "tensor([[-0.3855]], grad_fn=<AddmmBackward0>) tensor(-0.8341)\n",
      "tensor([[-0.5740]], grad_fn=<AddmmBackward0>) tensor(-0.4710)\n",
      "tensor([[0.0609]], grad_fn=<AddmmBackward0>) tensor(-0.0162)\n",
      "tensor([[-0.4968]], grad_fn=<AddmmBackward0>) tensor(0.0555)\n",
      "tensor([[-0.4903]], grad_fn=<AddmmBackward0>) tensor(-0.5155)\n",
      "tensor([[0.0561]], grad_fn=<AddmmBackward0>) tensor(0.1604)\n",
      "tensor([[-0.9625]], grad_fn=<AddmmBackward0>) tensor(-0.3774)\n",
      "tensor([[-0.2850]], grad_fn=<AddmmBackward0>) tensor(-0.1749)\n",
      "tensor([[-0.0259]], grad_fn=<AddmmBackward0>) tensor(-0.1978)\n",
      "tensor([[-0.7016]], grad_fn=<AddmmBackward0>) tensor(-0.6576)\n",
      "tensor([[-0.6647]], grad_fn=<AddmmBackward0>) tensor(-0.3087)\n",
      "tensor([[-0.3953]], grad_fn=<AddmmBackward0>) tensor(-0.6488)\n",
      "tensor([[-0.1126]], grad_fn=<AddmmBackward0>) tensor(-0.1202)\n",
      "tensor([[0.6982]], grad_fn=<AddmmBackward0>) tensor(0.7472)\n",
      "tensor([[-0.4143]], grad_fn=<AddmmBackward0>) tensor(-0.6792)\n",
      "tensor([[-0.5757]], grad_fn=<AddmmBackward0>) tensor(-0.6116)\n",
      "tensor([[-0.6762]], grad_fn=<AddmmBackward0>) tensor(-0.3073)\n",
      "tensor([[0.4798]], grad_fn=<AddmmBackward0>) tensor(0.7490)\n",
      "tensor([[0.1399]], grad_fn=<AddmmBackward0>) tensor(0.0043)\n",
      "tensor([[-0.6653]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.3412]], grad_fn=<AddmmBackward0>) tensor(-0.7719)\n",
      "tensor([[0.2129]], grad_fn=<AddmmBackward0>) tensor(0.5553)\n",
      "tensor([[-0.8624]], grad_fn=<AddmmBackward0>) tensor(-0.9683)\n",
      "tensor([[-0.4197]], grad_fn=<AddmmBackward0>) tensor(-0.1098)\n",
      "tensor([[-0.2578]], grad_fn=<AddmmBackward0>) tensor(-0.9265)\n",
      "tensor([[-0.0516]], grad_fn=<AddmmBackward0>) tensor(0.1068)\n",
      "tensor([[0.5110]], grad_fn=<AddmmBackward0>) tensor(0.6503)\n",
      "tensor([[-0.6066]], grad_fn=<AddmmBackward0>) tensor(-0.3621)\n",
      "tensor([[-0.4508]], grad_fn=<AddmmBackward0>) tensor(-0.6003)\n",
      "tensor([[0.0427]], grad_fn=<AddmmBackward0>) tensor(0.6184)\n",
      "tensor([[-0.7819]], grad_fn=<AddmmBackward0>) tensor(-0.7970)\n",
      "tensor([[-0.0339]], grad_fn=<AddmmBackward0>) tensor(0.5742)\n",
      "tensor([[-0.4481]], grad_fn=<AddmmBackward0>) tensor(-0.0993)\n",
      "tensor([[-0.3062]], grad_fn=<AddmmBackward0>) tensor(-0.1953)\n",
      "tensor([[0.3663]], grad_fn=<AddmmBackward0>) tensor(0.6706)\n",
      "tensor([[0.3460]], grad_fn=<AddmmBackward0>) tensor(0.7358)\n",
      "tensor([[-0.0803]], grad_fn=<AddmmBackward0>) tensor(-0.0419)\n",
      "tensor([[-0.2818]], grad_fn=<AddmmBackward0>) tensor(0.2183)\n",
      "tensor([[-0.7164]], grad_fn=<AddmmBackward0>) tensor(-0.3298)\n",
      "tensor([[0.0309]], grad_fn=<AddmmBackward0>) tensor(0.5947)\n",
      "tensor([[-0.2863]], grad_fn=<AddmmBackward0>) tensor(-0.2973)\n",
      "tensor([[-0.8258]], grad_fn=<AddmmBackward0>) tensor(-0.8620)\n",
      "tensor([[-0.4907]], grad_fn=<AddmmBackward0>) tensor(-0.4099)\n",
      "tensor([[0.5986]], grad_fn=<AddmmBackward0>) tensor(0.5849)\n",
      "tensor([[-0.6588]], grad_fn=<AddmmBackward0>) tensor(-0.6030)\n",
      "tensor([[0.0277]], grad_fn=<AddmmBackward0>) tensor(0.3318)\n",
      "tensor([[0.0134]], grad_fn=<AddmmBackward0>) tensor(0.0082)\n",
      "tensor([[-0.5561]], grad_fn=<AddmmBackward0>) tensor(-0.7218)\n",
      "tensor([[-0.2184]], grad_fn=<AddmmBackward0>) tensor(-0.0825)\n",
      "tensor([[-0.7349]], grad_fn=<AddmmBackward0>) tensor(-0.8909)\n",
      "tensor([[0.4422]], grad_fn=<AddmmBackward0>) tensor(0.5163)\n",
      "tensor([[-0.7542]], grad_fn=<AddmmBackward0>) tensor(-0.9142)\n",
      "tensor([[-0.7994]], grad_fn=<AddmmBackward0>) tensor(-0.4209)\n",
      "tensor([[0.3849]], grad_fn=<AddmmBackward0>) tensor(0.5203)\n",
      "tensor([[-0.3014]], grad_fn=<AddmmBackward0>) tensor(-0.4518)\n",
      "tensor([[-0.6822]], grad_fn=<AddmmBackward0>) tensor(-0.5046)\n",
      "tensor([[-0.7969]], grad_fn=<AddmmBackward0>) tensor(-0.6952)\n",
      "tensor([[-1.0053]], grad_fn=<AddmmBackward0>) tensor(-0.7077)\n",
      "tensor([[0.0681]], grad_fn=<AddmmBackward0>) tensor(0.0333)\n",
      "tensor([[-0.3868]], grad_fn=<AddmmBackward0>) tensor(-0.4995)\n",
      "tensor([[0.9228]], grad_fn=<AddmmBackward0>) tensor(0.7487)\n",
      "tensor([[-0.8891]], grad_fn=<AddmmBackward0>) tensor(-0.4409)\n",
      "tensor([[-0.6212]], grad_fn=<AddmmBackward0>) tensor(-0.7199)\n",
      "tensor([[-0.3263]], grad_fn=<AddmmBackward0>) tensor(0.1376)\n",
      "tensor([[-0.6278]], grad_fn=<AddmmBackward0>) tensor(-0.6594)\n",
      "tensor([[0.2108]], grad_fn=<AddmmBackward0>) tensor(0.6456)\n",
      "tensor([[0.6275]], grad_fn=<AddmmBackward0>) tensor(0.5529)\n",
      "tensor([[0.2393]], grad_fn=<AddmmBackward0>) tensor(0.5882)\n",
      "tensor([[0.3081]], grad_fn=<AddmmBackward0>) tensor(0.1323)\n",
      "tensor([[0.1045]], grad_fn=<AddmmBackward0>) tensor(0.5079)\n",
      "tensor([[-0.2626]], grad_fn=<AddmmBackward0>) tensor(0.5751)\n",
      "tensor([[-0.6842]], grad_fn=<AddmmBackward0>) tensor(-0.6687)\n",
      "tensor([[0.4119]], grad_fn=<AddmmBackward0>) tensor(0.7339)\n",
      "tensor([[0.5499]], grad_fn=<AddmmBackward0>) tensor(0.5540)\n",
      "tensor([[-0.5670]], grad_fn=<AddmmBackward0>) tensor(-0.6617)\n",
      "tensor([[0.0336]], grad_fn=<AddmmBackward0>) tensor(0.3042)\n",
      "tensor([[-0.6031]], grad_fn=<AddmmBackward0>) tensor(-0.8444)\n",
      "tensor([[-0.7267]], grad_fn=<AddmmBackward0>) tensor(-0.5389)\n",
      "tensor([[-0.3045]], grad_fn=<AddmmBackward0>) tensor(-0.3369)\n",
      "tensor([[-0.5085]], grad_fn=<AddmmBackward0>) tensor(-0.5392)\n",
      "tensor([[-0.5096]], grad_fn=<AddmmBackward0>) tensor(-0.2849)\n",
      "tensor([[-0.4196]], grad_fn=<AddmmBackward0>) tensor(-0.1514)\n",
      "tensor([[0.2899]], grad_fn=<AddmmBackward0>) tensor(0.5330)\n",
      "tensor([[-1.0558]], grad_fn=<AddmmBackward0>) tensor(-0.7428)\n",
      "tensor([[0.3201]], grad_fn=<AddmmBackward0>) tensor(0.5793)\n",
      "tensor([[-0.9611]], grad_fn=<AddmmBackward0>) tensor(-0.7775)\n",
      "tensor([[-0.6369]], grad_fn=<AddmmBackward0>) tensor(-0.4242)\n",
      "tensor([[0.5147]], grad_fn=<AddmmBackward0>) tensor(0.5417)\n",
      "tensor([[0.4980]], grad_fn=<AddmmBackward0>) tensor(0.7998)\n",
      "tensor([[-0.5985]], grad_fn=<AddmmBackward0>) tensor(-0.6105)\n",
      "tensor([[0.2288]], grad_fn=<AddmmBackward0>) tensor(0.5876)\n",
      "tensor([[-0.5489]], grad_fn=<AddmmBackward0>) tensor(-0.5338)\n",
      "tensor([[-0.6059]], grad_fn=<AddmmBackward0>) tensor(-0.7282)\n",
      "tensor([[-0.8513]], grad_fn=<AddmmBackward0>) tensor(-0.5269)\n",
      "tensor([[-0.2164]], grad_fn=<AddmmBackward0>) tensor(0.2951)\n",
      "tensor([[-0.2753]], grad_fn=<AddmmBackward0>) tensor(-0.0648)\n",
      "tensor([[0.3988]], grad_fn=<AddmmBackward0>) tensor(0.5694)\n",
      "tensor([[-0.3995]], grad_fn=<AddmmBackward0>) tensor(-0.7660)\n",
      "tensor([[-0.1378]], grad_fn=<AddmmBackward0>) tensor(0.0064)\n",
      "tensor([[0.5117]], grad_fn=<AddmmBackward0>) tensor(0.5167)\n",
      "tensor([[0.5197]], grad_fn=<AddmmBackward0>) tensor(0.5943)\n",
      "tensor([[-0.5420]], grad_fn=<AddmmBackward0>) tensor(-0.6153)\n",
      "tensor([[-0.7165]], grad_fn=<AddmmBackward0>) tensor(-0.1531)\n",
      "tensor([[0.3335]], grad_fn=<AddmmBackward0>) tensor(0.6375)\n",
      "tensor([[0.4354]], grad_fn=<AddmmBackward0>) tensor(0.7390)\n",
      "tensor([[-0.6967]], grad_fn=<AddmmBackward0>) tensor(-0.8114)\n",
      "tensor([[0.2515]], grad_fn=<AddmmBackward0>) tensor(0.7942)\n",
      "tensor([[0.7480]], grad_fn=<AddmmBackward0>) tensor(0.7372)\n",
      "tensor([[-0.4043]], grad_fn=<AddmmBackward0>) tensor(-0.0539)\n",
      "tensor([[-0.1052]], grad_fn=<AddmmBackward0>) tensor(0.0044)\n",
      "tensor([[0.6065]], grad_fn=<AddmmBackward0>) tensor(0.6808)\n",
      "tensor([[-0.6539]], grad_fn=<AddmmBackward0>) tensor(-0.8569)\n",
      "tensor([[-0.4582]], grad_fn=<AddmmBackward0>) tensor(-0.5263)\n",
      "tensor([[0.3069]], grad_fn=<AddmmBackward0>) tensor(0.5781)\n",
      "tensor([[-0.4409]], grad_fn=<AddmmBackward0>) tensor(-0.2505)\n",
      "tensor([[-0.4933]], grad_fn=<AddmmBackward0>) tensor(-0.1082)\n",
      "tensor([[0.0523]], grad_fn=<AddmmBackward0>) tensor(0.4337)\n",
      "tensor([[-0.4811]], grad_fn=<AddmmBackward0>) tensor(-0.4350)\n",
      "tensor([[0.2397]], grad_fn=<AddmmBackward0>) tensor(-0.1639)\n",
      "tensor([[0.4977]], grad_fn=<AddmmBackward0>) tensor(0.7923)\n",
      "tensor([[-0.7384]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.1708]], grad_fn=<AddmmBackward0>) tensor(0.6960)\n",
      "tensor([[-0.4739]], grad_fn=<AddmmBackward0>) tensor(-0.5813)\n",
      "tensor([[0.3501]], grad_fn=<AddmmBackward0>) tensor(0.5934)\n",
      "tensor([[-0.8538]], grad_fn=<AddmmBackward0>) tensor(-0.8462)\n",
      "tensor([[-0.1599]], grad_fn=<AddmmBackward0>) tensor(0.0391)\n",
      "tensor([[-0.8395]], grad_fn=<AddmmBackward0>) tensor(-0.8550)\n",
      "tensor([[-0.8389]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.1526]], grad_fn=<AddmmBackward0>) tensor(0.4796)\n",
      "tensor([[-0.1231]], grad_fn=<AddmmBackward0>) tensor(0.0094)\n",
      "tensor([[-0.4783]], grad_fn=<AddmmBackward0>) tensor(-0.2365)\n",
      "tensor([[-0.6949]], grad_fn=<AddmmBackward0>) tensor(-0.4005)\n",
      "tensor([[-0.4852]], grad_fn=<AddmmBackward0>) tensor(-0.5342)\n",
      "tensor([[-0.6421]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.4274]], grad_fn=<AddmmBackward0>) tensor(0.8025)\n",
      "tensor([[-0.5410]], grad_fn=<AddmmBackward0>) tensor(-0.4066)\n",
      "tensor([[0.5047]], grad_fn=<AddmmBackward0>) tensor(0.7550)\n",
      "tensor([[0.5240]], grad_fn=<AddmmBackward0>) tensor(0.7243)\n",
      "tensor([[-0.7209]], grad_fn=<AddmmBackward0>) tensor(-0.5893)\n",
      "tensor([[0.4045]], grad_fn=<AddmmBackward0>) tensor(0.7396)\n",
      "tensor([[-0.7548]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.2821]], grad_fn=<AddmmBackward0>) tensor(-0.0888)\n",
      "tensor([[-0.7769]], grad_fn=<AddmmBackward0>) tensor(-0.7371)\n",
      "tensor([[-0.3399]], grad_fn=<AddmmBackward0>) tensor(-0.5401)\n",
      "tensor([[0.6114]], grad_fn=<AddmmBackward0>) tensor(0.6448)\n",
      "tensor([[-0.5850]], grad_fn=<AddmmBackward0>) tensor(-0.4942)\n",
      "tensor([[0.3644]], grad_fn=<AddmmBackward0>) tensor(0.5855)\n",
      "tensor([[-0.4065]], grad_fn=<AddmmBackward0>) tensor(-0.7142)\n",
      "tensor([[0.0574]], grad_fn=<AddmmBackward0>) tensor(-0.2569)\n",
      "tensor([[0.0324]], grad_fn=<AddmmBackward0>) tensor(-0.2511)\n",
      "tensor([[-0.2507]], grad_fn=<AddmmBackward0>) tensor(0.0012)\n",
      "tensor([[-0.9650]], grad_fn=<AddmmBackward0>) tensor(-0.9029)\n",
      "tensor([[0.3126]], grad_fn=<AddmmBackward0>) tensor(0.4255)\n",
      "tensor([[0.1483]], grad_fn=<AddmmBackward0>) tensor(-0.1115)\n",
      "tensor([[0.1374]], grad_fn=<AddmmBackward0>) tensor(0.2251)\n",
      "tensor([[-0.4597]], grad_fn=<AddmmBackward0>) tensor(-0.4701)\n",
      "tensor([[0.6738]], grad_fn=<AddmmBackward0>) tensor(0.5271)\n",
      "tensor([[-0.6055]], grad_fn=<AddmmBackward0>) tensor(-0.5009)\n",
      "tensor([[-0.6870]], grad_fn=<AddmmBackward0>) tensor(-0.5989)\n",
      "tensor([[0.2529]], grad_fn=<AddmmBackward0>) tensor(0.1145)\n",
      "tensor([[-0.7266]], grad_fn=<AddmmBackward0>) tensor(-0.8836)\n",
      "tensor([[0.5620]], grad_fn=<AddmmBackward0>) tensor(0.6448)\n",
      "tensor([[0.4289]], grad_fn=<AddmmBackward0>) tensor(0.3929)\n",
      "tensor([[-0.4600]], grad_fn=<AddmmBackward0>) tensor(-0.1030)\n",
      "tensor([[-0.4000]], grad_fn=<AddmmBackward0>) tensor(-0.2302)\n",
      "tensor([[-0.7607]], grad_fn=<AddmmBackward0>) tensor(-0.6526)\n",
      "tensor([[-0.8454]], grad_fn=<AddmmBackward0>) tensor(-0.7518)\n",
      "tensor([[-0.7898]], grad_fn=<AddmmBackward0>) tensor(-0.4110)\n",
      "tensor([[0.5368]], grad_fn=<AddmmBackward0>) tensor(0.6389)\n",
      "tensor([[-0.7252]], grad_fn=<AddmmBackward0>) tensor(-0.7262)\n",
      "tensor([[-0.7720]], grad_fn=<AddmmBackward0>) tensor(-0.3557)\n",
      "tensor([[-0.4541]], grad_fn=<AddmmBackward0>) tensor(-0.6344)\n",
      "tensor([[-0.6136]], grad_fn=<AddmmBackward0>) tensor(-0.7176)\n",
      "tensor([[0.3014]], grad_fn=<AddmmBackward0>) tensor(0.5288)\n",
      "tensor([[-0.2553]], grad_fn=<AddmmBackward0>) tensor(-0.9446)\n",
      "tensor([[-0.7061]], grad_fn=<AddmmBackward0>) tensor(-0.8553)\n",
      "tensor([[-0.5550]], grad_fn=<AddmmBackward0>) tensor(-0.4274)\n",
      "tensor([[0.5970]], grad_fn=<AddmmBackward0>) tensor(0.7581)\n",
      "tensor([[-0.8445]], grad_fn=<AddmmBackward0>) tensor(-0.4401)\n",
      "tensor([[0.0968]], grad_fn=<AddmmBackward0>) tensor(0.3130)\n",
      "tensor([[-0.0318]], grad_fn=<AddmmBackward0>) tensor(-0.0235)\n",
      "tensor([[-0.6606]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.1160]], grad_fn=<AddmmBackward0>) tensor(-0.1107)\n",
      "tensor([[-0.8506]], grad_fn=<AddmmBackward0>) tensor(-0.8342)\n",
      "tensor([[-0.5807]], grad_fn=<AddmmBackward0>) tensor(-0.4400)\n",
      "tensor([[-0.6956]], grad_fn=<AddmmBackward0>) tensor(-0.8901)\n",
      "tensor([[0.3954]], grad_fn=<AddmmBackward0>) tensor(0.7231)\n",
      "tensor([[-0.4909]], grad_fn=<AddmmBackward0>) tensor(-0.5523)\n",
      "tensor([[-0.6576]], grad_fn=<AddmmBackward0>) tensor(-0.8397)\n",
      "tensor([[0.4281]], grad_fn=<AddmmBackward0>) tensor(0.7176)\n",
      "tensor([[-0.0204]], grad_fn=<AddmmBackward0>) tensor(0.0413)\n",
      "tensor([[0.3616]], grad_fn=<AddmmBackward0>) tensor(0.5820)\n",
      "tensor([[-0.6903]], grad_fn=<AddmmBackward0>) tensor(-0.8500)\n",
      "tensor([[0.3390]], grad_fn=<AddmmBackward0>) tensor(0.5767)\n",
      "tensor([[0.1431]], grad_fn=<AddmmBackward0>) tensor(0.3720)\n",
      "tensor([[0.3158]], grad_fn=<AddmmBackward0>) tensor(0.9091)\n",
      "tensor([[-0.5734]], grad_fn=<AddmmBackward0>) tensor(0.4284)\n",
      "tensor([[0.4588]], grad_fn=<AddmmBackward0>) tensor(0.5629)\n",
      "tensor([[-0.4561]], grad_fn=<AddmmBackward0>) tensor(-0.1113)\n",
      "tensor([[-0.8510]], grad_fn=<AddmmBackward0>) tensor(-0.8203)\n",
      "tensor([[-0.7410]], grad_fn=<AddmmBackward0>) tensor(-0.7982)\n",
      "tensor([[0.8666]], grad_fn=<AddmmBackward0>) tensor(0.7585)\n",
      "tensor([[0.4727]], grad_fn=<AddmmBackward0>) tensor(0.7285)\n",
      "tensor([[-0.5161]], grad_fn=<AddmmBackward0>) tensor(-0.4152)\n",
      "tensor([[-0.7637]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.7372]], grad_fn=<AddmmBackward0>) tensor(-0.8633)\n",
      "tensor([[-0.5118]], grad_fn=<AddmmBackward0>) tensor(-0.5282)\n",
      "tensor([[-0.5786]], grad_fn=<AddmmBackward0>) tensor(-0.1806)\n",
      "tensor([[0.3978]], grad_fn=<AddmmBackward0>) tensor(0.5178)\n",
      "tensor([[-0.0084]], grad_fn=<AddmmBackward0>) tensor(0.0245)\n",
      "tensor([[0.2769]], grad_fn=<AddmmBackward0>) tensor(0.5225)\n",
      "tensor([[-0.7469]], grad_fn=<AddmmBackward0>) tensor(-0.8102)\n",
      "tensor([[0.5158]], grad_fn=<AddmmBackward0>) tensor(0.8440)\n",
      "tensor([[-0.4432]], grad_fn=<AddmmBackward0>) tensor(-0.2663)\n",
      "tensor([[-0.7777]], grad_fn=<AddmmBackward0>) tensor(-0.7986)\n",
      "tensor([[0.1176]], grad_fn=<AddmmBackward0>) tensor(0.2672)\n",
      "tensor([[0.1526]], grad_fn=<AddmmBackward0>) tensor(0.2225)\n",
      "tensor([[0.4400]], grad_fn=<AddmmBackward0>) tensor(0.7867)\n",
      "tensor([[-0.4091]], grad_fn=<AddmmBackward0>) tensor(-0.3981)\n",
      "tensor([[-0.3181]], grad_fn=<AddmmBackward0>) tensor(0.0352)\n",
      "tensor([[-0.6673]], grad_fn=<AddmmBackward0>) tensor(-0.5765)\n",
      "tensor([[-0.5437]], grad_fn=<AddmmBackward0>) tensor(-0.1087)\n",
      "tensor([[-0.7362]], grad_fn=<AddmmBackward0>) tensor(-0.8310)\n",
      "tensor([[-0.8558]], grad_fn=<AddmmBackward0>) tensor(-0.4721)\n",
      "tensor([[0.6558]], grad_fn=<AddmmBackward0>) tensor(0.5167)\n",
      "tensor([[-0.4642]], grad_fn=<AddmmBackward0>) tensor(-0.4656)\n",
      "tensor([[0.5598]], grad_fn=<AddmmBackward0>) tensor(0.5273)\n",
      "tensor([[-0.0627]], grad_fn=<AddmmBackward0>) tensor(-0.0081)\n",
      "tensor([[0.2114]], grad_fn=<AddmmBackward0>) tensor(0.1781)\n",
      "tensor([[-0.2264]], grad_fn=<AddmmBackward0>) tensor(0.3519)\n",
      "tensor([[0.1432]], grad_fn=<AddmmBackward0>) tensor(0.4157)\n",
      "tensor([[-0.6593]], grad_fn=<AddmmBackward0>) tensor(-0.1944)\n",
      "tensor([[-0.6817]], grad_fn=<AddmmBackward0>) tensor(-0.9724)\n",
      "tensor([[-0.6146]], grad_fn=<AddmmBackward0>) tensor(-0.7307)\n",
      "tensor([[0.3822]], grad_fn=<AddmmBackward0>) tensor(0.6320)\n",
      "tensor([[-0.6982]], grad_fn=<AddmmBackward0>) tensor(-0.4489)\n",
      "tensor([[-0.2139]], grad_fn=<AddmmBackward0>) tensor(-0.2564)\n",
      "tensor([[-0.3417]], grad_fn=<AddmmBackward0>) tensor(-0.3020)\n",
      "tensor([[-0.7836]], grad_fn=<AddmmBackward0>) tensor(-0.1151)\n",
      "tensor([[-0.5702]], grad_fn=<AddmmBackward0>) tensor(-0.9470)\n",
      "tensor([[0.4188]], grad_fn=<AddmmBackward0>) tensor(0.5793)\n",
      "tensor([[-0.8202]], grad_fn=<AddmmBackward0>) tensor(-0.8012)\n",
      "tensor([[0.3204]], grad_fn=<AddmmBackward0>) tensor(0.7214)\n",
      "tensor([[-0.2995]], grad_fn=<AddmmBackward0>) tensor(-0.3862)\n",
      "tensor([[-0.0292]], grad_fn=<AddmmBackward0>) tensor(-0.4349)\n",
      "tensor([[-0.7231]], grad_fn=<AddmmBackward0>) tensor(-0.8879)\n",
      "tensor([[-0.5371]], grad_fn=<AddmmBackward0>) tensor(0.0348)\n",
      "tensor([[-0.5085]], grad_fn=<AddmmBackward0>) tensor(-0.4986)\n",
      "tensor([[0.1496]], grad_fn=<AddmmBackward0>) tensor(0.6023)\n",
      "tensor([[-0.1305]], grad_fn=<AddmmBackward0>) tensor(-0.2671)\n",
      "tensor([[-0.3339]], grad_fn=<AddmmBackward0>) tensor(-0.0431)\n",
      "tensor([[-0.6793]], grad_fn=<AddmmBackward0>) tensor(-0.6671)\n",
      "tensor([[-0.5280]], grad_fn=<AddmmBackward0>) tensor(-0.3308)\n",
      "tensor([[0.5149]], grad_fn=<AddmmBackward0>) tensor(0.7508)\n",
      "tensor([[-0.4297]], grad_fn=<AddmmBackward0>) tensor(-0.6855)\n",
      "tensor([[-0.6057]], grad_fn=<AddmmBackward0>) tensor(-0.2748)\n",
      "tensor([[0.4991]], grad_fn=<AddmmBackward0>) tensor(0.7185)\n",
      "tensor([[-0.5466]], grad_fn=<AddmmBackward0>) tensor(-0.6056)\n",
      "tensor([[-0.5737]], grad_fn=<AddmmBackward0>) tensor(0.0430)\n",
      "tensor([[0.6367]], grad_fn=<AddmmBackward0>) tensor(0.5411)\n",
      "tensor([[-0.6739]], grad_fn=<AddmmBackward0>) tensor(-0.8430)\n",
      "tensor([[-0.4991]], grad_fn=<AddmmBackward0>) tensor(-0.2903)\n",
      "tensor([[0.6403]], grad_fn=<AddmmBackward0>) tensor(0.5946)\n",
      "tensor([[-0.5518]], grad_fn=<AddmmBackward0>) tensor(-0.7224)\n",
      "tensor([[0.6154]], grad_fn=<AddmmBackward0>) tensor(0.7251)\n",
      "tensor([[-0.4234]], grad_fn=<AddmmBackward0>) tensor(-0.1597)\n",
      "tensor([[-0.5300]], grad_fn=<AddmmBackward0>) tensor(-0.5011)\n",
      "tensor([[-0.6201]], grad_fn=<AddmmBackward0>) tensor(-0.2624)\n",
      "tensor([[-0.0270]], grad_fn=<AddmmBackward0>) tensor(0.1814)\n",
      "tensor([[-0.2042]], grad_fn=<AddmmBackward0>) tensor(0.2153)\n",
      "tensor([[-0.5959]], grad_fn=<AddmmBackward0>) tensor(-0.4710)\n",
      "tensor([[0.1539]], grad_fn=<AddmmBackward0>) tensor(-0.2209)\n",
      "tensor([[0.1562]], grad_fn=<AddmmBackward0>) tensor(0.6546)\n",
      "tensor([[-0.7852]], grad_fn=<AddmmBackward0>) tensor(-0.7002)\n",
      "tensor([[-0.4233]], grad_fn=<AddmmBackward0>) tensor(-0.1137)\n",
      "tensor([[-0.2651]], grad_fn=<AddmmBackward0>) tensor(-0.2833)\n",
      "tensor([[-0.8032]], grad_fn=<AddmmBackward0>) tensor(-0.9642)\n",
      "tensor([[-0.2573]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.0058]], grad_fn=<AddmmBackward0>) tensor(0.1846)\n",
      "tensor([[-0.1577]], grad_fn=<AddmmBackward0>) tensor(0.9108)\n",
      "tensor([[-0.8341]], grad_fn=<AddmmBackward0>) tensor(-0.5459)\n",
      "tensor([[-0.6378]], grad_fn=<AddmmBackward0>) tensor(-0.8924)\n",
      "tensor([[0.3985]], grad_fn=<AddmmBackward0>) tensor(0.5585)\n",
      "tensor([[-0.2126]], grad_fn=<AddmmBackward0>) tensor(-0.0598)\n",
      "tensor([[-0.5696]], grad_fn=<AddmmBackward0>) tensor(-0.2456)\n",
      "tensor([[-0.4445]], grad_fn=<AddmmBackward0>) tensor(-0.3626)\n",
      "tensor([[-0.6209]], grad_fn=<AddmmBackward0>) tensor(-0.5647)\n",
      "tensor([[-0.4873]], grad_fn=<AddmmBackward0>) tensor(-0.6982)\n",
      "tensor([[-0.6438]], grad_fn=<AddmmBackward0>) tensor(-0.8887)\n",
      "tensor([[-0.7284]], grad_fn=<AddmmBackward0>) tensor(-0.7065)\n",
      "tensor([[-0.5310]], grad_fn=<AddmmBackward0>) tensor(-0.6308)\n",
      "tensor([[-0.6578]], grad_fn=<AddmmBackward0>) tensor(-0.6090)\n",
      "tensor([[0.9283]], grad_fn=<AddmmBackward0>) tensor(0.8067)\n",
      "tensor([[0.3033]], grad_fn=<AddmmBackward0>) tensor(0.6620)\n",
      "tensor([[-0.8322]], grad_fn=<AddmmBackward0>) tensor(-0.9286)\n",
      "tensor([[-0.8637]], grad_fn=<AddmmBackward0>) tensor(-0.9038)\n",
      "tensor([[-1.1208]], grad_fn=<AddmmBackward0>) tensor(-0.9437)\n",
      "tensor([[-0.6072]], grad_fn=<AddmmBackward0>) tensor(-0.5648)\n",
      "tensor([[-0.7112]], grad_fn=<AddmmBackward0>) tensor(-0.7538)\n",
      "tensor([[-0.5826]], grad_fn=<AddmmBackward0>) tensor(-0.5071)\n",
      "tensor([[0.6123]], grad_fn=<AddmmBackward0>) tensor(0.5376)\n",
      "tensor([[0.5824]], grad_fn=<AddmmBackward0>) tensor(0.5525)\n",
      "tensor([[-0.1755]], grad_fn=<AddmmBackward0>) tensor(-0.1580)\n",
      "tensor([[-1.0046]], grad_fn=<AddmmBackward0>) tensor(-0.7419)\n",
      "tensor([[-0.0534]], grad_fn=<AddmmBackward0>) tensor(0.7532)\n",
      "tensor([[-0.2611]], grad_fn=<AddmmBackward0>) tensor(-0.1307)\n",
      "tensor([[0.1326]], grad_fn=<AddmmBackward0>) tensor(0.0461)\n",
      "tensor([[-1.1077]], grad_fn=<AddmmBackward0>) tensor(-0.6204)\n",
      "tensor([[-0.8081]], grad_fn=<AddmmBackward0>) tensor(-0.6961)\n",
      "tensor([[-0.1009]], grad_fn=<AddmmBackward0>) tensor(0.1949)\n",
      "tensor([[-0.2667]], grad_fn=<AddmmBackward0>) tensor(-0.2535)\n",
      "tensor([[-0.0622]], grad_fn=<AddmmBackward0>) tensor(0.2218)\n",
      "tensor([[-0.8754]], grad_fn=<AddmmBackward0>) tensor(-0.8347)\n",
      "tensor([[-0.1761]], grad_fn=<AddmmBackward0>) tensor(0.3517)\n",
      "tensor([[0.1857]], grad_fn=<AddmmBackward0>) tensor(0.0593)\n",
      "tensor([[-0.2523]], grad_fn=<AddmmBackward0>) tensor(-0.3171)\n",
      "tensor([[-0.6382]], grad_fn=<AddmmBackward0>) tensor(-0.9217)\n",
      "tensor([[-0.8271]], grad_fn=<AddmmBackward0>) tensor(-0.6960)\n",
      "tensor([[-0.3841]], grad_fn=<AddmmBackward0>) tensor(0.4191)\n",
      "tensor([[0.3880]], grad_fn=<AddmmBackward0>) tensor(0.5776)\n",
      "tensor([[-0.2326]], grad_fn=<AddmmBackward0>) tensor(-0.3590)\n",
      "tensor([[-0.3086]], grad_fn=<AddmmBackward0>) tensor(-0.4648)\n",
      "tensor([[0.5112]], grad_fn=<AddmmBackward0>) tensor(0.6395)\n",
      "tensor([[0.3254]], grad_fn=<AddmmBackward0>) tensor(0.5078)\n",
      "tensor([[-0.6624]], grad_fn=<AddmmBackward0>) tensor(-0.8696)\n",
      "tensor([[0.5269]], grad_fn=<AddmmBackward0>) tensor(0.5342)\n",
      "tensor([[-0.8511]], grad_fn=<AddmmBackward0>) tensor(-0.5395)\n",
      "tensor([[-0.7185]], grad_fn=<AddmmBackward0>) tensor(-0.4915)\n",
      "tensor([[-0.7109]], grad_fn=<AddmmBackward0>) tensor(-0.8027)\n",
      "tensor([[-0.9248]], grad_fn=<AddmmBackward0>) tensor(-0.1533)\n",
      "tensor([[-0.7127]], grad_fn=<AddmmBackward0>) tensor(-0.7511)\n",
      "tensor([[-0.7651]], grad_fn=<AddmmBackward0>) tensor(-0.7297)\n",
      "tensor([[-0.4478]], grad_fn=<AddmmBackward0>) tensor(-0.1214)\n",
      "tensor([[-0.4522]], grad_fn=<AddmmBackward0>) tensor(-0.3459)\n",
      "tensor([[-0.4867]], grad_fn=<AddmmBackward0>) tensor(-0.5296)\n",
      "tensor([[0.1413]], grad_fn=<AddmmBackward0>) tensor(0.6950)\n",
      "tensor([[0.2981]], grad_fn=<AddmmBackward0>) tensor(0.3375)\n",
      "tensor([[-0.5321]], grad_fn=<AddmmBackward0>) tensor(-0.5113)\n",
      "tensor([[-0.4158]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.1793]], grad_fn=<AddmmBackward0>) tensor(0.2356)\n",
      "tensor([[0.5411]], grad_fn=<AddmmBackward0>) tensor(0.5298)\n",
      "tensor([[0.5386]], grad_fn=<AddmmBackward0>) tensor(0.2823)\n",
      "tensor([[-0.3874]], grad_fn=<AddmmBackward0>) tensor(-0.1072)\n",
      "tensor([[0.2496]], grad_fn=<AddmmBackward0>) tensor(0.5625)\n",
      "tensor([[-0.6857]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.6192]], grad_fn=<AddmmBackward0>) tensor(0.5515)\n",
      "tensor([[0.2825]], grad_fn=<AddmmBackward0>) tensor(0.5676)\n",
      "tensor([[0.0544]], grad_fn=<AddmmBackward0>) tensor(0.2636)\n",
      "tensor([[-0.7928]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.6009]], grad_fn=<AddmmBackward0>) tensor(0.5587)\n",
      "tensor([[-0.5757]], grad_fn=<AddmmBackward0>) tensor(-0.4268)\n",
      "tensor([[0.5086]], grad_fn=<AddmmBackward0>) tensor(0.7851)\n",
      "tensor([[-1.0444]], grad_fn=<AddmmBackward0>) tensor(-0.9058)\n",
      "tensor([[-0.7359]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.5790]], grad_fn=<AddmmBackward0>) tensor(0.6856)\n",
      "tensor([[0.0930]], grad_fn=<AddmmBackward0>) tensor(-0.1766)\n",
      "tensor([[-0.5138]], grad_fn=<AddmmBackward0>) tensor(-0.5228)\n",
      "tensor([[-0.9579]], grad_fn=<AddmmBackward0>) tensor(-0.8643)\n",
      "tensor([[0.6485]], grad_fn=<AddmmBackward0>) tensor(0.5292)\n",
      "tensor([[-0.5837]], grad_fn=<AddmmBackward0>) tensor(-0.6675)\n",
      "tensor([[-0.8231]], grad_fn=<AddmmBackward0>) tensor(-0.5784)\n",
      "tensor([[-0.3788]], grad_fn=<AddmmBackward0>) tensor(-0.7801)\n",
      "tensor([[0.1423]], grad_fn=<AddmmBackward0>) tensor(0.6722)\n",
      "tensor([[-0.2648]], grad_fn=<AddmmBackward0>) tensor(-0.7320)\n",
      "tensor([[-0.4355]], grad_fn=<AddmmBackward0>) tensor(-0.4659)\n",
      "tensor([[-0.5904]], grad_fn=<AddmmBackward0>) tensor(-0.6425)\n",
      "tensor([[-0.4924]], grad_fn=<AddmmBackward0>) tensor(-0.2731)\n",
      "tensor([[-0.5845]], grad_fn=<AddmmBackward0>) tensor(-0.9257)\n",
      "tensor([[-0.6686]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.7187]], grad_fn=<AddmmBackward0>) tensor(-0.8554)\n",
      "tensor([[0.6139]], grad_fn=<AddmmBackward0>) tensor(-0.3904)\n",
      "tensor([[-0.1569]], grad_fn=<AddmmBackward0>) tensor(-0.1671)\n",
      "tensor([[0.1964]], grad_fn=<AddmmBackward0>) tensor(0.5317)\n",
      "tensor([[-1.1083]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.0687]], grad_fn=<AddmmBackward0>) tensor(0.6181)\n",
      "tensor([[-1.4120]], grad_fn=<AddmmBackward0>) tensor(-0.9547)\n",
      "tensor([[-0.6595]], grad_fn=<AddmmBackward0>) tensor(-0.6847)\n",
      "tensor([[-0.5111]], grad_fn=<AddmmBackward0>) tensor(-0.5445)\n",
      "tensor([[-0.2522]], grad_fn=<AddmmBackward0>) tensor(-0.2541)\n",
      "tensor([[0.3203]], grad_fn=<AddmmBackward0>) tensor(0.6916)\n",
      "tensor([[0.4730]], grad_fn=<AddmmBackward0>) tensor(0.0986)\n",
      "tensor([[0.1115]], grad_fn=<AddmmBackward0>) tensor(0.8434)\n",
      "tensor([[-0.6960]], grad_fn=<AddmmBackward0>) tensor(-0.8509)\n",
      "tensor([[0.2582]], grad_fn=<AddmmBackward0>) tensor(0.7111)\n",
      "tensor([[-0.7911]], grad_fn=<AddmmBackward0>) tensor(-0.8201)\n",
      "tensor([[-0.1352]], grad_fn=<AddmmBackward0>) tensor(0.0180)\n",
      "tensor([[0.7154]], grad_fn=<AddmmBackward0>) tensor(0.6018)\n",
      "tensor([[0.4827]], grad_fn=<AddmmBackward0>) tensor(0.5844)\n",
      "tensor([[-0.5840]], grad_fn=<AddmmBackward0>) tensor(-0.4301)\n",
      "tensor([[0.4338]], grad_fn=<AddmmBackward0>) tensor(0.5482)\n",
      "tensor([[-1.0174]], grad_fn=<AddmmBackward0>) tensor(-0.9531)\n",
      "tensor([[-0.0192]], grad_fn=<AddmmBackward0>) tensor(0.0563)\n",
      "tensor([[-0.5527]], grad_fn=<AddmmBackward0>) tensor(-0.9449)\n",
      "tensor([[0.5888]], grad_fn=<AddmmBackward0>) tensor(0.5704)\n",
      "tensor([[0.6787]], grad_fn=<AddmmBackward0>) tensor(0.9689)\n",
      "tensor([[0.5866]], grad_fn=<AddmmBackward0>) tensor(0.5461)\n",
      "tensor([[0.0400]], grad_fn=<AddmmBackward0>) tensor(0.0270)\n",
      "tensor([[-0.6488]], grad_fn=<AddmmBackward0>) tensor(-0.5008)\n",
      "tensor([[-0.6042]], grad_fn=<AddmmBackward0>) tensor(-0.4378)\n",
      "tensor([[-0.6284]], grad_fn=<AddmmBackward0>) tensor(-0.4566)\n",
      "tensor([[-0.0406]], grad_fn=<AddmmBackward0>) tensor(0.5587)\n",
      "tensor([[0.1373]], grad_fn=<AddmmBackward0>) tensor(0.5350)\n",
      "tensor([[0.3196]], grad_fn=<AddmmBackward0>) tensor(0.3625)\n",
      "tensor([[0.2289]], grad_fn=<AddmmBackward0>) tensor(0.1280)\n",
      "tensor([[-0.4816]], grad_fn=<AddmmBackward0>) tensor(-0.9607)\n",
      "tensor([[-0.3646]], grad_fn=<AddmmBackward0>) tensor(-0.4090)\n",
      "tensor([[-0.4100]], grad_fn=<AddmmBackward0>) tensor(-0.1398)\n",
      "tensor([[-0.5948]], grad_fn=<AddmmBackward0>) tensor(-0.6013)\n",
      "tensor([[0.5616]], grad_fn=<AddmmBackward0>) tensor(0.5705)\n",
      "tensor([[-0.6165]], grad_fn=<AddmmBackward0>) tensor(-0.8010)\n",
      "tensor([[-0.5104]], grad_fn=<AddmmBackward0>) tensor(-0.7191)\n",
      "tensor([[-0.5398]], grad_fn=<AddmmBackward0>) tensor(-0.4690)\n",
      "tensor([[-0.6530]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.0365]], grad_fn=<AddmmBackward0>) tensor(-0.3067)\n",
      "tensor([[-0.8124]], grad_fn=<AddmmBackward0>) tensor(-0.7047)\n",
      "tensor([[0.6167]], grad_fn=<AddmmBackward0>) tensor(0.8289)\n",
      "tensor([[-0.3899]], grad_fn=<AddmmBackward0>) tensor(-0.3577)\n",
      "tensor([[0.2067]], grad_fn=<AddmmBackward0>) tensor(0.5276)\n",
      "tensor([[-1.1052]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.0421]], grad_fn=<AddmmBackward0>) tensor(0.4949)\n",
      "tensor([[-0.6612]], grad_fn=<AddmmBackward0>) tensor(-0.6565)\n",
      "tensor([[-0.7723]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.1254]], grad_fn=<AddmmBackward0>) tensor(0.5503)\n",
      "tensor([[-0.0569]], grad_fn=<AddmmBackward0>) tensor(-0.2043)\n",
      "tensor([[-0.0585]], grad_fn=<AddmmBackward0>) tensor(0.1094)\n",
      "tensor([[-0.6662]], grad_fn=<AddmmBackward0>) tensor(-0.4776)\n",
      "tensor([[-0.6000]], grad_fn=<AddmmBackward0>) tensor(-0.3933)\n",
      "tensor([[0.1421]], grad_fn=<AddmmBackward0>) tensor(0.0860)\n",
      "tensor([[0.2009]], grad_fn=<AddmmBackward0>) tensor(0.6165)\n",
      "tensor([[0.2730]], grad_fn=<AddmmBackward0>) tensor(0.5209)\n",
      "tensor([[-0.4591]], grad_fn=<AddmmBackward0>) tensor(-0.3681)\n",
      "tensor([[-0.6880]], grad_fn=<AddmmBackward0>) tensor(-0.5986)\n",
      "tensor([[-0.6121]], grad_fn=<AddmmBackward0>) tensor(-0.6422)\n",
      "tensor([[-0.8329]], grad_fn=<AddmmBackward0>) tensor(-0.8723)\n",
      "tensor([[0.4275]], grad_fn=<AddmmBackward0>) tensor(0.3731)\n",
      "tensor([[0.4278]], grad_fn=<AddmmBackward0>) tensor(0.5944)\n",
      "tensor([[-0.1331]], grad_fn=<AddmmBackward0>) tensor(-0.1460)\n",
      "tensor([[-0.5407]], grad_fn=<AddmmBackward0>) tensor(-0.9559)\n",
      "tensor([[-0.1251]], grad_fn=<AddmmBackward0>) tensor(0.0216)\n",
      "tensor([[-0.2849]], grad_fn=<AddmmBackward0>) tensor(-0.2563)\n",
      "tensor([[0.3717]], grad_fn=<AddmmBackward0>) tensor(0.5141)\n",
      "tensor([[0.3477]], grad_fn=<AddmmBackward0>) tensor(0.5419)\n",
      "tensor([[-0.4935]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.0545]], grad_fn=<AddmmBackward0>) tensor(-0.0441)\n",
      "tensor([[-0.5825]], grad_fn=<AddmmBackward0>) tensor(-0.8656)\n",
      "tensor([[-0.5161]], grad_fn=<AddmmBackward0>) tensor(-0.5100)\n",
      "tensor([[-0.3648]], grad_fn=<AddmmBackward0>) tensor(-0.5494)\n",
      "tensor([[-0.4098]], grad_fn=<AddmmBackward0>) tensor(-0.9558)\n",
      "tensor([[0.1476]], grad_fn=<AddmmBackward0>) tensor(0.1862)\n",
      "tensor([[0.2873]], grad_fn=<AddmmBackward0>) tensor(0.3660)\n",
      "tensor([[-0.2513]], grad_fn=<AddmmBackward0>) tensor(-0.2576)\n",
      "tensor([[-0.4923]], grad_fn=<AddmmBackward0>) tensor(-0.4882)\n",
      "tensor([[0.4247]], grad_fn=<AddmmBackward0>) tensor(0.5306)\n",
      "tensor([[-0.0146]], grad_fn=<AddmmBackward0>) tensor(0.1637)\n",
      "tensor([[-0.7316]], grad_fn=<AddmmBackward0>) tensor(-0.4902)\n",
      "tensor([[-0.6559]], grad_fn=<AddmmBackward0>) tensor(-0.4237)\n",
      "tensor([[-0.7731]], grad_fn=<AddmmBackward0>) tensor(-0.8484)\n",
      "tensor([[-0.0509]], grad_fn=<AddmmBackward0>) tensor(0.2719)\n",
      "tensor([[0.2383]], grad_fn=<AddmmBackward0>) tensor(0.2915)\n",
      "tensor([[-0.2332]], grad_fn=<AddmmBackward0>) tensor(-0.0445)\n",
      "tensor([[0.5920]], grad_fn=<AddmmBackward0>) tensor(0.7954)\n",
      "tensor([[0.1826]], grad_fn=<AddmmBackward0>) tensor(0.3647)\n",
      "tensor([[-0.7445]], grad_fn=<AddmmBackward0>) tensor(-0.9755)\n",
      "tensor([[-0.4205]], grad_fn=<AddmmBackward0>) tensor(-0.5217)\n",
      "tensor([[-0.5912]], grad_fn=<AddmmBackward0>) tensor(-0.4649)\n",
      "tensor([[0.1411]], grad_fn=<AddmmBackward0>) tensor(0.1869)\n",
      "tensor([[-0.6045]], grad_fn=<AddmmBackward0>) tensor(-0.4792)\n",
      "tensor([[-0.3004]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.0808]], grad_fn=<AddmmBackward0>) tensor(0.0548)\n",
      "tensor([[-0.0233]], grad_fn=<AddmmBackward0>) tensor(0.7314)\n",
      "tensor([[-0.7567]], grad_fn=<AddmmBackward0>) tensor(-0.2795)\n",
      "tensor([[-0.7936]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.6786]], grad_fn=<AddmmBackward0>) tensor(-0.8838)\n",
      "tensor([[-0.4156]], grad_fn=<AddmmBackward0>) tensor(-0.1514)\n",
      "tensor([[-0.3141]], grad_fn=<AddmmBackward0>) tensor(-0.2147)\n",
      "tensor([[0.2264]], grad_fn=<AddmmBackward0>) tensor(0.9214)\n",
      "tensor([[0.4055]], grad_fn=<AddmmBackward0>) tensor(0.5883)\n",
      "tensor([[-0.5183]], grad_fn=<AddmmBackward0>) tensor(-0.4852)\n",
      "tensor([[0.3241]], grad_fn=<AddmmBackward0>) tensor(0.5637)\n",
      "tensor([[0.2043]], grad_fn=<AddmmBackward0>) tensor(0.7731)\n",
      "tensor([[-0.4091]], grad_fn=<AddmmBackward0>) tensor(-0.8895)\n",
      "tensor([[-0.6686]], grad_fn=<AddmmBackward0>) tensor(-0.9118)\n",
      "tensor([[0.5633]], grad_fn=<AddmmBackward0>) tensor(0.5400)\n",
      "tensor([[-1.0429]], grad_fn=<AddmmBackward0>) tensor(-0.9775)\n",
      "tensor([[0.4802]], grad_fn=<AddmmBackward0>) tensor(0.7870)\n",
      "tensor([[0.3156]], grad_fn=<AddmmBackward0>) tensor(0.3688)\n",
      "tensor([[-0.0940]], grad_fn=<AddmmBackward0>) tensor(0.5187)\n",
      "tensor([[-0.7831]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.3162]], grad_fn=<AddmmBackward0>) tensor(-0.2191)\n",
      "tensor([[-0.7607]], grad_fn=<AddmmBackward0>) tensor(-0.8334)\n",
      "tensor([[-0.4853]], grad_fn=<AddmmBackward0>) tensor(-0.4778)\n",
      "tensor([[-0.1936]], grad_fn=<AddmmBackward0>) tensor(0.0337)\n",
      "tensor([[0.0241]], grad_fn=<AddmmBackward0>) tensor(0.7661)\n",
      "tensor([[-0.0460]], grad_fn=<AddmmBackward0>) tensor(-0.0917)\n",
      "tensor([[0.0695]], grad_fn=<AddmmBackward0>) tensor(0.5224)\n",
      "tensor([[-0.6326]], grad_fn=<AddmmBackward0>) tensor(-0.9704)\n",
      "tensor([[0.2164]], grad_fn=<AddmmBackward0>) tensor(0.2431)\n",
      "tensor([[-0.5239]], grad_fn=<AddmmBackward0>) tensor(-0.8552)\n",
      "tensor([[-0.4051]], grad_fn=<AddmmBackward0>) tensor(0.0779)\n",
      "tensor([[-0.2633]], grad_fn=<AddmmBackward0>) tensor(0.5098)\n",
      "tensor([[-0.1274]], grad_fn=<AddmmBackward0>) tensor(-0.2425)\n",
      "tensor([[0.4962]], grad_fn=<AddmmBackward0>) tensor(0.7072)\n",
      "tensor([[-0.6754]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.4704]], grad_fn=<AddmmBackward0>) tensor(0.0353)\n",
      "tensor([[0.6314]], grad_fn=<AddmmBackward0>) tensor(0.7150)\n",
      "tensor([[-0.7990]], grad_fn=<AddmmBackward0>) tensor(-0.5658)\n",
      "tensor([[-0.7664]], grad_fn=<AddmmBackward0>) tensor(-0.7214)\n",
      "tensor([[-0.0296]], grad_fn=<AddmmBackward0>) tensor(0.6168)\n",
      "tensor([[-0.6497]], grad_fn=<AddmmBackward0>) tensor(-0.6185)\n",
      "tensor([[0.1037]], grad_fn=<AddmmBackward0>) tensor(0.2426)\n",
      "tensor([[-0.9248]], grad_fn=<AddmmBackward0>) tensor(-0.9502)\n",
      "tensor([[-0.9767]], grad_fn=<AddmmBackward0>) tensor(-0.8737)\n",
      "tensor([[-0.5375]], grad_fn=<AddmmBackward0>) tensor(-0.7287)\n",
      "tensor([[0.2966]], grad_fn=<AddmmBackward0>) tensor(0.2251)\n",
      "tensor([[-0.0117]], grad_fn=<AddmmBackward0>) tensor(0.1303)\n",
      "tensor([[-0.2324]], grad_fn=<AddmmBackward0>) tensor(0.1247)\n",
      "tensor([[0.7692]], grad_fn=<AddmmBackward0>) tensor(0.5957)\n",
      "tensor([[-0.6526]], grad_fn=<AddmmBackward0>) tensor(-0.4151)\n",
      "tensor([[-0.7434]], grad_fn=<AddmmBackward0>) tensor(-0.8528)\n",
      "tensor([[-0.0515]], grad_fn=<AddmmBackward0>) tensor(0.3031)\n",
      "tensor([[-0.0314]], grad_fn=<AddmmBackward0>) tensor(0.2020)\n",
      "tensor([[0.6555]], grad_fn=<AddmmBackward0>) tensor(0.6037)\n",
      "tensor([[-0.1256]], grad_fn=<AddmmBackward0>) tensor(-0.0284)\n",
      "tensor([[-0.5773]], grad_fn=<AddmmBackward0>) tensor(-0.8993)\n",
      "tensor([[-0.2460]], grad_fn=<AddmmBackward0>) tensor(0.4827)\n",
      "tensor([[-0.5645]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.7298]], grad_fn=<AddmmBackward0>) tensor(0.7416)\n",
      "tensor([[0.5786]], grad_fn=<AddmmBackward0>) tensor(0.5357)\n",
      "tensor([[-0.5715]], grad_fn=<AddmmBackward0>) tensor(-0.1870)\n",
      "tensor([[0.3558]], grad_fn=<AddmmBackward0>) tensor(0.3942)\n",
      "tensor([[-0.7958]], grad_fn=<AddmmBackward0>) tensor(-0.8726)\n",
      "tensor([[-0.1065]], grad_fn=<AddmmBackward0>) tensor(0.0344)\n",
      "tensor([[-0.1361]], grad_fn=<AddmmBackward0>) tensor(0.0199)\n",
      "tensor([[-0.1079]], grad_fn=<AddmmBackward0>) tensor(0.3090)\n",
      "tensor([[-0.9764]], grad_fn=<AddmmBackward0>) tensor(-0.7326)\n",
      "tensor([[-0.9172]], grad_fn=<AddmmBackward0>) tensor(-0.9064)\n",
      "tensor([[-0.7557]], grad_fn=<AddmmBackward0>) tensor(-0.3470)\n",
      "tensor([[-0.2723]], grad_fn=<AddmmBackward0>) tensor(-0.2881)\n",
      "tensor([[-0.4849]], grad_fn=<AddmmBackward0>) tensor(-0.5516)\n",
      "tensor([[-0.6668]], grad_fn=<AddmmBackward0>) tensor(-0.8536)\n",
      "tensor([[0.1960]], grad_fn=<AddmmBackward0>) tensor(0.3439)\n",
      "tensor([[-0.4126]], grad_fn=<AddmmBackward0>) tensor(-0.8035)\n",
      "tensor([[-0.8156]], grad_fn=<AddmmBackward0>) tensor(-0.8470)\n",
      "tensor([[-0.6659]], grad_fn=<AddmmBackward0>) tensor(-0.4106)\n",
      "tensor([[0.4113]], grad_fn=<AddmmBackward0>) tensor(0.7202)\n",
      "tensor([[-0.8580]], grad_fn=<AddmmBackward0>) tensor(-0.7954)\n",
      "tensor([[-0.6040]], grad_fn=<AddmmBackward0>) tensor(-0.2723)\n",
      "tensor([[-0.8843]], grad_fn=<AddmmBackward0>) tensor(-0.8480)\n",
      "tensor([[-0.4533]], grad_fn=<AddmmBackward0>) tensor(-0.3882)\n",
      "tensor([[-0.4215]], grad_fn=<AddmmBackward0>) tensor(-0.3759)\n",
      "tensor([[0.5657]], grad_fn=<AddmmBackward0>) tensor(0.5249)\n",
      "tensor([[-0.9625]], grad_fn=<AddmmBackward0>) tensor(-0.8663)\n",
      "tensor([[-0.8829]], grad_fn=<AddmmBackward0>) tensor(-0.7931)\n",
      "tensor([[0.4724]], grad_fn=<AddmmBackward0>) tensor(0.7408)\n",
      "tensor([[-1.2534]], grad_fn=<AddmmBackward0>) tensor(-0.7302)\n",
      "tensor([[-0.7507]], grad_fn=<AddmmBackward0>) tensor(-0.9207)\n",
      "tensor([[-0.4371]], grad_fn=<AddmmBackward0>) tensor(-0.6907)\n",
      "tensor([[-0.4817]], grad_fn=<AddmmBackward0>) tensor(-0.9397)\n",
      "tensor([[-0.3068]], grad_fn=<AddmmBackward0>) tensor(-0.1803)\n",
      "tensor([[-0.7811]], grad_fn=<AddmmBackward0>) tensor(-0.8618)\n",
      "tensor([[0.2834]], grad_fn=<AddmmBackward0>) tensor(0.5672)\n",
      "tensor([[-0.0283]], grad_fn=<AddmmBackward0>) tensor(0.3959)\n",
      "tensor([[-0.4313]], grad_fn=<AddmmBackward0>) tensor(-0.4661)\n",
      "tensor([[-0.0929]], grad_fn=<AddmmBackward0>) tensor(-0.2919)\n",
      "tensor([[-0.7334]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.1613]], grad_fn=<AddmmBackward0>) tensor(-0.8654)\n",
      "tensor([[-0.5430]], grad_fn=<AddmmBackward0>) tensor(-0.2519)\n",
      "tensor([[-0.2632]], grad_fn=<AddmmBackward0>) tensor(-0.1877)\n",
      "tensor([[-0.6861]], grad_fn=<AddmmBackward0>) tensor(-0.4474)\n",
      "tensor([[0.2033]], grad_fn=<AddmmBackward0>) tensor(0.5559)\n",
      "tensor([[-0.8484]], grad_fn=<AddmmBackward0>) tensor(-0.4839)\n",
      "tensor([[-0.6176]], grad_fn=<AddmmBackward0>) tensor(-0.3713)\n",
      "tensor([[0.1962]], grad_fn=<AddmmBackward0>) tensor(0.3418)\n",
      "tensor([[-0.2254]], grad_fn=<AddmmBackward0>) tensor(-0.2628)\n",
      "tensor([[0.2606]], grad_fn=<AddmmBackward0>) tensor(0.6163)\n",
      "tensor([[-0.7540]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.5237]], grad_fn=<AddmmBackward0>) tensor(-0.3896)\n",
      "tensor([[-0.7601]], grad_fn=<AddmmBackward0>) tensor(-0.8307)\n",
      "tensor([[-0.2303]], grad_fn=<AddmmBackward0>) tensor(-0.2077)\n",
      "tensor([[0.5069]], grad_fn=<AddmmBackward0>) tensor(0.7507)\n",
      "tensor([[-0.7979]], grad_fn=<AddmmBackward0>) tensor(-0.3970)\n",
      "tensor([[-0.0853]], grad_fn=<AddmmBackward0>) tensor(0.5151)\n",
      "tensor([[-0.5226]], grad_fn=<AddmmBackward0>) tensor(-0.5246)\n",
      "tensor([[0.6933]], grad_fn=<AddmmBackward0>) tensor(0.6023)\n",
      "tensor([[-1.1376]], grad_fn=<AddmmBackward0>) tensor(-0.6356)\n",
      "tensor([[-0.6281]], grad_fn=<AddmmBackward0>) tensor(-0.5009)\n",
      "tensor([[-0.2363]], grad_fn=<AddmmBackward0>) tensor(-0.5111)\n",
      "tensor([[0.4293]], grad_fn=<AddmmBackward0>) tensor(0.7142)\n",
      "tensor([[-0.3390]], grad_fn=<AddmmBackward0>) tensor(-0.2970)\n",
      "tensor([[0.8020]], grad_fn=<AddmmBackward0>) tensor(0.7406)\n",
      "tensor([[-0.0254]], grad_fn=<AddmmBackward0>) tensor(0.1524)\n",
      "tensor([[-0.0955]], grad_fn=<AddmmBackward0>) tensor(0.2058)\n",
      "tensor([[-0.5721]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.0807]], grad_fn=<AddmmBackward0>) tensor(-0.0005)\n",
      "tensor([[-0.8164]], grad_fn=<AddmmBackward0>) tensor(-0.7968)\n",
      "tensor([[-0.3090]], grad_fn=<AddmmBackward0>) tensor(-0.1972)\n",
      "tensor([[0.5183]], grad_fn=<AddmmBackward0>) tensor(0.5298)\n",
      "tensor([[0.0018]], grad_fn=<AddmmBackward0>) tensor(0.0923)\n",
      "tensor([[-0.5880]], grad_fn=<AddmmBackward0>) tensor(-0.5037)\n",
      "tensor([[-0.0062]], grad_fn=<AddmmBackward0>) tensor(-0.2902)\n",
      "tensor([[-0.8332]], grad_fn=<AddmmBackward0>) tensor(-0.8482)\n",
      "tensor([[0.9055]], grad_fn=<AddmmBackward0>) tensor(0.5301)\n",
      "tensor([[-0.5260]], grad_fn=<AddmmBackward0>) tensor(-0.7944)\n",
      "tensor([[0.6439]], grad_fn=<AddmmBackward0>) tensor(0.6697)\n",
      "tensor([[-0.6089]], grad_fn=<AddmmBackward0>) tensor(-0.5780)\n",
      "tensor([[0.7245]], grad_fn=<AddmmBackward0>) tensor(0.7214)\n",
      "tensor([[-0.6796]], grad_fn=<AddmmBackward0>) tensor(-0.5797)\n",
      "tensor([[-0.7292]], grad_fn=<AddmmBackward0>) tensor(0.5021)\n",
      "tensor([[-0.3866]], grad_fn=<AddmmBackward0>) tensor(-0.4958)\n",
      "tensor([[0.3994]], grad_fn=<AddmmBackward0>) tensor(0.5726)\n",
      "tensor([[0.1346]], grad_fn=<AddmmBackward0>) tensor(0.6835)\n",
      "tensor([[0.4907]], grad_fn=<AddmmBackward0>) tensor(0.5915)\n",
      "tensor([[-0.4834]], grad_fn=<AddmmBackward0>) tensor(-0.3118)\n",
      "tensor([[-0.0930]], grad_fn=<AddmmBackward0>) tensor(-0.2189)\n",
      "tensor([[-0.1163]], grad_fn=<AddmmBackward0>) tensor(0.3183)\n",
      "tensor([[-0.0154]], grad_fn=<AddmmBackward0>) tensor(0.1581)\n",
      "tensor([[-0.3758]], grad_fn=<AddmmBackward0>) tensor(-0.1566)\n",
      "tensor([[-0.6143]], grad_fn=<AddmmBackward0>) tensor(-0.6563)\n",
      "tensor([[0.2895]], grad_fn=<AddmmBackward0>) tensor(0.7195)\n",
      "tensor([[-0.5683]], grad_fn=<AddmmBackward0>) tensor(-0.4890)\n",
      "tensor([[-0.2614]], grad_fn=<AddmmBackward0>) tensor(0.5734)\n",
      "tensor([[-0.4576]], grad_fn=<AddmmBackward0>) tensor(-0.6304)\n",
      "tensor([[0.1919]], grad_fn=<AddmmBackward0>) tensor(0.7371)\n",
      "tensor([[0.4645]], grad_fn=<AddmmBackward0>) tensor(0.5392)\n",
      "tensor([[-0.5112]], grad_fn=<AddmmBackward0>) tensor(-0.7992)\n",
      "tensor([[0.6256]], grad_fn=<AddmmBackward0>) tensor(0.9029)\n",
      "tensor([[-0.1718]], grad_fn=<AddmmBackward0>) tensor(-0.1402)\n",
      "tensor([[-0.1186]], grad_fn=<AddmmBackward0>) tensor(-0.3096)\n",
      "tensor([[-0.5239]], grad_fn=<AddmmBackward0>) tensor(-0.0620)\n",
      "tensor([[-1.2031]], grad_fn=<AddmmBackward0>) tensor(-0.7342)\n",
      "tensor([[-0.3011]], grad_fn=<AddmmBackward0>) tensor(0.3248)\n",
      "tensor([[-0.4774]], grad_fn=<AddmmBackward0>) tensor(-0.8703)\n",
      "tensor([[0.3310]], grad_fn=<AddmmBackward0>) tensor(0.5412)\n",
      "tensor([[0.2638]], grad_fn=<AddmmBackward0>) tensor(0.6403)\n",
      "tensor([[0.2111]], grad_fn=<AddmmBackward0>) tensor(0.3855)\n",
      "tensor([[0.1017]], grad_fn=<AddmmBackward0>) tensor(0.6586)\n",
      "tensor([[-0.4445]], grad_fn=<AddmmBackward0>) tensor(-0.6064)\n",
      "tensor([[-0.4100]], grad_fn=<AddmmBackward0>) tensor(-0.6071)\n",
      "tensor([[-0.7393]], grad_fn=<AddmmBackward0>) tensor(-0.6011)\n",
      "tensor([[-0.7159]], grad_fn=<AddmmBackward0>) tensor(-0.8628)\n",
      "tensor([[-0.6914]], grad_fn=<AddmmBackward0>) tensor(-0.4758)\n",
      "tensor([[-0.7284]], grad_fn=<AddmmBackward0>) tensor(-0.7268)\n",
      "tensor([[-0.6267]], grad_fn=<AddmmBackward0>) tensor(-0.8043)\n",
      "tensor([[-0.6970]], grad_fn=<AddmmBackward0>) tensor(-0.8839)\n",
      "tensor([[-0.6556]], grad_fn=<AddmmBackward0>) tensor(-0.8718)\n",
      "tensor([[0.4572]], grad_fn=<AddmmBackward0>) tensor(0.0429)\n",
      "tensor([[-1.2506]], grad_fn=<AddmmBackward0>) tensor(-0.4635)\n",
      "tensor([[-0.5893]], grad_fn=<AddmmBackward0>) tensor(-0.7685)\n",
      "tensor([[-0.6687]], grad_fn=<AddmmBackward0>) tensor(-0.7180)\n",
      "tensor([[-0.4364]], grad_fn=<AddmmBackward0>) tensor(-0.9196)\n",
      "tensor([[0.5054]], grad_fn=<AddmmBackward0>) tensor(0.7978)\n",
      "tensor([[-0.0481]], grad_fn=<AddmmBackward0>) tensor(-0.1130)\n",
      "tensor([[0.3817]], grad_fn=<AddmmBackward0>) tensor(0.8095)\n",
      "tensor([[0.7137]], grad_fn=<AddmmBackward0>) tensor(0.5738)\n",
      "tensor([[-0.3188]], grad_fn=<AddmmBackward0>) tensor(-0.2532)\n",
      "tensor([[-0.0981]], grad_fn=<AddmmBackward0>) tensor(-0.0241)\n",
      "tensor([[0.5782]], grad_fn=<AddmmBackward0>) tensor(0.8282)\n",
      "tensor([[0.1013]], grad_fn=<AddmmBackward0>) tensor(-0.0357)\n",
      "tensor([[-0.5905]], grad_fn=<AddmmBackward0>) tensor(0.9771)\n",
      "tensor([[0.0460]], grad_fn=<AddmmBackward0>) tensor(0.2045)\n",
      "tensor([[-0.3514]], grad_fn=<AddmmBackward0>) tensor(0.0048)\n",
      "tensor([[-0.5778]], grad_fn=<AddmmBackward0>) tensor(-0.4821)\n",
      "tensor([[-0.0283]], grad_fn=<AddmmBackward0>) tensor(0.0246)\n",
      "tensor([[-0.6274]], grad_fn=<AddmmBackward0>) tensor(-0.2917)\n",
      "tensor([[-0.4701]], grad_fn=<AddmmBackward0>) tensor(-0.7361)\n",
      "tensor([[0.5242]], grad_fn=<AddmmBackward0>) tensor(0.8027)\n",
      "tensor([[-0.8976]], grad_fn=<AddmmBackward0>) tensor(-0.7441)\n",
      "tensor([[0.2997]], grad_fn=<AddmmBackward0>) tensor(0.7300)\n",
      "tensor([[0.5427]], grad_fn=<AddmmBackward0>) tensor(0.7011)\n",
      "tensor([[-0.6353]], grad_fn=<AddmmBackward0>) tensor(-0.7071)\n",
      "tensor([[-0.6984]], grad_fn=<AddmmBackward0>) tensor(-0.6044)\n",
      "tensor([[-0.1223]], grad_fn=<AddmmBackward0>) tensor(0.0541)\n",
      "tensor([[-0.5810]], grad_fn=<AddmmBackward0>) tensor(-0.9473)\n",
      "tensor([[-0.0393]], grad_fn=<AddmmBackward0>) tensor(-0.2468)\n",
      "tensor([[-0.6675]], grad_fn=<AddmmBackward0>) tensor(-0.8388)\n",
      "tensor([[-0.0295]], grad_fn=<AddmmBackward0>) tensor(0.4301)\n",
      "tensor([[0.4529]], grad_fn=<AddmmBackward0>) tensor(0.6618)\n",
      "tensor([[-0.7261]], grad_fn=<AddmmBackward0>) tensor(-0.8331)\n",
      "tensor([[0.5734]], grad_fn=<AddmmBackward0>) tensor(0.5919)\n",
      "tensor([[-0.6710]], grad_fn=<AddmmBackward0>) tensor(-0.6748)\n",
      "tensor([[0.1125]], grad_fn=<AddmmBackward0>) tensor(0.0047)\n",
      "tensor([[-0.1677]], grad_fn=<AddmmBackward0>) tensor(0.4414)\n",
      "tensor([[-0.5677]], grad_fn=<AddmmBackward0>) tensor(-0.7764)\n",
      "tensor([[0.2061]], grad_fn=<AddmmBackward0>) tensor(0.5770)\n",
      "tensor([[-0.6008]], grad_fn=<AddmmBackward0>) tensor(-0.3517)\n",
      "tensor([[0.8895]], grad_fn=<AddmmBackward0>) tensor(0.5809)\n",
      "tensor([[0.3995]], grad_fn=<AddmmBackward0>) tensor(0.8221)\n",
      "tensor([[-0.4046]], grad_fn=<AddmmBackward0>) tensor(-0.5329)\n",
      "tensor([[-0.5087]], grad_fn=<AddmmBackward0>) tensor(-0.2558)\n",
      "tensor([[-0.4372]], grad_fn=<AddmmBackward0>) tensor(-0.4686)\n",
      "tensor([[-0.3507]], grad_fn=<AddmmBackward0>) tensor(-0.4755)\n",
      "tensor([[0.3111]], grad_fn=<AddmmBackward0>) tensor(0.8313)\n",
      "tensor([[-0.6876]], grad_fn=<AddmmBackward0>) tensor(-0.9353)\n",
      "tensor([[0.1932]], grad_fn=<AddmmBackward0>) tensor(0.3048)\n",
      "tensor([[-0.7541]], grad_fn=<AddmmBackward0>) tensor(-0.7896)\n",
      "tensor([[0.0002]], grad_fn=<AddmmBackward0>) tensor(-0.0010)\n",
      "tensor([[-0.3226]], grad_fn=<AddmmBackward0>) tensor(0.0113)\n",
      "tensor([[0.0617]], grad_fn=<AddmmBackward0>) tensor(0.3861)\n",
      "tensor([[-0.8652]], grad_fn=<AddmmBackward0>) tensor(-0.7619)\n",
      "tensor([[-0.5632]], grad_fn=<AddmmBackward0>) tensor(-0.5508)\n",
      "tensor([[-0.1898]], grad_fn=<AddmmBackward0>) tensor(0.0362)\n",
      "tensor([[-0.7224]], grad_fn=<AddmmBackward0>) tensor(-0.5969)\n",
      "tensor([[0.0908]], grad_fn=<AddmmBackward0>) tensor(0.0183)\n",
      "tensor([[-0.6362]], grad_fn=<AddmmBackward0>) tensor(-0.6256)\n",
      "tensor([[0.4588]], grad_fn=<AddmmBackward0>) tensor(0.5683)\n",
      "tensor([[-0.8518]], grad_fn=<AddmmBackward0>) tensor(-0.9676)\n",
      "tensor([[0.4783]], grad_fn=<AddmmBackward0>) tensor(0.5742)\n",
      "tensor([[0.0572]], grad_fn=<AddmmBackward0>) tensor(-0.0549)\n",
      "tensor([[0.5533]], grad_fn=<AddmmBackward0>) tensor(0.5155)\n",
      "tensor([[0.1318]], grad_fn=<AddmmBackward0>) tensor(0.1280)\n",
      "tensor([[-0.7487]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.4825]], grad_fn=<AddmmBackward0>) tensor(0.7950)\n",
      "tensor([[-0.3039]], grad_fn=<AddmmBackward0>) tensor(0.6186)\n",
      "tensor([[-0.6797]], grad_fn=<AddmmBackward0>) tensor(-0.5146)\n",
      "tensor([[0.3779]], grad_fn=<AddmmBackward0>) tensor(0.5420)\n",
      "tensor([[-0.5107]], grad_fn=<AddmmBackward0>) tensor(-0.5866)\n",
      "tensor([[0.6873]], grad_fn=<AddmmBackward0>) tensor(0.5558)\n",
      "tensor([[-0.1217]], grad_fn=<AddmmBackward0>) tensor(-0.0853)\n",
      "tensor([[-0.3103]], grad_fn=<AddmmBackward0>) tensor(-0.1293)\n",
      "tensor([[-1.0981]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.4199]], grad_fn=<AddmmBackward0>) tensor(-0.2696)\n",
      "tensor([[-0.7766]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.6823]], grad_fn=<AddmmBackward0>) tensor(0.7364)\n",
      "tensor([[0.3505]], grad_fn=<AddmmBackward0>) tensor(0.5470)\n",
      "tensor([[-0.3020]], grad_fn=<AddmmBackward0>) tensor(-0.2780)\n",
      "tensor([[-0.7551]], grad_fn=<AddmmBackward0>) tensor(-0.8332)\n",
      "tensor([[-0.7939]], grad_fn=<AddmmBackward0>) tensor(-0.7215)\n",
      "tensor([[0.0262]], grad_fn=<AddmmBackward0>) tensor(0.6414)\n",
      "tensor([[-0.6215]], grad_fn=<AddmmBackward0>) tensor(-0.4721)\n",
      "tensor([[0.0176]], grad_fn=<AddmmBackward0>) tensor(0.2142)\n",
      "tensor([[-0.7573]], grad_fn=<AddmmBackward0>) tensor(-0.7723)\n",
      "tensor([[-0.6989]], grad_fn=<AddmmBackward0>) tensor(-0.9097)\n",
      "tensor([[-0.7905]], grad_fn=<AddmmBackward0>) tensor(-0.6677)\n",
      "tensor([[-0.8365]], grad_fn=<AddmmBackward0>) tensor(-0.8116)\n",
      "tensor([[0.7265]], grad_fn=<AddmmBackward0>) tensor(0.5193)\n",
      "tensor([[0.4581]], grad_fn=<AddmmBackward0>) tensor(0.6289)\n",
      "tensor([[-0.6422]], grad_fn=<AddmmBackward0>) tensor(-0.7691)\n",
      "tensor([[0.4461]], grad_fn=<AddmmBackward0>) tensor(0.8327)\n",
      "tensor([[0.3749]], grad_fn=<AddmmBackward0>) tensor(0.5151)\n",
      "tensor([[0.2663]], grad_fn=<AddmmBackward0>) tensor(0.7926)\n",
      "tensor([[0.1356]], grad_fn=<AddmmBackward0>) tensor(0.1362)\n",
      "tensor([[-0.4784]], grad_fn=<AddmmBackward0>) tensor(-0.5682)\n",
      "tensor([[-0.9809]], grad_fn=<AddmmBackward0>) tensor(-0.3925)\n",
      "tensor([[0.0682]], grad_fn=<AddmmBackward0>) tensor(0.1938)\n",
      "tensor([[-0.2646]], grad_fn=<AddmmBackward0>) tensor(-0.1276)\n",
      "tensor([[-0.2114]], grad_fn=<AddmmBackward0>) tensor(-0.2993)\n",
      "tensor([[-0.4989]], grad_fn=<AddmmBackward0>) tensor(-0.8785)\n",
      "tensor([[-0.7575]], grad_fn=<AddmmBackward0>) tensor(-0.6821)\n",
      "tensor([[-0.7202]], grad_fn=<AddmmBackward0>) tensor(-0.5336)\n",
      "tensor([[-0.0349]], grad_fn=<AddmmBackward0>) tensor(-0.1030)\n",
      "tensor([[-0.6655]], grad_fn=<AddmmBackward0>) tensor(-0.9105)\n",
      "tensor([[0.1858]], grad_fn=<AddmmBackward0>) tensor(0.6497)\n",
      "tensor([[0.6800]], grad_fn=<AddmmBackward0>) tensor(0.4654)\n",
      "tensor([[-0.3774]], grad_fn=<AddmmBackward0>) tensor(-0.5021)\n",
      "tensor([[0.7738]], grad_fn=<AddmmBackward0>) tensor(0.8435)\n",
      "tensor([[-0.7607]], grad_fn=<AddmmBackward0>) tensor(-0.6896)\n",
      "tensor([[0.3086]], grad_fn=<AddmmBackward0>) tensor(0.5407)\n",
      "tensor([[-0.0799]], grad_fn=<AddmmBackward0>) tensor(0.2073)\n",
      "tensor([[-0.6247]], grad_fn=<AddmmBackward0>) tensor(-0.4162)\n",
      "tensor([[-0.4782]], grad_fn=<AddmmBackward0>) tensor(-0.1648)\n",
      "tensor([[0.5978]], grad_fn=<AddmmBackward0>) tensor(0.5291)\n",
      "tensor([[-0.7668]], grad_fn=<AddmmBackward0>) tensor(-0.8610)\n",
      "tensor([[-0.5821]], grad_fn=<AddmmBackward0>) tensor(-0.6156)\n",
      "tensor([[-0.5999]], grad_fn=<AddmmBackward0>) tensor(-0.6510)\n",
      "tensor([[-0.5708]], grad_fn=<AddmmBackward0>) tensor(-0.5336)\n",
      "tensor([[-0.6813]], grad_fn=<AddmmBackward0>) tensor(-0.8884)\n",
      "tensor([[-0.7284]], grad_fn=<AddmmBackward0>) tensor(-0.7990)\n",
      "tensor([[0.5934]], grad_fn=<AddmmBackward0>) tensor(0.7886)\n",
      "tensor([[-0.8391]], grad_fn=<AddmmBackward0>) tensor(-0.8474)\n",
      "tensor([[0.8664]], grad_fn=<AddmmBackward0>) tensor(0.5378)\n",
      "tensor([[-0.1045]], grad_fn=<AddmmBackward0>) tensor(-0.1147)\n",
      "tensor([[-0.3885]], grad_fn=<AddmmBackward0>) tensor(-0.8889)\n",
      "tensor([[-0.5057]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.4451]], grad_fn=<AddmmBackward0>) tensor(-0.2733)\n",
      "tensor([[0.5082]], grad_fn=<AddmmBackward0>) tensor(0.7587)\n",
      "tensor([[-0.7599]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.7253]], grad_fn=<AddmmBackward0>) tensor(-0.8772)\n",
      "tensor([[-0.1912]], grad_fn=<AddmmBackward0>) tensor(-0.2394)\n",
      "tensor([[0.0918]], grad_fn=<AddmmBackward0>) tensor(0.3613)\n",
      "tensor([[-0.4422]], grad_fn=<AddmmBackward0>) tensor(-0.2933)\n",
      "tensor([[-0.5375]], grad_fn=<AddmmBackward0>) tensor(-0.7631)\n",
      "tensor([[0.6503]], grad_fn=<AddmmBackward0>) tensor(0.5327)\n",
      "tensor([[-0.6791]], grad_fn=<AddmmBackward0>) tensor(-0.8519)\n",
      "tensor([[-0.1189]], grad_fn=<AddmmBackward0>) tensor(-0.2814)\n",
      "tensor([[-0.6563]], grad_fn=<AddmmBackward0>) tensor(-0.6542)\n",
      "tensor([[-1.4311]], grad_fn=<AddmmBackward0>) tensor(-0.9608)\n",
      "tensor([[0.4224]], grad_fn=<AddmmBackward0>) tensor(0.3844)\n",
      "tensor([[-0.7970]], grad_fn=<AddmmBackward0>) tensor(-0.8334)\n",
      "tensor([[0.0385]], grad_fn=<AddmmBackward0>) tensor(0.8516)\n",
      "tensor([[-0.9462]], grad_fn=<AddmmBackward0>) tensor(-0.8906)\n",
      "tensor([[0.6553]], grad_fn=<AddmmBackward0>) tensor(0.7400)\n",
      "tensor([[-0.4509]], grad_fn=<AddmmBackward0>) tensor(-0.9471)\n",
      "tensor([[0.5068]], grad_fn=<AddmmBackward0>) tensor(0.5643)\n",
      "tensor([[-0.1722]], grad_fn=<AddmmBackward0>) tensor(0.4954)\n",
      "tensor([[-0.9228]], grad_fn=<AddmmBackward0>) tensor(-0.8079)\n",
      "tensor([[-0.2947]], grad_fn=<AddmmBackward0>) tensor(-0.9292)\n",
      "tensor([[-0.3125]], grad_fn=<AddmmBackward0>) tensor(-0.5428)\n",
      "tensor([[0.2023]], grad_fn=<AddmmBackward0>) tensor(0.4521)\n",
      "tensor([[-0.4494]], grad_fn=<AddmmBackward0>) tensor(-0.6421)\n",
      "tensor([[-0.2400]], grad_fn=<AddmmBackward0>) tensor(0.0199)\n",
      "tensor([[0.1922]], grad_fn=<AddmmBackward0>) tensor(0.5760)\n",
      "tensor([[0.4532]], grad_fn=<AddmmBackward0>) tensor(0.6236)\n",
      "tensor([[-0.6942]], grad_fn=<AddmmBackward0>) tensor(-0.9153)\n",
      "tensor([[0.4682]], grad_fn=<AddmmBackward0>) tensor(0.5444)\n",
      "tensor([[-0.5278]], grad_fn=<AddmmBackward0>) tensor(-0.4521)\n",
      "tensor([[-0.2773]], grad_fn=<AddmmBackward0>) tensor(0.1284)\n",
      "tensor([[-0.4453]], grad_fn=<AddmmBackward0>) tensor(-0.3726)\n",
      "tensor([[-0.1541]], grad_fn=<AddmmBackward0>) tensor(-0.2529)\n",
      "tensor([[0.0890]], grad_fn=<AddmmBackward0>) tensor(0.2925)\n",
      "tensor([[-0.6585]], grad_fn=<AddmmBackward0>) tensor(-0.6835)\n",
      "tensor([[-0.7058]], grad_fn=<AddmmBackward0>) tensor(-0.8647)\n",
      "tensor([[-0.3202]], grad_fn=<AddmmBackward0>) tensor(-0.4056)\n",
      "tensor([[-0.8446]], grad_fn=<AddmmBackward0>) tensor(-0.8343)\n",
      "tensor([[0.4890]], grad_fn=<AddmmBackward0>) tensor(0.5841)\n",
      "tensor([[0.2347]], grad_fn=<AddmmBackward0>) tensor(0.3494)\n",
      "tensor([[0.0039]], grad_fn=<AddmmBackward0>) tensor(-0.2918)\n",
      "tensor([[0.2719]], grad_fn=<AddmmBackward0>) tensor(0.6497)\n",
      "tensor([[-1.1597]], grad_fn=<AddmmBackward0>) tensor(-0.8816)\n",
      "tensor([[-0.0810]], grad_fn=<AddmmBackward0>) tensor(0.2352)\n",
      "tensor([[0.1639]], grad_fn=<AddmmBackward0>) tensor(0.6241)\n",
      "tensor([[-0.3984]], grad_fn=<AddmmBackward0>) tensor(-0.4007)\n",
      "tensor([[0.7997]], grad_fn=<AddmmBackward0>) tensor(0.5540)\n",
      "tensor([[-0.7618]], grad_fn=<AddmmBackward0>) tensor(-0.4377)\n",
      "tensor([[-0.7927]], grad_fn=<AddmmBackward0>) tensor(-0.8692)\n",
      "tensor([[-0.4787]], grad_fn=<AddmmBackward0>) tensor(0.8398)\n",
      "tensor([[0.2797]], grad_fn=<AddmmBackward0>) tensor(0.6188)\n",
      "tensor([[-0.5236]], grad_fn=<AddmmBackward0>) tensor(-0.8544)\n",
      "tensor([[-0.7241]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.4335]], grad_fn=<AddmmBackward0>) tensor(0.5649)\n",
      "tensor([[-0.5495]], grad_fn=<AddmmBackward0>) tensor(-0.2791)\n",
      "tensor([[-0.4012]], grad_fn=<AddmmBackward0>) tensor(0.3491)\n",
      "tensor([[-0.6877]], grad_fn=<AddmmBackward0>) tensor(-0.8872)\n",
      "tensor([[0.3278]], grad_fn=<AddmmBackward0>) tensor(0.6475)\n",
      "tensor([[-0.6157]], grad_fn=<AddmmBackward0>) tensor(-0.6752)\n",
      "tensor([[-0.7458]], grad_fn=<AddmmBackward0>) tensor(-0.8508)\n",
      "tensor([[-0.4843]], grad_fn=<AddmmBackward0>) tensor(-0.3510)\n",
      "tensor([[0.2256]], grad_fn=<AddmmBackward0>) tensor(0.5463)\n",
      "tensor([[-0.4751]], grad_fn=<AddmmBackward0>) tensor(0.4392)\n",
      "tensor([[0.4378]], grad_fn=<AddmmBackward0>) tensor(0.5497)\n",
      "tensor([[-0.7255]], grad_fn=<AddmmBackward0>) tensor(-0.4392)\n",
      "tensor([[0.0478]], grad_fn=<AddmmBackward0>) tensor(0.1280)\n",
      "tensor([[0.0609]], grad_fn=<AddmmBackward0>) tensor(0.8263)\n",
      "tensor([[0.0500]], grad_fn=<AddmmBackward0>) tensor(-0.1179)\n",
      "tensor([[-0.0698]], grad_fn=<AddmmBackward0>) tensor(0.7350)\n",
      "tensor([[-0.1825]], grad_fn=<AddmmBackward0>) tensor(0.3422)\n",
      "tensor([[-0.2741]], grad_fn=<AddmmBackward0>) tensor(0.1315)\n",
      "tensor([[-0.4108]], grad_fn=<AddmmBackward0>) tensor(-0.6014)\n",
      "tensor([[0.8063]], grad_fn=<AddmmBackward0>) tensor(0.5819)\n",
      "tensor([[-0.7878]], grad_fn=<AddmmBackward0>) tensor(-0.2462)\n",
      "tensor([[-0.4691]], grad_fn=<AddmmBackward0>) tensor(-0.5075)\n",
      "tensor([[-0.1458]], grad_fn=<AddmmBackward0>) tensor(-0.2900)\n",
      "tensor([[-0.5221]], grad_fn=<AddmmBackward0>) tensor(-0.4278)\n",
      "tensor([[-0.2242]], grad_fn=<AddmmBackward0>) tensor(-0.4555)\n",
      "tensor([[0.5105]], grad_fn=<AddmmBackward0>) tensor(0.7896)\n",
      "tensor([[-0.4896]], grad_fn=<AddmmBackward0>) tensor(-0.4412)\n",
      "tensor([[-0.5879]], grad_fn=<AddmmBackward0>) tensor(-0.5380)\n",
      "tensor([[0.5310]], grad_fn=<AddmmBackward0>) tensor(0.6108)\n",
      "tensor([[-0.3022]], grad_fn=<AddmmBackward0>) tensor(0.2847)\n",
      "tensor([[0.2845]], grad_fn=<AddmmBackward0>) tensor(0.5664)\n",
      "tensor([[-0.6831]], grad_fn=<AddmmBackward0>) tensor(-0.6700)\n",
      "tensor([[0.4810]], grad_fn=<AddmmBackward0>) tensor(0.5287)\n",
      "tensor([[-0.7583]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.6189]], grad_fn=<AddmmBackward0>) tensor(-0.3214)\n",
      "tensor([[-0.4799]], grad_fn=<AddmmBackward0>) tensor(-0.4908)\n",
      "tensor([[-0.4144]], grad_fn=<AddmmBackward0>) tensor(-0.9032)\n",
      "tensor([[-0.5732]], grad_fn=<AddmmBackward0>) tensor(-0.3569)\n",
      "tensor([[-0.2929]], grad_fn=<AddmmBackward0>) tensor(-0.2409)\n",
      "tensor([[0.6549]], grad_fn=<AddmmBackward0>) tensor(0.6019)\n",
      "tensor([[0.4339]], grad_fn=<AddmmBackward0>) tensor(0.6247)\n",
      "tensor([[-0.0041]], grad_fn=<AddmmBackward0>) tensor(-0.0205)\n",
      "tensor([[-0.5320]], grad_fn=<AddmmBackward0>) tensor(-0.7087)\n",
      "tensor([[-0.1466]], grad_fn=<AddmmBackward0>) tensor(0.8697)\n",
      "tensor([[-0.5258]], grad_fn=<AddmmBackward0>) tensor(0.4467)\n",
      "tensor([[0.0715]], grad_fn=<AddmmBackward0>) tensor(0.6319)\n",
      "tensor([[0.2044]], grad_fn=<AddmmBackward0>) tensor(0.7991)\n",
      "tensor([[-0.5676]], grad_fn=<AddmmBackward0>) tensor(-0.6265)\n",
      "tensor([[0.4351]], grad_fn=<AddmmBackward0>) tensor(0.5084)\n",
      "tensor([[0.3864]], grad_fn=<AddmmBackward0>) tensor(0.5561)\n",
      "tensor([[-0.4621]], grad_fn=<AddmmBackward0>) tensor(-0.3511)\n",
      "tensor([[-0.6564]], grad_fn=<AddmmBackward0>) tensor(-0.5771)\n",
      "tensor([[-0.4880]], grad_fn=<AddmmBackward0>) tensor(-0.5503)\n",
      "tensor([[-0.2286]], grad_fn=<AddmmBackward0>) tensor(0.3474)\n",
      "tensor([[-0.9734]], grad_fn=<AddmmBackward0>) tensor(-0.8741)\n",
      "tensor([[-0.3116]], grad_fn=<AddmmBackward0>) tensor(0.0356)\n",
      "tensor([[0.1359]], grad_fn=<AddmmBackward0>) tensor(0.5780)\n",
      "tensor([[-0.6668]], grad_fn=<AddmmBackward0>) tensor(-0.7795)\n",
      "tensor([[0.4019]], grad_fn=<AddmmBackward0>) tensor(0.4568)\n",
      "tensor([[-0.1730]], grad_fn=<AddmmBackward0>) tensor(0.2973)\n",
      "tensor([[0.1089]], grad_fn=<AddmmBackward0>) tensor(0.3102)\n",
      "tensor([[-0.6879]], grad_fn=<AddmmBackward0>) tensor(-0.4798)\n",
      "tensor([[0.1842]], grad_fn=<AddmmBackward0>) tensor(0.6873)\n",
      "tensor([[0.1559]], grad_fn=<AddmmBackward0>) tensor(0.5302)\n",
      "tensor([[-0.3844]], grad_fn=<AddmmBackward0>) tensor(-0.8875)\n",
      "tensor([[-0.6307]], grad_fn=<AddmmBackward0>) tensor(-0.4410)\n",
      "tensor([[0.3852]], grad_fn=<AddmmBackward0>) tensor(0.7575)\n",
      "tensor([[0.2039]], grad_fn=<AddmmBackward0>) tensor(0.6290)\n",
      "tensor([[-0.7252]], grad_fn=<AddmmBackward0>) tensor(-0.5780)\n",
      "tensor([[-0.3273]], grad_fn=<AddmmBackward0>) tensor(-0.5083)\n",
      "tensor([[0.5403]], grad_fn=<AddmmBackward0>) tensor(0.6931)\n",
      "tensor([[-0.5457]], grad_fn=<AddmmBackward0>) tensor(-0.3849)\n",
      "tensor([[-0.1718]], grad_fn=<AddmmBackward0>) tensor(0.6171)\n",
      "tensor([[-0.3545]], grad_fn=<AddmmBackward0>) tensor(0.0069)\n",
      "tensor([[-0.9611]], grad_fn=<AddmmBackward0>) tensor(-0.8631)\n",
      "tensor([[-0.5386]], grad_fn=<AddmmBackward0>) tensor(-0.4872)\n",
      "tensor([[-0.9484]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.2338]], grad_fn=<AddmmBackward0>) tensor(-0.3251)\n",
      "tensor([[-0.6581]], grad_fn=<AddmmBackward0>) tensor(-0.4397)\n",
      "tensor([[0.1758]], grad_fn=<AddmmBackward0>) tensor(0.6151)\n",
      "tensor([[0.0093]], grad_fn=<AddmmBackward0>) tensor(0.5704)\n",
      "tensor([[-0.2411]], grad_fn=<AddmmBackward0>) tensor(-0.3589)\n",
      "tensor([[-0.8713]], grad_fn=<AddmmBackward0>) tensor(-0.3413)\n",
      "tensor([[-0.7657]], grad_fn=<AddmmBackward0>) tensor(-0.4821)\n",
      "tensor([[0.4360]], grad_fn=<AddmmBackward0>) tensor(0.5508)\n",
      "tensor([[-0.0763]], grad_fn=<AddmmBackward0>) tensor(0.0355)\n",
      "tensor([[-0.7849]], grad_fn=<AddmmBackward0>) tensor(-0.6137)\n",
      "tensor([[-0.4915]], grad_fn=<AddmmBackward0>) tensor(-0.3663)\n",
      "tensor([[-0.1455]], grad_fn=<AddmmBackward0>) tensor(-0.2626)\n",
      "tensor([[0.7905]], grad_fn=<AddmmBackward0>) tensor(0.5509)\n",
      "tensor([[0.5130]], grad_fn=<AddmmBackward0>) tensor(0.8059)\n",
      "tensor([[-0.2369]], grad_fn=<AddmmBackward0>) tensor(0.7558)\n",
      "tensor([[-0.5562]], grad_fn=<AddmmBackward0>) tensor(-0.9546)\n",
      "tensor([[-0.6937]], grad_fn=<AddmmBackward0>) tensor(-0.8999)\n",
      "tensor([[-0.6616]], grad_fn=<AddmmBackward0>) tensor(-0.1093)\n",
      "tensor([[0.2825]], grad_fn=<AddmmBackward0>) tensor(0.1344)\n",
      "tensor([[-0.4646]], grad_fn=<AddmmBackward0>) tensor(-0.2822)\n",
      "tensor([[-0.5935]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.2752]], grad_fn=<AddmmBackward0>) tensor(-0.1712)\n",
      "tensor([[-0.5902]], grad_fn=<AddmmBackward0>) tensor(-0.3496)\n",
      "tensor([[0.0854]], grad_fn=<AddmmBackward0>) tensor(0.5509)\n",
      "tensor([[-0.3423]], grad_fn=<AddmmBackward0>) tensor(-0.1121)\n",
      "tensor([[-1.0454]], grad_fn=<AddmmBackward0>) tensor(-0.9364)\n",
      "tensor([[0.1109]], grad_fn=<AddmmBackward0>) tensor(-0.0516)\n",
      "tensor([[-0.3453]], grad_fn=<AddmmBackward0>) tensor(-0.8388)\n",
      "tensor([[-0.7011]], grad_fn=<AddmmBackward0>) tensor(-0.4694)\n",
      "tensor([[-0.4784]], grad_fn=<AddmmBackward0>) tensor(-0.5950)\n",
      "tensor([[-0.3118]], grad_fn=<AddmmBackward0>) tensor(-0.2948)\n",
      "tensor([[0.6414]], grad_fn=<AddmmBackward0>) tensor(0.5094)\n",
      "tensor([[0.2304]], grad_fn=<AddmmBackward0>) tensor(0.5296)\n",
      "tensor([[-0.7216]], grad_fn=<AddmmBackward0>) tensor(-0.2926)\n",
      "tensor([[-0.7543]], grad_fn=<AddmmBackward0>) tensor(-0.8618)\n",
      "tensor([[-0.7305]], grad_fn=<AddmmBackward0>) tensor(-0.8632)\n",
      "tensor([[-0.1912]], grad_fn=<AddmmBackward0>) tensor(-0.1347)\n",
      "tensor([[0.4139]], grad_fn=<AddmmBackward0>) tensor(0.8064)\n",
      "tensor([[0.3833]], grad_fn=<AddmmBackward0>) tensor(0.6162)\n",
      "tensor([[-0.6211]], grad_fn=<AddmmBackward0>) tensor(-0.4689)\n",
      "tensor([[-0.6312]], grad_fn=<AddmmBackward0>) tensor(-0.8805)\n",
      "tensor([[-0.5670]], grad_fn=<AddmmBackward0>) tensor(-0.4798)\n",
      "tensor([[-0.5932]], grad_fn=<AddmmBackward0>) tensor(-0.4589)\n",
      "tensor([[-1.0466]], grad_fn=<AddmmBackward0>) tensor(-0.4776)\n",
      "tensor([[-0.2873]], grad_fn=<AddmmBackward0>) tensor(-0.2985)\n",
      "tensor([[-0.5326]], grad_fn=<AddmmBackward0>) tensor(-0.4943)\n",
      "tensor([[-0.8566]], grad_fn=<AddmmBackward0>) tensor(-0.8765)\n",
      "tensor([[0.4039]], grad_fn=<AddmmBackward0>) tensor(0.5608)\n",
      "tensor([[-0.4454]], grad_fn=<AddmmBackward0>) tensor(0.3459)\n",
      "tensor([[-0.0677]], grad_fn=<AddmmBackward0>) tensor(-0.1385)\n",
      "tensor([[-0.7333]], grad_fn=<AddmmBackward0>) tensor(-0.5521)\n",
      "tensor([[-0.8292]], grad_fn=<AddmmBackward0>) tensor(0.8338)\n",
      "tensor([[0.2978]], grad_fn=<AddmmBackward0>) tensor(0.5127)\n",
      "tensor([[0.1798]], grad_fn=<AddmmBackward0>) tensor(0.1140)\n",
      "tensor([[0.1236]], grad_fn=<AddmmBackward0>) tensor(0.3734)\n",
      "tensor([[0.3827]], grad_fn=<AddmmBackward0>) tensor(0.5387)\n",
      "tensor([[0.4486]], grad_fn=<AddmmBackward0>) tensor(0.7958)\n",
      "tensor([[-0.4629]], grad_fn=<AddmmBackward0>) tensor(-0.3538)\n",
      "tensor([[-0.5445]], grad_fn=<AddmmBackward0>) tensor(-0.9030)\n",
      "tensor([[0.8454]], grad_fn=<AddmmBackward0>) tensor(0.5256)\n",
      "tensor([[0.3822]], grad_fn=<AddmmBackward0>) tensor(0.5685)\n",
      "tensor([[0.1697]], grad_fn=<AddmmBackward0>) tensor(0.5197)\n",
      "tensor([[0.3435]], grad_fn=<AddmmBackward0>) tensor(0.5707)\n",
      "tensor([[-0.9979]], grad_fn=<AddmmBackward0>) tensor(-0.9123)\n",
      "tensor([[-0.1055]], grad_fn=<AddmmBackward0>) tensor(0.2323)\n",
      "tensor([[-0.1108]], grad_fn=<AddmmBackward0>) tensor(-0.3204)\n",
      "tensor([[0.6713]], grad_fn=<AddmmBackward0>) tensor(0.5823)\n",
      "tensor([[-0.2528]], grad_fn=<AddmmBackward0>) tensor(-0.1036)\n",
      "tensor([[-0.8333]], grad_fn=<AddmmBackward0>) tensor(-0.9582)\n",
      "tensor([[-0.2356]], grad_fn=<AddmmBackward0>) tensor(0.4897)\n",
      "tensor([[-0.4788]], grad_fn=<AddmmBackward0>) tensor(-0.4921)\n",
      "tensor([[0.2041]], grad_fn=<AddmmBackward0>) tensor(0.3565)\n",
      "tensor([[-0.4943]], grad_fn=<AddmmBackward0>) tensor(-0.7915)\n",
      "tensor([[-0.5880]], grad_fn=<AddmmBackward0>) tensor(0.6923)\n",
      "tensor([[-0.5748]], grad_fn=<AddmmBackward0>) tensor(-0.4903)\n",
      "tensor([[-0.7692]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.5071]], grad_fn=<AddmmBackward0>) tensor(-0.7675)\n",
      "tensor([[-0.6109]], grad_fn=<AddmmBackward0>) tensor(-0.8742)\n",
      "tensor([[-0.6454]], grad_fn=<AddmmBackward0>) tensor(-0.6947)\n",
      "tensor([[-0.7070]], grad_fn=<AddmmBackward0>) tensor(-0.7690)\n",
      "tensor([[0.4611]], grad_fn=<AddmmBackward0>) tensor(0.8157)\n",
      "tensor([[0.4022]], grad_fn=<AddmmBackward0>) tensor(0.8367)\n",
      "tensor([[0.2337]], grad_fn=<AddmmBackward0>) tensor(0.6306)\n",
      "tensor([[-0.6002]], grad_fn=<AddmmBackward0>) tensor(0.5018)\n",
      "tensor([[-0.6069]], grad_fn=<AddmmBackward0>) tensor(0.3310)\n",
      "tensor([[-0.5716]], grad_fn=<AddmmBackward0>) tensor(-0.5097)\n",
      "tensor([[-0.1381]], grad_fn=<AddmmBackward0>) tensor(0.0176)\n",
      "tensor([[-0.6324]], grad_fn=<AddmmBackward0>) tensor(-0.7378)\n",
      "tensor([[-0.7974]], grad_fn=<AddmmBackward0>) tensor(-0.6095)\n",
      "tensor([[0.2352]], grad_fn=<AddmmBackward0>) tensor(0.3662)\n",
      "tensor([[-0.9274]], grad_fn=<AddmmBackward0>) tensor(-0.8885)\n",
      "tensor([[-0.7391]], grad_fn=<AddmmBackward0>) tensor(-0.7280)\n",
      "tensor([[-0.7056]], grad_fn=<AddmmBackward0>) tensor(-0.8469)\n",
      "tensor([[-0.1662]], grad_fn=<AddmmBackward0>) tensor(0.1954)\n",
      "tensor([[0.4933]], grad_fn=<AddmmBackward0>) tensor(0.5430)\n",
      "tensor([[-0.3706]], grad_fn=<AddmmBackward0>) tensor(-0.4224)\n",
      "tensor([[-0.7078]], grad_fn=<AddmmBackward0>) tensor(-0.6441)\n",
      "tensor([[-0.5133]], grad_fn=<AddmmBackward0>) tensor(-0.3326)\n",
      "tensor([[-0.7125]], grad_fn=<AddmmBackward0>) tensor(-0.9213)\n",
      "tensor([[-0.4861]], grad_fn=<AddmmBackward0>) tensor(-0.6408)\n",
      "tensor([[0.1090]], grad_fn=<AddmmBackward0>) tensor(0.1547)\n",
      "tensor([[-0.7657]], grad_fn=<AddmmBackward0>) tensor(-0.8481)\n",
      "tensor([[0.5081]], grad_fn=<AddmmBackward0>) tensor(0.1455)\n",
      "tensor([[-0.1692]], grad_fn=<AddmmBackward0>) tensor(-0.1328)\n",
      "tensor([[0.7040]], grad_fn=<AddmmBackward0>) tensor(0.6559)\n",
      "tensor([[-1.2602]], grad_fn=<AddmmBackward0>) tensor(-0.9160)\n",
      "tensor([[-0.4408]], grad_fn=<AddmmBackward0>) tensor(-0.6611)\n",
      "tensor([[-0.5040]], grad_fn=<AddmmBackward0>) tensor(-0.3371)\n",
      "tensor([[-0.7153]], grad_fn=<AddmmBackward0>) tensor(-0.7696)\n",
      "tensor([[0.6912]], grad_fn=<AddmmBackward0>) tensor(0.6047)\n",
      "tensor([[-0.1961]], grad_fn=<AddmmBackward0>) tensor(-0.2041)\n",
      "tensor([[0.3657]], grad_fn=<AddmmBackward0>) tensor(0.6476)\n",
      "tensor([[-0.7505]], grad_fn=<AddmmBackward0>) tensor(-0.8185)\n",
      "tensor([[0.4577]], grad_fn=<AddmmBackward0>) tensor(0.5764)\n",
      "tensor([[-0.1461]], grad_fn=<AddmmBackward0>) tensor(0.1687)\n",
      "tensor([[-0.3490]], grad_fn=<AddmmBackward0>) tensor(-0.0813)\n",
      "tensor([[-0.1313]], grad_fn=<AddmmBackward0>) tensor(0.3328)\n",
      "tensor([[-0.6740]], grad_fn=<AddmmBackward0>) tensor(-0.4634)\n",
      "tensor([[-0.4431]], grad_fn=<AddmmBackward0>) tensor(-0.8291)\n",
      "tensor([[-0.7630]], grad_fn=<AddmmBackward0>) tensor(-0.8455)\n",
      "tensor([[-1.0709]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.7003]], grad_fn=<AddmmBackward0>) tensor(0.5540)\n",
      "tensor([[0.1971]], grad_fn=<AddmmBackward0>) tensor(0.3786)\n",
      "tensor([[-0.0923]], grad_fn=<AddmmBackward0>) tensor(0.6527)\n",
      "tensor([[-0.7273]], grad_fn=<AddmmBackward0>) tensor(-0.8791)\n",
      "tensor([[0.6886]], grad_fn=<AddmmBackward0>) tensor(0.7162)\n",
      "tensor([[0.2803]], grad_fn=<AddmmBackward0>) tensor(0.5941)\n",
      "tensor([[-0.3134]], grad_fn=<AddmmBackward0>) tensor(0.0226)\n",
      "tensor([[0.5211]], grad_fn=<AddmmBackward0>) tensor(0.6385)\n",
      "tensor([[0.0620]], grad_fn=<AddmmBackward0>) tensor(-0.2483)\n",
      "tensor([[0.2892]], grad_fn=<AddmmBackward0>) tensor(0.5821)\n",
      "tensor([[0.4783]], grad_fn=<AddmmBackward0>) tensor(0.7555)\n",
      "tensor([[0.4701]], grad_fn=<AddmmBackward0>) tensor(0.9013)\n",
      "tensor([[-0.4496]], grad_fn=<AddmmBackward0>) tensor(-0.3845)\n",
      "tensor([[-0.6628]], grad_fn=<AddmmBackward0>) tensor(-0.4376)\n",
      "tensor([[-0.4638]], grad_fn=<AddmmBackward0>) tensor(-0.7097)\n",
      "tensor([[0.5948]], grad_fn=<AddmmBackward0>) tensor(0.6714)\n",
      "tensor([[-0.5569]], grad_fn=<AddmmBackward0>) tensor(-0.4535)\n",
      "tensor([[0.2956]], grad_fn=<AddmmBackward0>) tensor(0.5552)\n",
      "tensor([[-0.6977]], grad_fn=<AddmmBackward0>) tensor(-0.6044)\n",
      "tensor([[-0.7193]], grad_fn=<AddmmBackward0>) tensor(-0.8646)\n",
      "tensor([[-0.3773]], grad_fn=<AddmmBackward0>) tensor(-0.3132)\n",
      "tensor([[-0.2430]], grad_fn=<AddmmBackward0>) tensor(0.2791)\n",
      "tensor([[0.4378]], grad_fn=<AddmmBackward0>) tensor(0.5689)\n",
      "tensor([[-0.0131]], grad_fn=<AddmmBackward0>) tensor(0.2352)\n",
      "tensor([[0.3416]], grad_fn=<AddmmBackward0>) tensor(0.8120)\n",
      "tensor([[-0.5969]], grad_fn=<AddmmBackward0>) tensor(-0.8568)\n",
      "tensor([[0.5286]], grad_fn=<AddmmBackward0>) tensor(0.7454)\n",
      "tensor([[0.3524]], grad_fn=<AddmmBackward0>) tensor(0.4459)\n",
      "tensor([[-0.3457]], grad_fn=<AddmmBackward0>) tensor(-0.0136)\n",
      "tensor([[0.3602]], grad_fn=<AddmmBackward0>) tensor(0.5344)\n",
      "tensor([[0.1958]], grad_fn=<AddmmBackward0>) tensor(0.6205)\n",
      "tensor([[-0.3313]], grad_fn=<AddmmBackward0>) tensor(-0.8241)\n",
      "tensor([[-0.7700]], grad_fn=<AddmmBackward0>) tensor(-0.6273)\n",
      "tensor([[-0.6714]], grad_fn=<AddmmBackward0>) tensor(-0.5956)\n",
      "tensor([[-0.8924]], grad_fn=<AddmmBackward0>) tensor(-0.5659)\n",
      "tensor([[-0.4819]], grad_fn=<AddmmBackward0>) tensor(-0.7048)\n",
      "tensor([[0.3598]], grad_fn=<AddmmBackward0>) tensor(0.5326)\n",
      "tensor([[-0.1331]], grad_fn=<AddmmBackward0>) tensor(-0.1667)\n",
      "tensor([[0.4305]], grad_fn=<AddmmBackward0>) tensor(0.7828)\n",
      "tensor([[0.6536]], grad_fn=<AddmmBackward0>) tensor(0.8023)\n",
      "tensor([[-0.3773]], grad_fn=<AddmmBackward0>) tensor(-0.3287)\n",
      "tensor([[0.2436]], grad_fn=<AddmmBackward0>) tensor(0.7017)\n",
      "tensor([[0.6913]], grad_fn=<AddmmBackward0>) tensor(0.6170)\n",
      "tensor([[-0.6558]], grad_fn=<AddmmBackward0>) tensor(-0.4040)\n",
      "tensor([[-0.4162]], grad_fn=<AddmmBackward0>) tensor(-0.4158)\n",
      "tensor([[-0.7428]], grad_fn=<AddmmBackward0>) tensor(-0.4588)\n",
      "tensor([[0.2218]], grad_fn=<AddmmBackward0>) tensor(0.8530)\n",
      "tensor([[-0.5802]], grad_fn=<AddmmBackward0>) tensor(-0.2766)\n",
      "tensor([[-0.7631]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.5917]], grad_fn=<AddmmBackward0>) tensor(-0.1116)\n",
      "tensor([[0.6176]], grad_fn=<AddmmBackward0>) tensor(0.5085)\n",
      "tensor([[0.5549]], grad_fn=<AddmmBackward0>) tensor(0.7187)\n",
      "tensor([[0.4832]], grad_fn=<AddmmBackward0>) tensor(0.5714)\n",
      "tensor([[-0.3906]], grad_fn=<AddmmBackward0>) tensor(-0.6720)\n",
      "tensor([[-0.4429]], grad_fn=<AddmmBackward0>) tensor(-0.2915)\n",
      "tensor([[-0.7927]], grad_fn=<AddmmBackward0>) tensor(-0.7889)\n",
      "tensor([[0.2316]], grad_fn=<AddmmBackward0>) tensor(0.6682)\n",
      "tensor([[-0.2723]], grad_fn=<AddmmBackward0>) tensor(-0.2507)\n",
      "tensor([[-0.4157]], grad_fn=<AddmmBackward0>) tensor(-0.7375)\n",
      "tensor([[-0.0221]], grad_fn=<AddmmBackward0>) tensor(0.8297)\n",
      "tensor([[-0.2004]], grad_fn=<AddmmBackward0>) tensor(0.2001)\n",
      "tensor([[-0.6608]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.6064]], grad_fn=<AddmmBackward0>) tensor(0.7248)\n",
      "tensor([[-0.4650]], grad_fn=<AddmmBackward0>) tensor(-0.1028)\n",
      "tensor([[0.1270]], grad_fn=<AddmmBackward0>) tensor(0.1978)\n",
      "tensor([[-0.6155]], grad_fn=<AddmmBackward0>) tensor(-0.5559)\n",
      "tensor([[-0.7291]], grad_fn=<AddmmBackward0>) tensor(-0.8768)\n",
      "tensor([[-0.8978]], grad_fn=<AddmmBackward0>) tensor(-0.7836)\n",
      "tensor([[0.7537]], grad_fn=<AddmmBackward0>) tensor(0.6522)\n",
      "tensor([[-0.0422]], grad_fn=<AddmmBackward0>) tensor(0.2222)\n",
      "tensor([[-0.4857]], grad_fn=<AddmmBackward0>) tensor(-0.5085)\n",
      "tensor([[0.4603]], grad_fn=<AddmmBackward0>) tensor(0.5224)\n",
      "tensor([[-0.3485]], grad_fn=<AddmmBackward0>) tensor(-0.7334)\n",
      "tensor([[-0.6176]], grad_fn=<AddmmBackward0>) tensor(-0.5282)\n",
      "tensor([[0.7907]], grad_fn=<AddmmBackward0>) tensor(0.7258)\n",
      "tensor([[-0.1884]], grad_fn=<AddmmBackward0>) tensor(-0.3468)\n",
      "tensor([[0.0884]], grad_fn=<AddmmBackward0>) tensor(-0.1019)\n",
      "tensor([[-0.2053]], grad_fn=<AddmmBackward0>) tensor(-0.0016)\n",
      "tensor([[0.1910]], grad_fn=<AddmmBackward0>) tensor(0.3800)\n",
      "tensor([[0.0481]], grad_fn=<AddmmBackward0>) tensor(0.5072)\n",
      "tensor([[-0.6934]], grad_fn=<AddmmBackward0>) tensor(-0.6224)\n",
      "tensor([[-0.5110]], grad_fn=<AddmmBackward0>) tensor(-0.8513)\n",
      "tensor([[0.0100]], grad_fn=<AddmmBackward0>) tensor(0.0004)\n",
      "tensor([[-0.0978]], grad_fn=<AddmmBackward0>) tensor(0.2313)\n",
      "tensor([[-0.4048]], grad_fn=<AddmmBackward0>) tensor(0.2736)\n",
      "tensor([[0.4023]], grad_fn=<AddmmBackward0>) tensor(0.6888)\n",
      "tensor([[-1.4069]], grad_fn=<AddmmBackward0>) tensor(-0.9602)\n",
      "tensor([[0.0868]], grad_fn=<AddmmBackward0>) tensor(-0.1062)\n",
      "tensor([[-0.0590]], grad_fn=<AddmmBackward0>) tensor(-0.2592)\n",
      "tensor([[-1.1331]], grad_fn=<AddmmBackward0>) tensor(-0.9169)\n",
      "tensor([[-0.4857]], grad_fn=<AddmmBackward0>) tensor(-0.5975)\n",
      "tensor([[0.0656]], grad_fn=<AddmmBackward0>) tensor(0.6521)\n",
      "tensor([[0.0074]], grad_fn=<AddmmBackward0>) tensor(-0.0166)\n",
      "tensor([[-0.7461]], grad_fn=<AddmmBackward0>) tensor(-0.1087)\n",
      "tensor([[-0.6085]], grad_fn=<AddmmBackward0>) tensor(-0.2968)\n",
      "tensor([[0.6165]], grad_fn=<AddmmBackward0>) tensor(0.5926)\n",
      "tensor([[-0.2459]], grad_fn=<AddmmBackward0>) tensor(0.9902)\n",
      "tensor([[-0.5603]], grad_fn=<AddmmBackward0>) tensor(-0.7998)\n",
      "tensor([[-0.0596]], grad_fn=<AddmmBackward0>) tensor(0.5419)\n",
      "tensor([[-0.5025]], grad_fn=<AddmmBackward0>) tensor(-0.6496)\n",
      "tensor([[-0.6851]], grad_fn=<AddmmBackward0>) tensor(-0.4617)\n",
      "tensor([[-0.0062]], grad_fn=<AddmmBackward0>) tensor(-0.1753)\n",
      "tensor([[-0.1436]], grad_fn=<AddmmBackward0>) tensor(-0.3201)\n",
      "tensor([[0.3643]], grad_fn=<AddmmBackward0>) tensor(0.6129)\n",
      "tensor([[-0.5273]], grad_fn=<AddmmBackward0>) tensor(-0.2962)\n",
      "tensor([[0.4790]], grad_fn=<AddmmBackward0>) tensor(0.1674)\n",
      "tensor([[-0.9887]], grad_fn=<AddmmBackward0>) tensor(-0.8608)\n",
      "tensor([[-0.6962]], grad_fn=<AddmmBackward0>) tensor(-0.8534)\n",
      "tensor([[0.3366]], grad_fn=<AddmmBackward0>) tensor(0.5917)\n",
      "tensor([[-0.0035]], grad_fn=<AddmmBackward0>) tensor(-0.1175)\n",
      "tensor([[-0.7383]], grad_fn=<AddmmBackward0>) tensor(-0.8499)\n",
      "tensor([[-0.6749]], grad_fn=<AddmmBackward0>) tensor(-0.6804)\n",
      "tensor([[0.2581]], grad_fn=<AddmmBackward0>) tensor(0.7475)\n",
      "tensor([[-0.7261]], grad_fn=<AddmmBackward0>) tensor(-0.8279)\n",
      "tensor([[-0.6347]], grad_fn=<AddmmBackward0>) tensor(-0.8637)\n",
      "tensor([[-0.5600]], grad_fn=<AddmmBackward0>) tensor(-0.7766)\n",
      "tensor([[0.2335]], grad_fn=<AddmmBackward0>) tensor(0.1338)\n",
      "tensor([[0.3774]], grad_fn=<AddmmBackward0>) tensor(0.2547)\n",
      "tensor([[-0.5698]], grad_fn=<AddmmBackward0>) tensor(-0.4089)\n",
      "tensor([[-0.2084]], grad_fn=<AddmmBackward0>) tensor(0.5691)\n",
      "tensor([[-0.4560]], grad_fn=<AddmmBackward0>) tensor(-0.5421)\n",
      "tensor([[-0.2091]], grad_fn=<AddmmBackward0>) tensor(-0.3290)\n",
      "tensor([[-0.8082]], grad_fn=<AddmmBackward0>) tensor(-0.9529)\n",
      "tensor([[-0.5018]], grad_fn=<AddmmBackward0>) tensor(-0.5483)\n",
      "tensor([[-0.2320]], grad_fn=<AddmmBackward0>) tensor(-0.1751)\n",
      "tensor([[-0.3626]], grad_fn=<AddmmBackward0>) tensor(0.0249)\n",
      "tensor([[0.3954]], grad_fn=<AddmmBackward0>) tensor(0.8149)\n",
      "tensor([[0.2090]], grad_fn=<AddmmBackward0>) tensor(0.5694)\n",
      "tensor([[0.6922]], grad_fn=<AddmmBackward0>) tensor(0.5829)\n",
      "tensor([[-0.8000]], grad_fn=<AddmmBackward0>) tensor(-0.8748)\n",
      "tensor([[0.5268]], grad_fn=<AddmmBackward0>) tensor(0.5157)\n",
      "tensor([[-0.3512]], grad_fn=<AddmmBackward0>) tensor(-0.4649)\n",
      "tensor([[-0.7661]], grad_fn=<AddmmBackward0>) tensor(-0.8674)\n",
      "tensor([[-0.1275]], grad_fn=<AddmmBackward0>) tensor(0.4991)\n",
      "tensor([[-1.4385]], grad_fn=<AddmmBackward0>) tensor(-0.9601)\n",
      "tensor([[-0.1736]], grad_fn=<AddmmBackward0>) tensor(-0.2684)\n",
      "tensor([[-0.7364]], grad_fn=<AddmmBackward0>) tensor(-0.6513)\n",
      "tensor([[-0.2729]], grad_fn=<AddmmBackward0>) tensor(-0.1571)\n",
      "tensor([[-0.1565]], grad_fn=<AddmmBackward0>) tensor(-0.8248)\n",
      "tensor([[-0.5194]], grad_fn=<AddmmBackward0>) tensor(-0.5746)\n",
      "tensor([[0.6565]], grad_fn=<AddmmBackward0>) tensor(0.6023)\n",
      "tensor([[0.0041]], grad_fn=<AddmmBackward0>) tensor(0.3862)\n",
      "tensor([[0.5681]], grad_fn=<AddmmBackward0>) tensor(0.8288)\n",
      "tensor([[-0.1553]], grad_fn=<AddmmBackward0>) tensor(-0.3689)\n",
      "tensor([[-0.7063]], grad_fn=<AddmmBackward0>) tensor(-0.7097)\n",
      "tensor([[0.2937]], grad_fn=<AddmmBackward0>) tensor(0.3985)\n",
      "tensor([[-0.3233]], grad_fn=<AddmmBackward0>) tensor(0.2633)\n",
      "tensor([[0.5061]], grad_fn=<AddmmBackward0>) tensor(0.7658)\n",
      "tensor([[-0.6958]], grad_fn=<AddmmBackward0>) tensor(-0.8028)\n",
      "tensor([[-0.9539]], grad_fn=<AddmmBackward0>) tensor(-0.9431)\n",
      "tensor([[0.6357]], grad_fn=<AddmmBackward0>) tensor(0.5720)\n",
      "tensor([[-0.6627]], grad_fn=<AddmmBackward0>) tensor(-0.9025)\n",
      "tensor([[-0.6812]], grad_fn=<AddmmBackward0>) tensor(-0.4946)\n",
      "tensor([[-0.6101]], grad_fn=<AddmmBackward0>) tensor(-0.3562)\n",
      "tensor([[-0.6524]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.4886]], grad_fn=<AddmmBackward0>) tensor(-0.4729)\n",
      "tensor([[0.4004]], grad_fn=<AddmmBackward0>) tensor(0.5459)\n",
      "tensor([[-0.6942]], grad_fn=<AddmmBackward0>) tensor(-0.8505)\n",
      "tensor([[-0.5101]], grad_fn=<AddmmBackward0>) tensor(-0.5192)\n",
      "tensor([[0.1621]], grad_fn=<AddmmBackward0>) tensor(0.4504)\n",
      "tensor([[-0.6859]], grad_fn=<AddmmBackward0>) tensor(-0.2997)\n",
      "tensor([[-0.5776]], grad_fn=<AddmmBackward0>) tensor(-0.6046)\n",
      "tensor([[-0.6504]], grad_fn=<AddmmBackward0>) tensor(-0.6480)\n",
      "tensor([[-0.2003]], grad_fn=<AddmmBackward0>) tensor(-0.0025)\n",
      "tensor([[-0.7576]], grad_fn=<AddmmBackward0>) tensor(-0.6291)\n",
      "tensor([[-0.9446]], grad_fn=<AddmmBackward0>) tensor(-0.4626)\n",
      "tensor([[0.4625]], grad_fn=<AddmmBackward0>) tensor(0.7966)\n",
      "tensor([[-0.6059]], grad_fn=<AddmmBackward0>) tensor(-0.4647)\n",
      "tensor([[-0.5965]], grad_fn=<AddmmBackward0>) tensor(-0.4427)\n",
      "tensor([[-0.3763]], grad_fn=<AddmmBackward0>) tensor(-0.8065)\n",
      "tensor([[0.4323]], grad_fn=<AddmmBackward0>) tensor(0.3865)\n",
      "tensor([[0.0056]], grad_fn=<AddmmBackward0>) tensor(0.1087)\n",
      "tensor([[0.1099]], grad_fn=<AddmmBackward0>) tensor(0.1079)\n",
      "tensor([[-0.6392]], grad_fn=<AddmmBackward0>) tensor(-0.9209)\n",
      "tensor([[-0.3426]], grad_fn=<AddmmBackward0>) tensor(-0.1774)\n",
      "tensor([[0.3351]], grad_fn=<AddmmBackward0>) tensor(0.7415)\n",
      "tensor([[0.5914]], grad_fn=<AddmmBackward0>) tensor(0.5664)\n",
      "tensor([[-0.3715]], grad_fn=<AddmmBackward0>) tensor(-0.8599)\n",
      "tensor([[-0.0337]], grad_fn=<AddmmBackward0>) tensor(0.0113)\n",
      "tensor([[-0.0550]], grad_fn=<AddmmBackward0>) tensor(-0.1784)\n",
      "tensor([[-0.3497]], grad_fn=<AddmmBackward0>) tensor(0.0956)\n",
      "tensor([[-0.7383]], grad_fn=<AddmmBackward0>) tensor(-0.7396)\n",
      "tensor([[-0.0799]], grad_fn=<AddmmBackward0>) tensor(0.6830)\n",
      "tensor([[0.4405]], grad_fn=<AddmmBackward0>) tensor(0.5468)\n",
      "tensor([[0.7019]], grad_fn=<AddmmBackward0>) tensor(0.5784)\n",
      "tensor([[-1.0967]], grad_fn=<AddmmBackward0>) tensor(-0.9306)\n",
      "tensor([[-0.9689]], grad_fn=<AddmmBackward0>) tensor(-0.3296)\n",
      "tensor([[-0.7334]], grad_fn=<AddmmBackward0>) tensor(-0.9770)\n",
      "tensor([[-0.2162]], grad_fn=<AddmmBackward0>) tensor(-0.1371)\n",
      "tensor([[-0.6066]], grad_fn=<AddmmBackward0>) tensor(-0.8076)\n",
      "tensor([[0.4491]], grad_fn=<AddmmBackward0>) tensor(0.6259)\n",
      "tensor([[-0.4341]], grad_fn=<AddmmBackward0>) tensor(-0.6658)\n",
      "tensor([[-0.4011]], grad_fn=<AddmmBackward0>) tensor(-0.3920)\n",
      "tensor([[0.4407]], grad_fn=<AddmmBackward0>) tensor(0.6825)\n",
      "tensor([[0.4285]], grad_fn=<AddmmBackward0>) tensor(0.6171)\n",
      "tensor([[0.2760]], grad_fn=<AddmmBackward0>) tensor(0.8536)\n",
      "tensor([[0.4514]], grad_fn=<AddmmBackward0>) tensor(0.5547)\n",
      "tensor([[-0.5932]], grad_fn=<AddmmBackward0>) tensor(-0.2681)\n",
      "tensor([[-0.6860]], grad_fn=<AddmmBackward0>) tensor(-0.5516)\n",
      "tensor([[-0.4981]], grad_fn=<AddmmBackward0>) tensor(-0.2296)\n",
      "tensor([[-0.6427]], grad_fn=<AddmmBackward0>) tensor(-0.7789)\n",
      "tensor([[-0.0897]], grad_fn=<AddmmBackward0>) tensor(-0.9016)\n",
      "tensor([[-0.6782]], grad_fn=<AddmmBackward0>) tensor(-0.9405)\n",
      "tensor([[-0.1911]], grad_fn=<AddmmBackward0>) tensor(-0.3978)\n",
      "tensor([[0.1531]], grad_fn=<AddmmBackward0>) tensor(0.7819)\n",
      "tensor([[-0.6049]], grad_fn=<AddmmBackward0>) tensor(-0.4569)\n",
      "tensor([[-0.3267]], grad_fn=<AddmmBackward0>) tensor(-0.2471)\n",
      "tensor([[-0.6760]], grad_fn=<AddmmBackward0>) tensor(-0.5401)\n",
      "tensor([[-0.0528]], grad_fn=<AddmmBackward0>) tensor(0.4056)\n",
      "tensor([[0.3865]], grad_fn=<AddmmBackward0>) tensor(0.7516)\n",
      "tensor([[0.1709]], grad_fn=<AddmmBackward0>) tensor(0.6275)\n",
      "tensor([[-0.5264]], grad_fn=<AddmmBackward0>) tensor(-0.4381)\n",
      "tensor([[-0.4408]], grad_fn=<AddmmBackward0>) tensor(-0.8269)\n",
      "tensor([[-0.3939]], grad_fn=<AddmmBackward0>) tensor(-0.8277)\n",
      "tensor([[-0.6651]], grad_fn=<AddmmBackward0>) tensor(-0.5892)\n",
      "tensor([[-0.7459]], grad_fn=<AddmmBackward0>) tensor(-0.7825)\n",
      "tensor([[-0.0349]], grad_fn=<AddmmBackward0>) tensor(0.4684)\n",
      "tensor([[0.6564]], grad_fn=<AddmmBackward0>) tensor(0.5569)\n",
      "tensor([[0.2615]], grad_fn=<AddmmBackward0>) tensor(0.5876)\n",
      "tensor([[0.4457]], grad_fn=<AddmmBackward0>) tensor(0.7094)\n",
      "tensor([[-1.0150]], grad_fn=<AddmmBackward0>) tensor(-0.8633)\n",
      "tensor([[-0.8036]], grad_fn=<AddmmBackward0>) tensor(-0.8704)\n",
      "tensor([[0.0209]], grad_fn=<AddmmBackward0>) tensor(-0.2242)\n",
      "tensor([[-0.9534]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.4571]], grad_fn=<AddmmBackward0>) tensor(-0.9352)\n",
      "tensor([[0.2566]], grad_fn=<AddmmBackward0>) tensor(0.1323)\n",
      "tensor([[-0.7944]], grad_fn=<AddmmBackward0>) tensor(-0.8750)\n",
      "tensor([[1.0140]], grad_fn=<AddmmBackward0>) tensor(0.7371)\n",
      "tensor([[-0.5719]], grad_fn=<AddmmBackward0>) tensor(-0.8629)\n",
      "tensor([[-0.8509]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.5010]], grad_fn=<AddmmBackward0>) tensor(-0.9211)\n",
      "tensor([[-0.8323]], grad_fn=<AddmmBackward0>) tensor(-0.5432)\n",
      "tensor([[-0.3054]], grad_fn=<AddmmBackward0>) tensor(-0.3687)\n",
      "tensor([[-0.8541]], grad_fn=<AddmmBackward0>) tensor(-0.4701)\n",
      "tensor([[0.3455]], grad_fn=<AddmmBackward0>) tensor(0.6480)\n",
      "tensor([[-0.3579]], grad_fn=<AddmmBackward0>) tensor(0.2891)\n",
      "tensor([[-0.3287]], grad_fn=<AddmmBackward0>) tensor(-0.7408)\n",
      "tensor([[-0.4780]], grad_fn=<AddmmBackward0>) tensor(-0.4133)\n",
      "tensor([[0.5469]], grad_fn=<AddmmBackward0>) tensor(0.7848)\n",
      "tensor([[0.2573]], grad_fn=<AddmmBackward0>) tensor(0.5471)\n",
      "tensor([[-0.8312]], grad_fn=<AddmmBackward0>) tensor(-0.9723)\n",
      "tensor([[-0.7679]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.4974]], grad_fn=<AddmmBackward0>) tensor(-0.5867)\n",
      "tensor([[0.1100]], grad_fn=<AddmmBackward0>) tensor(0.7375)\n",
      "tensor([[-0.4562]], grad_fn=<AddmmBackward0>) tensor(-0.6780)\n",
      "tensor([[0.1823]], grad_fn=<AddmmBackward0>) tensor(0.5282)\n",
      "tensor([[0.1083]], grad_fn=<AddmmBackward0>) tensor(0.7309)\n",
      "tensor([[-0.0612]], grad_fn=<AddmmBackward0>) tensor(-0.0185)\n",
      "tensor([[0.5789]], grad_fn=<AddmmBackward0>) tensor(0.7978)\n",
      "tensor([[-0.2828]], grad_fn=<AddmmBackward0>) tensor(-0.1478)\n",
      "tensor([[-0.7653]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.2743]], grad_fn=<AddmmBackward0>) tensor(0.0716)\n",
      "tensor([[0.4469]], grad_fn=<AddmmBackward0>) tensor(0.6889)\n",
      "tensor([[-0.4083]], grad_fn=<AddmmBackward0>) tensor(-0.3893)\n",
      "tensor([[0.2204]], grad_fn=<AddmmBackward0>) tensor(0.7994)\n",
      "tensor([[-0.2740]], grad_fn=<AddmmBackward0>) tensor(-0.5097)\n",
      "tensor([[0.6205]], grad_fn=<AddmmBackward0>) tensor(0.5301)\n",
      "tensor([[-0.0832]], grad_fn=<AddmmBackward0>) tensor(0.6134)\n",
      "tensor([[-0.3042]], grad_fn=<AddmmBackward0>) tensor(-0.4072)\n",
      "tensor([[-0.0468]], grad_fn=<AddmmBackward0>) tensor(0.1232)\n",
      "tensor([[0.1682]], grad_fn=<AddmmBackward0>) tensor(0.7669)\n",
      "tensor([[0.1688]], grad_fn=<AddmmBackward0>) tensor(0.4617)\n",
      "tensor([[-0.4503]], grad_fn=<AddmmBackward0>) tensor(0.6691)\n",
      "tensor([[-0.5479]], grad_fn=<AddmmBackward0>) tensor(-0.5897)\n",
      "tensor([[0.0244]], grad_fn=<AddmmBackward0>) tensor(0.8412)\n",
      "tensor([[-0.5075]], grad_fn=<AddmmBackward0>) tensor(-0.7362)\n",
      "tensor([[-0.6086]], grad_fn=<AddmmBackward0>) tensor(-0.4390)\n",
      "tensor([[-0.1075]], grad_fn=<AddmmBackward0>) tensor(-0.2588)\n",
      "tensor([[0.3961]], grad_fn=<AddmmBackward0>) tensor(0.5322)\n",
      "tensor([[-0.8051]], grad_fn=<AddmmBackward0>) tensor(-0.7074)\n",
      "tensor([[-0.5796]], grad_fn=<AddmmBackward0>) tensor(-0.4385)\n",
      "tensor([[-0.8310]], grad_fn=<AddmmBackward0>) tensor(-0.7912)\n",
      "tensor([[0.2569]], grad_fn=<AddmmBackward0>) tensor(0.5562)\n",
      "tensor([[-0.8328]], grad_fn=<AddmmBackward0>) tensor(-0.8868)\n",
      "tensor([[-0.2894]], grad_fn=<AddmmBackward0>) tensor(0.1430)\n",
      "tensor([[-0.0629]], grad_fn=<AddmmBackward0>) tensor(0.8476)\n",
      "tensor([[0.1818]], grad_fn=<AddmmBackward0>) tensor(0.9913)\n",
      "tensor([[-0.7509]], grad_fn=<AddmmBackward0>) tensor(-0.8259)\n",
      "tensor([[-0.9273]], grad_fn=<AddmmBackward0>) tensor(-0.9232)\n",
      "tensor([[-0.8109]], grad_fn=<AddmmBackward0>) tensor(-0.3448)\n",
      "tensor([[0.0159]], grad_fn=<AddmmBackward0>) tensor(0.1915)\n",
      "tensor([[-0.4736]], grad_fn=<AddmmBackward0>) tensor(-0.3655)\n",
      "tensor([[-0.8300]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.7009]], grad_fn=<AddmmBackward0>) tensor(0.7112)\n",
      "tensor([[-0.1268]], grad_fn=<AddmmBackward0>) tensor(-0.2711)\n",
      "tensor([[-0.7769]], grad_fn=<AddmmBackward0>) tensor(-0.8423)\n",
      "tensor([[-0.5687]], grad_fn=<AddmmBackward0>) tensor(-0.3997)\n",
      "tensor([[-0.5964]], grad_fn=<AddmmBackward0>) tensor(-0.4373)\n",
      "tensor([[-0.7250]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.2241]], grad_fn=<AddmmBackward0>) tensor(0.5545)\n",
      "tensor([[0.6493]], grad_fn=<AddmmBackward0>) tensor(0.5455)\n",
      "tensor([[0.6179]], grad_fn=<AddmmBackward0>) tensor(0.7334)\n",
      "tensor([[-0.9627]], grad_fn=<AddmmBackward0>) tensor(-0.9173)\n",
      "tensor([[0.0214]], grad_fn=<AddmmBackward0>) tensor(0.6165)\n",
      "tensor([[-0.2427]], grad_fn=<AddmmBackward0>) tensor(-0.7813)\n",
      "tensor([[-0.5185]], grad_fn=<AddmmBackward0>) tensor(-0.8006)\n",
      "tensor([[-0.5091]], grad_fn=<AddmmBackward0>) tensor(-0.3004)\n",
      "tensor([[-0.6419]], grad_fn=<AddmmBackward0>) tensor(-0.5550)\n",
      "tensor([[-0.5272]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.7384]], grad_fn=<AddmmBackward0>) tensor(0.7066)\n",
      "tensor([[-0.5592]], grad_fn=<AddmmBackward0>) tensor(-0.5222)\n",
      "tensor([[0.6168]], grad_fn=<AddmmBackward0>) tensor(0.7385)\n",
      "tensor([[0.5612]], grad_fn=<AddmmBackward0>) tensor(0.7540)\n",
      "tensor([[-0.3740]], grad_fn=<AddmmBackward0>) tensor(-0.3938)\n",
      "tensor([[-0.4648]], grad_fn=<AddmmBackward0>) tensor(-0.3658)\n",
      "tensor([[-0.5849]], grad_fn=<AddmmBackward0>) tensor(-0.4224)\n",
      "tensor([[-0.5902]], grad_fn=<AddmmBackward0>) tensor(-0.2800)\n",
      "tensor([[0.1648]], grad_fn=<AddmmBackward0>) tensor(0.8041)\n",
      "tensor([[-0.6961]], grad_fn=<AddmmBackward0>) tensor(-0.7514)\n",
      "tensor([[0.2692]], grad_fn=<AddmmBackward0>) tensor(0.5311)\n",
      "tensor([[-0.7474]], grad_fn=<AddmmBackward0>) tensor(-0.4189)\n",
      "tensor([[0.6247]], grad_fn=<AddmmBackward0>) tensor(0.7374)\n",
      "tensor([[0.0094]], grad_fn=<AddmmBackward0>) tensor(0.6160)\n",
      "tensor([[-0.9686]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.3343]], grad_fn=<AddmmBackward0>) tensor(0.2367)\n",
      "tensor([[-0.6561]], grad_fn=<AddmmBackward0>) tensor(-0.4769)\n",
      "tensor([[-0.1989]], grad_fn=<AddmmBackward0>) tensor(0.0888)\n",
      "tensor([[0.3650]], grad_fn=<AddmmBackward0>) tensor(0.3911)\n",
      "tensor([[-0.6087]], grad_fn=<AddmmBackward0>) tensor(-0.4313)\n",
      "tensor([[-0.5234]], grad_fn=<AddmmBackward0>) tensor(-0.2974)\n",
      "tensor([[0.4683]], grad_fn=<AddmmBackward0>) tensor(0.5147)\n",
      "tensor([[0.4857]], grad_fn=<AddmmBackward0>) tensor(0.7063)\n",
      "tensor([[-0.8247]], grad_fn=<AddmmBackward0>) tensor(-0.5513)\n",
      "tensor([[-0.4459]], grad_fn=<AddmmBackward0>) tensor(-0.3340)\n",
      "tensor([[-0.3186]], grad_fn=<AddmmBackward0>) tensor(0.6523)\n",
      "tensor([[-0.5770]], grad_fn=<AddmmBackward0>) tensor(-0.4659)\n",
      "tensor([[-0.3003]], grad_fn=<AddmmBackward0>) tensor(-0.0579)\n",
      "tensor([[0.5418]], grad_fn=<AddmmBackward0>) tensor(0.4104)\n",
      "tensor([[-0.3430]], grad_fn=<AddmmBackward0>) tensor(-0.2502)\n",
      "tensor([[0.1747]], grad_fn=<AddmmBackward0>) tensor(0.6285)\n",
      "tensor([[-0.4787]], grad_fn=<AddmmBackward0>) tensor(-0.0081)\n",
      "tensor([[0.4194]], grad_fn=<AddmmBackward0>) tensor(0.7571)\n",
      "tensor([[0.1727]], grad_fn=<AddmmBackward0>) tensor(0.7381)\n",
      "tensor([[0.4805]], grad_fn=<AddmmBackward0>) tensor(0.7378)\n",
      "tensor([[-0.5852]], grad_fn=<AddmmBackward0>) tensor(-0.8582)\n",
      "tensor([[1.0975]], grad_fn=<AddmmBackward0>) tensor(0.7536)\n",
      "tensor([[-1.0204]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.0933]], grad_fn=<AddmmBackward0>) tensor(0.4875)\n",
      "tensor([[0.4652]], grad_fn=<AddmmBackward0>) tensor(0.7912)\n",
      "tensor([[-0.4355]], grad_fn=<AddmmBackward0>) tensor(-0.5510)\n",
      "tensor([[-0.0564]], grad_fn=<AddmmBackward0>) tensor(0.0700)\n",
      "tensor([[-0.6240]], grad_fn=<AddmmBackward0>) tensor(0.0680)\n",
      "tensor([[0.6979]], grad_fn=<AddmmBackward0>) tensor(0.6229)\n",
      "tensor([[-0.8103]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.3932]], grad_fn=<AddmmBackward0>) tensor(-0.3784)\n",
      "tensor([[0.8145]], grad_fn=<AddmmBackward0>) tensor(0.5744)\n",
      "tensor([[-0.1319]], grad_fn=<AddmmBackward0>) tensor(-0.3045)\n",
      "tensor([[0.0502]], grad_fn=<AddmmBackward0>) tensor(-0.0422)\n",
      "tensor([[-0.0510]], grad_fn=<AddmmBackward0>) tensor(0.6954)\n",
      "tensor([[-0.4794]], grad_fn=<AddmmBackward0>) tensor(-0.2685)\n",
      "tensor([[0.5580]], grad_fn=<AddmmBackward0>) tensor(0.7659)\n",
      "tensor([[-0.6820]], grad_fn=<AddmmBackward0>) tensor(-0.7346)\n",
      "tensor([[-0.1546]], grad_fn=<AddmmBackward0>) tensor(0.1107)\n",
      "tensor([[-0.3039]], grad_fn=<AddmmBackward0>) tensor(-0.1473)\n",
      "tensor([[0.1619]], grad_fn=<AddmmBackward0>) tensor(0.7224)\n",
      "tensor([[-0.7644]], grad_fn=<AddmmBackward0>) tensor(-0.6614)\n",
      "tensor([[-0.0319]], grad_fn=<AddmmBackward0>) tensor(-0.1151)\n",
      "tensor([[-0.6395]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.0371]], grad_fn=<AddmmBackward0>) tensor(0.6857)\n",
      "tensor([[-0.8268]], grad_fn=<AddmmBackward0>) tensor(-0.2710)\n",
      "tensor([[-0.7118]], grad_fn=<AddmmBackward0>) tensor(-0.8751)\n",
      "tensor([[-0.3460]], grad_fn=<AddmmBackward0>) tensor(-0.2117)\n",
      "tensor([[-0.0516]], grad_fn=<AddmmBackward0>) tensor(0.1058)\n",
      "tensor([[-0.4509]], grad_fn=<AddmmBackward0>) tensor(-0.5193)\n",
      "tensor([[0.3617]], grad_fn=<AddmmBackward0>) tensor(0.5704)\n",
      "tensor([[0.2047]], grad_fn=<AddmmBackward0>) tensor(0.5277)\n",
      "tensor([[0.7290]], grad_fn=<AddmmBackward0>) tensor(0.5642)\n",
      "tensor([[-0.1205]], grad_fn=<AddmmBackward0>) tensor(-0.1607)\n",
      "tensor([[0.6877]], grad_fn=<AddmmBackward0>) tensor(0.5999)\n",
      "tensor([[-0.2559]], grad_fn=<AddmmBackward0>) tensor(0.2933)\n",
      "tensor([[0.7533]], grad_fn=<AddmmBackward0>) tensor(0.5753)\n",
      "tensor([[-0.3779]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.5144]], grad_fn=<AddmmBackward0>) tensor(0.5301)\n",
      "tensor([[-0.1140]], grad_fn=<AddmmBackward0>) tensor(-0.7964)\n",
      "tensor([[0.2054]], grad_fn=<AddmmBackward0>) tensor(0.4962)\n",
      "tensor([[-0.2729]], grad_fn=<AddmmBackward0>) tensor(-0.6192)\n",
      "tensor([[-0.7149]], grad_fn=<AddmmBackward0>) tensor(0.3376)\n",
      "tensor([[-0.1968]], grad_fn=<AddmmBackward0>) tensor(-0.3894)\n",
      "tensor([[-0.4597]], grad_fn=<AddmmBackward0>) tensor(-0.6338)\n",
      "tensor([[0.3288]], grad_fn=<AddmmBackward0>) tensor(0.5570)\n",
      "tensor([[-0.0154]], grad_fn=<AddmmBackward0>) tensor(0.3497)\n",
      "tensor([[-0.5988]], grad_fn=<AddmmBackward0>) tensor(-0.4535)\n",
      "tensor([[-0.7860]], grad_fn=<AddmmBackward0>) tensor(-0.6927)\n",
      "tensor([[0.2347]], grad_fn=<AddmmBackward0>) tensor(0.6642)\n",
      "tensor([[-0.3169]], grad_fn=<AddmmBackward0>) tensor(0.0342)\n",
      "tensor([[0.5040]], grad_fn=<AddmmBackward0>) tensor(0.6377)\n",
      "tensor([[-0.7165]], grad_fn=<AddmmBackward0>) tensor(-0.6784)\n",
      "tensor([[0.4820]], grad_fn=<AddmmBackward0>) tensor(0.8317)\n",
      "tensor([[-0.6792]], grad_fn=<AddmmBackward0>) tensor(-0.4763)\n",
      "tensor([[-0.2550]], grad_fn=<AddmmBackward0>) tensor(-0.2129)\n",
      "tensor([[-0.5299]], grad_fn=<AddmmBackward0>) tensor(-0.5954)\n",
      "tensor([[-0.4300]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.3145]], grad_fn=<AddmmBackward0>) tensor(-0.0590)\n",
      "tensor([[-0.1904]], grad_fn=<AddmmBackward0>) tensor(0.1787)\n",
      "tensor([[-0.8116]], grad_fn=<AddmmBackward0>) tensor(-0.8367)\n",
      "tensor([[-0.7976]], grad_fn=<AddmmBackward0>) tensor(-0.8253)\n",
      "tensor([[0.3583]], grad_fn=<AddmmBackward0>) tensor(0.7374)\n",
      "tensor([[-0.7565]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.2039]], grad_fn=<AddmmBackward0>) tensor(-0.0999)\n",
      "tensor([[-0.5402]], grad_fn=<AddmmBackward0>) tensor(-0.2967)\n",
      "tensor([[-0.5052]], grad_fn=<AddmmBackward0>) tensor(-0.3493)\n",
      "tensor([[0.5030]], grad_fn=<AddmmBackward0>) tensor(0.7341)\n",
      "tensor([[-0.0143]], grad_fn=<AddmmBackward0>) tensor(-0.2499)\n",
      "tensor([[0.5084]], grad_fn=<AddmmBackward0>) tensor(0.7552)\n",
      "tensor([[0.2120]], grad_fn=<AddmmBackward0>) tensor(0.6281)\n",
      "tensor([[-0.8301]], grad_fn=<AddmmBackward0>) tensor(-0.8559)\n",
      "tensor([[-0.5969]], grad_fn=<AddmmBackward0>) tensor(-0.4439)\n",
      "tensor([[-0.6847]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.3420]], grad_fn=<AddmmBackward0>) tensor(0.6405)\n",
      "tensor([[-0.7588]], grad_fn=<AddmmBackward0>) tensor(-0.6924)\n",
      "tensor([[-0.6931]], grad_fn=<AddmmBackward0>) tensor(-0.8118)\n",
      "tensor([[-0.1868]], grad_fn=<AddmmBackward0>) tensor(0.0149)\n",
      "tensor([[-0.7192]], grad_fn=<AddmmBackward0>) tensor(-0.9681)\n",
      "tensor([[-0.1837]], grad_fn=<AddmmBackward0>) tensor(-0.4503)\n",
      "tensor([[-0.8206]], grad_fn=<AddmmBackward0>) tensor(-0.5734)\n",
      "tensor([[-0.4797]], grad_fn=<AddmmBackward0>) tensor(-0.2582)\n",
      "tensor([[0.5478]], grad_fn=<AddmmBackward0>) tensor(0.8114)\n",
      "tensor([[0.1671]], grad_fn=<AddmmBackward0>) tensor(0.6076)\n",
      "tensor([[0.5236]], grad_fn=<AddmmBackward0>) tensor(0.5661)\n",
      "tensor([[0.1440]], grad_fn=<AddmmBackward0>) tensor(0.6149)\n",
      "tensor([[-0.2623]], grad_fn=<AddmmBackward0>) tensor(0.7175)\n",
      "tensor([[0.5214]], grad_fn=<AddmmBackward0>) tensor(0.7486)\n",
      "tensor([[0.2882]], grad_fn=<AddmmBackward0>) tensor(0.0205)\n",
      "tensor([[0.0445]], grad_fn=<AddmmBackward0>) tensor(0.2731)\n",
      "tensor([[0.1287]], grad_fn=<AddmmBackward0>) tensor(0.5420)\n",
      "tensor([[0.3732]], grad_fn=<AddmmBackward0>) tensor(0.5173)\n",
      "tensor([[-0.7577]], grad_fn=<AddmmBackward0>) tensor(-0.8334)\n",
      "tensor([[-0.4415]], grad_fn=<AddmmBackward0>) tensor(-0.1093)\n",
      "tensor([[-0.7028]], grad_fn=<AddmmBackward0>) tensor(-0.5413)\n",
      "tensor([[-0.5284]], grad_fn=<AddmmBackward0>) tensor(-0.6575)\n",
      "tensor([[0.1207]], grad_fn=<AddmmBackward0>) tensor(0.6691)\n",
      "tensor([[-0.7327]], grad_fn=<AddmmBackward0>) tensor(-0.5659)\n",
      "tensor([[-0.6318]], grad_fn=<AddmmBackward0>) tensor(-0.8568)\n",
      "tensor([[-0.1644]], grad_fn=<AddmmBackward0>) tensor(-0.1496)\n",
      "tensor([[-0.3226]], grad_fn=<AddmmBackward0>) tensor(-0.3016)\n",
      "tensor([[-0.7815]], grad_fn=<AddmmBackward0>) tensor(-0.7328)\n",
      "tensor([[-0.5457]], grad_fn=<AddmmBackward0>) tensor(-0.5859)\n",
      "tensor([[0.1281]], grad_fn=<AddmmBackward0>) tensor(-0.0208)\n",
      "tensor([[0.0490]], grad_fn=<AddmmBackward0>) tensor(0.5775)\n",
      "tensor([[-0.4737]], grad_fn=<AddmmBackward0>) tensor(-0.6505)\n",
      "tensor([[-0.4054]], grad_fn=<AddmmBackward0>) tensor(-0.3886)\n",
      "tensor([[0.7050]], grad_fn=<AddmmBackward0>) tensor(0.7311)\n",
      "tensor([[-0.2084]], grad_fn=<AddmmBackward0>) tensor(-0.9197)\n",
      "tensor([[-0.4887]], grad_fn=<AddmmBackward0>) tensor(-0.5333)\n",
      "tensor([[-0.2583]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.0490]], grad_fn=<AddmmBackward0>) tensor(0.3757)\n",
      "tensor([[-0.0118]], grad_fn=<AddmmBackward0>) tensor(0.0664)\n",
      "tensor([[-0.4984]], grad_fn=<AddmmBackward0>) tensor(-0.5571)\n",
      "tensor([[-0.2851]], grad_fn=<AddmmBackward0>) tensor(-0.0976)\n",
      "tensor([[-0.6862]], grad_fn=<AddmmBackward0>) tensor(-0.8055)\n",
      "tensor([[0.1068]], grad_fn=<AddmmBackward0>) tensor(0.1130)\n",
      "tensor([[-0.6685]], grad_fn=<AddmmBackward0>) tensor(-0.8810)\n",
      "tensor([[-1.4287]], grad_fn=<AddmmBackward0>) tensor(-0.9611)\n",
      "tensor([[-1.0508]], grad_fn=<AddmmBackward0>) tensor(-0.9847)\n",
      "tensor([[0.5411]], grad_fn=<AddmmBackward0>) tensor(0.5352)\n",
      "tensor([[-0.3769]], grad_fn=<AddmmBackward0>) tensor(0.0201)\n",
      "tensor([[-0.5429]], grad_fn=<AddmmBackward0>) tensor(-0.8545)\n",
      "tensor([[0.0509]], grad_fn=<AddmmBackward0>) tensor(0.3172)\n",
      "tensor([[-0.5418]], grad_fn=<AddmmBackward0>) tensor(-0.5121)\n",
      "tensor([[-0.5966]], grad_fn=<AddmmBackward0>) tensor(-0.7423)\n",
      "tensor([[0.9324]], grad_fn=<AddmmBackward0>) tensor(0.5524)\n",
      "tensor([[-0.9570]], grad_fn=<AddmmBackward0>) tensor(-0.5626)\n",
      "tensor([[-0.1144]], grad_fn=<AddmmBackward0>) tensor(-0.0261)\n",
      "tensor([[-0.8028]], grad_fn=<AddmmBackward0>) tensor(-0.4835)\n",
      "tensor([[-0.4769]], grad_fn=<AddmmBackward0>) tensor(0.2660)\n",
      "tensor([[0.5166]], grad_fn=<AddmmBackward0>) tensor(0.5554)\n",
      "tensor([[-1.0903]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.1374]], grad_fn=<AddmmBackward0>) tensor(0.7356)\n",
      "tensor([[-0.8141]], grad_fn=<AddmmBackward0>) tensor(-0.9651)\n",
      "tensor([[0.3651]], grad_fn=<AddmmBackward0>) tensor(0.6540)\n",
      "tensor([[-0.4688]], grad_fn=<AddmmBackward0>) tensor(-0.9410)\n",
      "tensor([[0.5092]], grad_fn=<AddmmBackward0>) tensor(0.7975)\n",
      "tensor([[-0.1693]], grad_fn=<AddmmBackward0>) tensor(0.3246)\n",
      "tensor([[-0.7679]], grad_fn=<AddmmBackward0>) tensor(-0.4484)\n",
      "tensor([[-0.5049]], grad_fn=<AddmmBackward0>) tensor(-0.3001)\n",
      "tensor([[-0.5225]], grad_fn=<AddmmBackward0>) tensor(-0.4785)\n",
      "tensor([[-0.5782]], grad_fn=<AddmmBackward0>) tensor(-0.4824)\n",
      "tensor([[-0.1890]], grad_fn=<AddmmBackward0>) tensor(-0.2498)\n",
      "tensor([[-0.5682]], grad_fn=<AddmmBackward0>) tensor(-0.9261)\n",
      "tensor([[-0.6931]], grad_fn=<AddmmBackward0>) tensor(-0.3013)\n",
      "tensor([[0.5367]], grad_fn=<AddmmBackward0>) tensor(0.5801)\n",
      "tensor([[-0.6018]], grad_fn=<AddmmBackward0>) tensor(-0.9344)\n",
      "tensor([[-0.1589]], grad_fn=<AddmmBackward0>) tensor(-0.1577)\n",
      "tensor([[-0.6487]], grad_fn=<AddmmBackward0>) tensor(-0.5977)\n",
      "tensor([[-0.2417]], grad_fn=<AddmmBackward0>) tensor(0.0352)\n",
      "tensor([[-0.6042]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.1047]], grad_fn=<AddmmBackward0>) tensor(-0.1164)\n",
      "tensor([[-0.5319]], grad_fn=<AddmmBackward0>) tensor(-0.9524)\n",
      "tensor([[-0.6613]], grad_fn=<AddmmBackward0>) tensor(-0.7985)\n",
      "tensor([[-0.5546]], grad_fn=<AddmmBackward0>) tensor(-0.2777)\n",
      "tensor([[0.4846]], grad_fn=<AddmmBackward0>) tensor(0.5386)\n",
      "tensor([[-0.6483]], grad_fn=<AddmmBackward0>) tensor(-0.7237)\n",
      "tensor([[0.3838]], grad_fn=<AddmmBackward0>) tensor(0.5734)\n",
      "tensor([[0.2997]], grad_fn=<AddmmBackward0>) tensor(0.4093)\n",
      "tensor([[0.5240]], grad_fn=<AddmmBackward0>) tensor(0.6902)\n",
      "tensor([[-0.0443]], grad_fn=<AddmmBackward0>) tensor(-0.1398)\n",
      "tensor([[-0.7545]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.8230]], grad_fn=<AddmmBackward0>) tensor(-0.8490)\n",
      "tensor([[0.0403]], grad_fn=<AddmmBackward0>) tensor(-0.0545)\n",
      "tensor([[0.1200]], grad_fn=<AddmmBackward0>) tensor(0.8148)\n",
      "tensor([[-0.7730]], grad_fn=<AddmmBackward0>) tensor(-0.5783)\n",
      "tensor([[-0.4039]], grad_fn=<AddmmBackward0>) tensor(-0.1028)\n",
      "tensor([[-0.2126]], grad_fn=<AddmmBackward0>) tensor(-0.4269)\n",
      "tensor([[0.6657]], grad_fn=<AddmmBackward0>) tensor(0.5977)\n",
      "tensor([[0.2869]], grad_fn=<AddmmBackward0>) tensor(0.5512)\n",
      "tensor([[-0.1396]], grad_fn=<AddmmBackward0>) tensor(0.0346)\n",
      "tensor([[0.1640]], grad_fn=<AddmmBackward0>) tensor(0.5991)\n",
      "tensor([[0.6125]], grad_fn=<AddmmBackward0>) tensor(0.5264)\n",
      "tensor([[-0.7046]], grad_fn=<AddmmBackward0>) tensor(-0.7993)\n",
      "tensor([[-0.4069]], grad_fn=<AddmmBackward0>) tensor(-0.5592)\n",
      "tensor([[-0.7520]], grad_fn=<AddmmBackward0>) tensor(-0.8037)\n",
      "tensor([[-0.9229]], grad_fn=<AddmmBackward0>) tensor(-0.8456)\n",
      "tensor([[-0.2019]], grad_fn=<AddmmBackward0>) tensor(0.6091)\n",
      "tensor([[-0.0571]], grad_fn=<AddmmBackward0>) tensor(-0.1361)\n",
      "tensor([[-0.3682]], grad_fn=<AddmmBackward0>) tensor(-0.3568)\n",
      "tensor([[-0.4949]], grad_fn=<AddmmBackward0>) tensor(-0.4379)\n",
      "tensor([[0.0912]], grad_fn=<AddmmBackward0>) tensor(0.6176)\n",
      "tensor([[-0.5873]], grad_fn=<AddmmBackward0>) tensor(-0.6498)\n",
      "tensor([[-0.6956]], grad_fn=<AddmmBackward0>) tensor(-0.8479)\n",
      "tensor([[0.0679]], grad_fn=<AddmmBackward0>) tensor(0.6510)\n",
      "tensor([[-0.3968]], grad_fn=<AddmmBackward0>) tensor(-0.2707)\n",
      "tensor([[-0.6366]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.6565]], grad_fn=<AddmmBackward0>) tensor(-0.6444)\n",
      "tensor([[-0.3911]], grad_fn=<AddmmBackward0>) tensor(-0.3328)\n",
      "tensor([[-0.3281]], grad_fn=<AddmmBackward0>) tensor(-0.2429)\n",
      "tensor([[-1.1437]], grad_fn=<AddmmBackward0>) tensor(-0.8828)\n",
      "tensor([[0.2673]], grad_fn=<AddmmBackward0>) tensor(0.7960)\n",
      "tensor([[0.1098]], grad_fn=<AddmmBackward0>) tensor(0.2814)\n",
      "tensor([[-0.3327]], grad_fn=<AddmmBackward0>) tensor(-0.1850)\n",
      "tensor([[-0.7317]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.2476]], grad_fn=<AddmmBackward0>) tensor(-0.1779)\n",
      "tensor([[0.7443]], grad_fn=<AddmmBackward0>) tensor(0.5389)\n",
      "tensor([[0.5228]], grad_fn=<AddmmBackward0>) tensor(0.7469)\n",
      "tensor([[-0.1785]], grad_fn=<AddmmBackward0>) tensor(0.1922)\n",
      "tensor([[0.7641]], grad_fn=<AddmmBackward0>) tensor(0.8234)\n",
      "tensor([[-0.3869]], grad_fn=<AddmmBackward0>) tensor(-0.2603)\n",
      "tensor([[-0.0178]], grad_fn=<AddmmBackward0>) tensor(0.2751)\n",
      "tensor([[-0.6940]], grad_fn=<AddmmBackward0>) tensor(-0.6511)\n",
      "tensor([[0.0792]], grad_fn=<AddmmBackward0>) tensor(0.7162)\n",
      "tensor([[0.5394]], grad_fn=<AddmmBackward0>) tensor(0.5249)\n",
      "tensor([[-0.4377]], grad_fn=<AddmmBackward0>) tensor(-0.8334)\n",
      "tensor([[-0.1936]], grad_fn=<AddmmBackward0>) tensor(-0.8571)\n",
      "tensor([[0.8268]], grad_fn=<AddmmBackward0>) tensor(0.6434)\n",
      "tensor([[0.3256]], grad_fn=<AddmmBackward0>) tensor(0.9687)\n",
      "tensor([[-0.6059]], grad_fn=<AddmmBackward0>) tensor(-0.4888)\n",
      "tensor([[0.2133]], grad_fn=<AddmmBackward0>) tensor(0.3720)\n",
      "tensor([[0.7005]], grad_fn=<AddmmBackward0>) tensor(0.7467)\n",
      "tensor([[-0.0131]], grad_fn=<AddmmBackward0>) tensor(0.0052)\n",
      "tensor([[0.4726]], grad_fn=<AddmmBackward0>) tensor(0.6011)\n",
      "tensor([[-0.1062]], grad_fn=<AddmmBackward0>) tensor(-0.2423)\n",
      "tensor([[-0.7313]], grad_fn=<AddmmBackward0>) tensor(-0.5807)\n",
      "tensor([[-0.7247]], grad_fn=<AddmmBackward0>) tensor(-0.5265)\n",
      "tensor([[0.4497]], grad_fn=<AddmmBackward0>) tensor(0.5693)\n",
      "tensor([[-0.9721]], grad_fn=<AddmmBackward0>) tensor(-0.7243)\n",
      "tensor([[-0.8164]], grad_fn=<AddmmBackward0>) tensor(-0.7952)\n",
      "tensor([[-0.9942]], grad_fn=<AddmmBackward0>) tensor(-0.4322)\n",
      "tensor([[-0.6438]], grad_fn=<AddmmBackward0>) tensor(-0.6481)\n",
      "tensor([[-0.6180]], grad_fn=<AddmmBackward0>) tensor(-0.7452)\n",
      "tensor([[-0.4317]], grad_fn=<AddmmBackward0>) tensor(-0.6231)\n",
      "tensor([[-0.5620]], grad_fn=<AddmmBackward0>) tensor(-0.7020)\n",
      "tensor([[-0.3835]], grad_fn=<AddmmBackward0>) tensor(-0.1673)\n",
      "tensor([[-0.8899]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.4011]], grad_fn=<AddmmBackward0>) tensor(0.6818)\n",
      "tensor([[-0.1619]], grad_fn=<AddmmBackward0>) tensor(-0.2542)\n",
      "tensor([[0.4070]], grad_fn=<AddmmBackward0>) tensor(0.5436)\n",
      "tensor([[-0.0723]], grad_fn=<AddmmBackward0>) tensor(0.8334)\n",
      "tensor([[0.0478]], grad_fn=<AddmmBackward0>) tensor(-0.1706)\n",
      "tensor([[-0.5179]], grad_fn=<AddmmBackward0>) tensor(-0.5692)\n",
      "tensor([[-0.9632]], grad_fn=<AddmmBackward0>) tensor(-0.8809)\n",
      "tensor([[-0.5673]], grad_fn=<AddmmBackward0>) tensor(-0.0368)\n",
      "tensor([[-0.4113]], grad_fn=<AddmmBackward0>) tensor(-0.1113)\n",
      "tensor([[0.1830]], grad_fn=<AddmmBackward0>) tensor(0.8094)\n",
      "tensor([[0.0312]], grad_fn=<AddmmBackward0>) tensor(0.3141)\n",
      "tensor([[-0.8053]], grad_fn=<AddmmBackward0>) tensor(-0.2882)\n",
      "tensor([[-0.5803]], grad_fn=<AddmmBackward0>) tensor(-0.3198)\n",
      "tensor([[-0.1407]], grad_fn=<AddmmBackward0>) tensor(0.1436)\n",
      "tensor([[-0.5899]], grad_fn=<AddmmBackward0>) tensor(-0.0772)\n",
      "tensor([[-0.4058]], grad_fn=<AddmmBackward0>) tensor(-0.2439)\n",
      "tensor([[0.3808]], grad_fn=<AddmmBackward0>) tensor(0.5158)\n",
      "tensor([[-0.5144]], grad_fn=<AddmmBackward0>) tensor(-0.4246)\n",
      "tensor([[0.2604]], grad_fn=<AddmmBackward0>) tensor(0.5154)\n",
      "tensor([[0.3677]], grad_fn=<AddmmBackward0>) tensor(0.5156)\n",
      "tensor([[0.3957]], grad_fn=<AddmmBackward0>) tensor(0.5785)\n",
      "tensor([[-0.4359]], grad_fn=<AddmmBackward0>) tensor(-0.5608)\n",
      "tensor([[-0.5813]], grad_fn=<AddmmBackward0>) tensor(-0.4497)\n",
      "tensor([[-0.7992]], grad_fn=<AddmmBackward0>) tensor(-0.9283)\n",
      "tensor([[-0.5077]], grad_fn=<AddmmBackward0>) tensor(-0.6817)\n",
      "tensor([[-0.3609]], grad_fn=<AddmmBackward0>) tensor(-0.5013)\n",
      "tensor([[-0.4467]], grad_fn=<AddmmBackward0>) tensor(-0.6464)\n",
      "tensor([[-0.7585]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.0477]], grad_fn=<AddmmBackward0>) tensor(0.5567)\n",
      "tensor([[-0.9007]], grad_fn=<AddmmBackward0>) tensor(-0.8499)\n",
      "tensor([[-0.7041]], grad_fn=<AddmmBackward0>) tensor(-0.8237)\n",
      "tensor([[-0.0600]], grad_fn=<AddmmBackward0>) tensor(-0.1994)\n",
      "tensor([[-1.0373]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.3745]], grad_fn=<AddmmBackward0>) tensor(0.5749)\n",
      "tensor([[0.9098]], grad_fn=<AddmmBackward0>) tensor(0.7527)\n",
      "tensor([[0.5275]], grad_fn=<AddmmBackward0>) tensor(0.6023)\n",
      "tensor([[-0.8491]], grad_fn=<AddmmBackward0>) tensor(-0.1849)\n",
      "tensor([[-0.6693]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.6327]], grad_fn=<AddmmBackward0>) tensor(0.7527)\n",
      "tensor([[-0.7249]], grad_fn=<AddmmBackward0>) tensor(-0.6210)\n",
      "tensor([[-0.0212]], grad_fn=<AddmmBackward0>) tensor(0.1293)\n",
      "tensor([[-0.5261]], grad_fn=<AddmmBackward0>) tensor(-0.2365)\n",
      "tensor([[0.5846]], grad_fn=<AddmmBackward0>) tensor(0.8331)\n",
      "tensor([[0.4930]], grad_fn=<AddmmBackward0>) tensor(0.5058)\n",
      "tensor([[-0.0004]], grad_fn=<AddmmBackward0>) tensor(0.1724)\n",
      "tensor([[-0.5600]], grad_fn=<AddmmBackward0>) tensor(-0.4402)\n",
      "tensor([[-0.5906]], grad_fn=<AddmmBackward0>) tensor(-0.4952)\n",
      "tensor([[-0.5840]], grad_fn=<AddmmBackward0>) tensor(-0.8572)\n",
      "tensor([[0.8969]], grad_fn=<AddmmBackward0>) tensor(0.5781)\n",
      "tensor([[-0.0761]], grad_fn=<AddmmBackward0>) tensor(0.7312)\n",
      "tensor([[0.5886]], grad_fn=<AddmmBackward0>) tensor(0.5630)\n",
      "tensor([[-0.7166]], grad_fn=<AddmmBackward0>) tensor(-0.4707)\n",
      "tensor([[-0.5137]], grad_fn=<AddmmBackward0>) tensor(-0.6469)\n",
      "tensor([[-0.4658]], grad_fn=<AddmmBackward0>) tensor(0.3437)\n",
      "tensor([[-0.6411]], grad_fn=<AddmmBackward0>) tensor(-0.7952)\n",
      "tensor([[0.0532]], grad_fn=<AddmmBackward0>) tensor(0.2106)\n",
      "tensor([[-0.7383]], grad_fn=<AddmmBackward0>) tensor(-0.8433)\n",
      "tensor([[-0.6843]], grad_fn=<AddmmBackward0>) tensor(-0.2755)\n",
      "tensor([[0.2075]], grad_fn=<AddmmBackward0>) tensor(0.5756)\n",
      "tensor([[0.3188]], grad_fn=<AddmmBackward0>) tensor(0.3581)\n",
      "tensor([[-0.2293]], grad_fn=<AddmmBackward0>) tensor(-0.4863)\n",
      "tensor([[-0.6909]], grad_fn=<AddmmBackward0>) tensor(-0.4370)\n",
      "tensor([[-0.5173]], grad_fn=<AddmmBackward0>) tensor(-0.3489)\n",
      "tensor([[-0.4322]], grad_fn=<AddmmBackward0>) tensor(-0.6852)\n",
      "tensor([[-0.1217]], grad_fn=<AddmmBackward0>) tensor(0.3277)\n",
      "tensor([[-0.5259]], grad_fn=<AddmmBackward0>) tensor(-0.2482)\n",
      "tensor([[-0.2928]], grad_fn=<AddmmBackward0>) tensor(-0.3271)\n",
      "tensor([[-0.6545]], grad_fn=<AddmmBackward0>) tensor(-0.7395)\n",
      "tensor([[0.4059]], grad_fn=<AddmmBackward0>) tensor(0.6212)\n",
      "tensor([[0.1698]], grad_fn=<AddmmBackward0>) tensor(0.1170)\n",
      "tensor([[-0.0790]], grad_fn=<AddmmBackward0>) tensor(0.0132)\n",
      "tensor([[-0.0894]], grad_fn=<AddmmBackward0>) tensor(0.2980)\n",
      "tensor([[0.5570]], grad_fn=<AddmmBackward0>) tensor(0.7316)\n",
      "tensor([[-0.5347]], grad_fn=<AddmmBackward0>) tensor(-0.8552)\n",
      "tensor([[-0.6858]], grad_fn=<AddmmBackward0>) tensor(-0.6924)\n",
      "tensor([[0.3122]], grad_fn=<AddmmBackward0>) tensor(0.5600)\n",
      "tensor([[0.1541]], grad_fn=<AddmmBackward0>) tensor(0.6841)\n",
      "tensor([[-0.0012]], grad_fn=<AddmmBackward0>) tensor(-0.0599)\n",
      "tensor([[0.4297]], grad_fn=<AddmmBackward0>) tensor(-0.1829)\n",
      "tensor([[-0.7687]], grad_fn=<AddmmBackward0>) tensor(-0.8371)\n",
      "tensor([[0.5318]], grad_fn=<AddmmBackward0>) tensor(0.2367)\n",
      "tensor([[0.3172]], grad_fn=<AddmmBackward0>) tensor(0.0472)\n",
      "tensor([[0.4545]], grad_fn=<AddmmBackward0>) tensor(0.5564)\n",
      "tensor([[0.4531]], grad_fn=<AddmmBackward0>) tensor(0.4076)\n",
      "tensor([[-0.7332]], grad_fn=<AddmmBackward0>) tensor(-0.4894)\n",
      "tensor([[-0.6393]], grad_fn=<AddmmBackward0>) tensor(-0.4256)\n",
      "tensor([[-0.5586]], grad_fn=<AddmmBackward0>) tensor(-0.6191)\n",
      "tensor([[0.3592]], grad_fn=<AddmmBackward0>) tensor(0.5842)\n",
      "tensor([[0.0681]], grad_fn=<AddmmBackward0>) tensor(0.2725)\n",
      "tensor([[-0.9795]], grad_fn=<AddmmBackward0>) tensor(-0.8772)\n",
      "tensor([[-0.6437]], grad_fn=<AddmmBackward0>) tensor(-0.8003)\n",
      "tensor([[0.2999]], grad_fn=<AddmmBackward0>) tensor(0.3652)\n",
      "tensor([[0.1646]], grad_fn=<AddmmBackward0>) tensor(0.5382)\n",
      "tensor([[0.6614]], grad_fn=<AddmmBackward0>) tensor(0.5521)\n",
      "tensor([[0.1664]], grad_fn=<AddmmBackward0>) tensor(0.3055)\n",
      "tensor([[0.0171]], grad_fn=<AddmmBackward0>) tensor(0.6144)\n",
      "tensor([[-0.6044]], grad_fn=<AddmmBackward0>) tensor(-0.3218)\n",
      "tensor([[-0.4879]], grad_fn=<AddmmBackward0>) tensor(-0.5109)\n",
      "tensor([[-0.9515]], grad_fn=<AddmmBackward0>) tensor(-0.9595)\n",
      "tensor([[-0.1248]], grad_fn=<AddmmBackward0>) tensor(-0.0201)\n",
      "tensor([[0.4620]], grad_fn=<AddmmBackward0>) tensor(0.6578)\n",
      "tensor([[-0.1335]], grad_fn=<AddmmBackward0>) tensor(0.0831)\n",
      "tensor([[-0.1351]], grad_fn=<AddmmBackward0>) tensor(-0.3239)\n",
      "tensor([[-0.8316]], grad_fn=<AddmmBackward0>) tensor(-0.8028)\n",
      "tensor([[-0.5826]], grad_fn=<AddmmBackward0>) tensor(-0.5698)\n",
      "tensor([[-0.6183]], grad_fn=<AddmmBackward0>) tensor(-0.4651)\n",
      "tensor([[-0.4002]], grad_fn=<AddmmBackward0>) tensor(-0.5428)\n",
      "tensor([[0.0098]], grad_fn=<AddmmBackward0>) tensor(0.0003)\n",
      "tensor([[-1.0020]], grad_fn=<AddmmBackward0>) tensor(-0.9270)\n",
      "tensor([[-0.5428]], grad_fn=<AddmmBackward0>) tensor(-0.3596)\n",
      "tensor([[-0.7994]], grad_fn=<AddmmBackward0>) tensor(-0.4178)\n",
      "tensor([[-1.0734]], grad_fn=<AddmmBackward0>) tensor(-0.6109)\n",
      "tensor([[-0.4716]], grad_fn=<AddmmBackward0>) tensor(-0.5130)\n",
      "tensor([[-0.0775]], grad_fn=<AddmmBackward0>) tensor(0.0275)\n",
      "tensor([[-0.1720]], grad_fn=<AddmmBackward0>) tensor(-0.1066)\n",
      "tensor([[-0.3817]], grad_fn=<AddmmBackward0>) tensor(-0.5376)\n",
      "tensor([[-0.0472]], grad_fn=<AddmmBackward0>) tensor(0.0010)\n",
      "tensor([[-0.3033]], grad_fn=<AddmmBackward0>) tensor(0.0985)\n",
      "tensor([[-0.0053]], grad_fn=<AddmmBackward0>) tensor(-0.2133)\n",
      "tensor([[-0.8095]], grad_fn=<AddmmBackward0>) tensor(-0.5757)\n",
      "tensor([[0.4886]], grad_fn=<AddmmBackward0>) tensor(0.5723)\n",
      "tensor([[-0.5191]], grad_fn=<AddmmBackward0>) tensor(-0.5603)\n",
      "tensor([[-0.5463]], grad_fn=<AddmmBackward0>) tensor(-0.6803)\n",
      "tensor([[-0.3412]], grad_fn=<AddmmBackward0>) tensor(-0.3675)\n",
      "tensor([[0.2374]], grad_fn=<AddmmBackward0>) tensor(0.5441)\n",
      "tensor([[-0.2013]], grad_fn=<AddmmBackward0>) tensor(-0.1968)\n",
      "tensor([[-0.0208]], grad_fn=<AddmmBackward0>) tensor(0.4156)\n",
      "tensor([[-0.5326]], grad_fn=<AddmmBackward0>) tensor(-0.5590)\n",
      "tensor([[-0.0168]], grad_fn=<AddmmBackward0>) tensor(-0.0453)\n",
      "tensor([[0.4503]], grad_fn=<AddmmBackward0>) tensor(0.5681)\n",
      "tensor([[-0.5357]], grad_fn=<AddmmBackward0>) tensor(-0.5036)\n",
      "tensor([[0.2331]], grad_fn=<AddmmBackward0>) tensor(0.8023)\n",
      "tensor([[-0.6809]], grad_fn=<AddmmBackward0>) tensor(-0.3799)\n",
      "tensor([[-0.4326]], grad_fn=<AddmmBackward0>) tensor(-0.3854)\n",
      "tensor([[0.2717]], grad_fn=<AddmmBackward0>) tensor(0.8592)\n",
      "tensor([[-0.6676]], grad_fn=<AddmmBackward0>) tensor(-0.4903)\n",
      "tensor([[-0.1367]], grad_fn=<AddmmBackward0>) tensor(-0.0492)\n",
      "tensor([[0.3481]], grad_fn=<AddmmBackward0>) tensor(0.4017)\n",
      "tensor([[0.5137]], grad_fn=<AddmmBackward0>) tensor(0.5717)\n",
      "tensor([[-0.5176]], grad_fn=<AddmmBackward0>) tensor(-0.7399)\n",
      "tensor([[-0.7525]], grad_fn=<AddmmBackward0>) tensor(-0.8681)\n",
      "tensor([[-0.7422]], grad_fn=<AddmmBackward0>) tensor(-0.4295)\n",
      "tensor([[0.3921]], grad_fn=<AddmmBackward0>) tensor(0.5510)\n",
      "tensor([[0.3373]], grad_fn=<AddmmBackward0>) tensor(0.3938)\n",
      "tensor([[-0.6449]], grad_fn=<AddmmBackward0>) tensor(-0.4786)\n",
      "tensor([[-0.2143]], grad_fn=<AddmmBackward0>) tensor(0.8171)\n",
      "tensor([[-1.0398]], grad_fn=<AddmmBackward0>) tensor(-0.7445)\n",
      "tensor([[-0.4367]], grad_fn=<AddmmBackward0>) tensor(-0.1257)\n",
      "tensor([[0.4972]], grad_fn=<AddmmBackward0>) tensor(0.5211)\n",
      "tensor([[0.2978]], grad_fn=<AddmmBackward0>) tensor(0.8257)\n",
      "tensor([[0.1136]], grad_fn=<AddmmBackward0>) tensor(0.4906)\n",
      "tensor([[0.4976]], grad_fn=<AddmmBackward0>) tensor(0.6217)\n",
      "tensor([[-0.7279]], grad_fn=<AddmmBackward0>) tensor(-0.8893)\n",
      "tensor([[0.5704]], grad_fn=<AddmmBackward0>) tensor(0.5541)\n",
      "tensor([[-0.0494]], grad_fn=<AddmmBackward0>) tensor(0.1404)\n",
      "tensor([[0.3962]], grad_fn=<AddmmBackward0>) tensor(0.5412)\n",
      "tensor([[-0.5053]], grad_fn=<AddmmBackward0>) tensor(-0.4896)\n",
      "tensor([[0.4493]], grad_fn=<AddmmBackward0>) tensor(0.4473)\n",
      "tensor([[0.2867]], grad_fn=<AddmmBackward0>) tensor(0.5734)\n",
      "tensor([[-0.6141]], grad_fn=<AddmmBackward0>) tensor(-0.3639)\n",
      "tensor([[0.5398]], grad_fn=<AddmmBackward0>) tensor(0.8383)\n",
      "tensor([[0.4799]], grad_fn=<AddmmBackward0>) tensor(0.5908)\n",
      "tensor([[-0.3323]], grad_fn=<AddmmBackward0>) tensor(-0.6290)\n",
      "tensor([[-0.7667]], grad_fn=<AddmmBackward0>) tensor(-0.7750)\n",
      "tensor([[-0.0419]], grad_fn=<AddmmBackward0>) tensor(0.4365)\n",
      "tensor([[0.2176]], grad_fn=<AddmmBackward0>) tensor(0.5333)\n",
      "tensor([[0.5013]], grad_fn=<AddmmBackward0>) tensor(0.5745)\n",
      "tensor([[0.6321]], grad_fn=<AddmmBackward0>) tensor(0.5445)\n",
      "tensor([[-0.8709]], grad_fn=<AddmmBackward0>) tensor(-0.8672)\n",
      "tensor([[-0.4694]], grad_fn=<AddmmBackward0>) tensor(-0.5541)\n",
      "tensor([[-0.0564]], grad_fn=<AddmmBackward0>) tensor(-0.0060)\n",
      "tensor([[-0.4206]], grad_fn=<AddmmBackward0>) tensor(-0.4594)\n",
      "tensor([[0.4936]], grad_fn=<AddmmBackward0>) tensor(0.7357)\n",
      "tensor([[-1.0911]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-1.0256]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.8705]], grad_fn=<AddmmBackward0>) tensor(-0.9693)\n",
      "tensor([[-0.0308]], grad_fn=<AddmmBackward0>) tensor(0.2848)\n",
      "tensor([[0.2286]], grad_fn=<AddmmBackward0>) tensor(0.4182)\n",
      "tensor([[0.5771]], grad_fn=<AddmmBackward0>) tensor(0.5197)\n",
      "tensor([[-0.0892]], grad_fn=<AddmmBackward0>) tensor(0.1590)\n",
      "tensor([[0.3617]], grad_fn=<AddmmBackward0>) tensor(0.5284)\n",
      "tensor([[0.5888]], grad_fn=<AddmmBackward0>) tensor(0.5338)\n",
      "tensor([[-0.0831]], grad_fn=<AddmmBackward0>) tensor(0.2803)\n",
      "tensor([[-0.8091]], grad_fn=<AddmmBackward0>) tensor(-0.9298)\n",
      "tensor([[-0.1192]], grad_fn=<AddmmBackward0>) tensor(0.8209)\n",
      "tensor([[-0.6496]], grad_fn=<AddmmBackward0>) tensor(-0.8983)\n",
      "tensor([[1.0223]], grad_fn=<AddmmBackward0>) tensor(0.7418)\n",
      "tensor([[-0.4887]], grad_fn=<AddmmBackward0>) tensor(-0.6465)\n",
      "tensor([[-0.1519]], grad_fn=<AddmmBackward0>) tensor(0.1455)\n",
      "tensor([[0.4123]], grad_fn=<AddmmBackward0>) tensor(0.2869)\n",
      "tensor([[0.7520]], grad_fn=<AddmmBackward0>) tensor(0.3635)\n",
      "tensor([[-0.6935]], grad_fn=<AddmmBackward0>) tensor(-0.7219)\n",
      "tensor([[-0.7771]], grad_fn=<AddmmBackward0>) tensor(-0.8614)\n",
      "tensor([[-0.7626]], grad_fn=<AddmmBackward0>) tensor(-0.7312)\n",
      "tensor([[-0.7943]], grad_fn=<AddmmBackward0>) tensor(-0.6503)\n",
      "tensor([[0.4623]], grad_fn=<AddmmBackward0>) tensor(0.5452)\n",
      "tensor([[-0.6209]], grad_fn=<AddmmBackward0>) tensor(-0.8551)\n",
      "tensor([[-0.0112]], grad_fn=<AddmmBackward0>) tensor(0.8874)\n",
      "tensor([[-0.9446]], grad_fn=<AddmmBackward0>) tensor(-0.7881)\n",
      "tensor([[-0.1709]], grad_fn=<AddmmBackward0>) tensor(-0.0590)\n",
      "tensor([[0.0891]], grad_fn=<AddmmBackward0>) tensor(0.2622)\n",
      "tensor([[-0.5410]], grad_fn=<AddmmBackward0>) tensor(-0.3215)\n",
      "tensor([[-0.9221]], grad_fn=<AddmmBackward0>) tensor(-0.3857)\n",
      "tensor([[-0.5857]], grad_fn=<AddmmBackward0>) tensor(-0.2902)\n",
      "tensor([[0.1067]], grad_fn=<AddmmBackward0>) tensor(0.8853)\n",
      "tensor([[-0.4649]], grad_fn=<AddmmBackward0>) tensor(-0.6380)\n",
      "tensor([[-0.0453]], grad_fn=<AddmmBackward0>) tensor(-0.1356)\n",
      "tensor([[0.5046]], grad_fn=<AddmmBackward0>) tensor(0.7562)\n",
      "tensor([[-0.5590]], grad_fn=<AddmmBackward0>) tensor(-0.7474)\n",
      "tensor([[-0.7459]], grad_fn=<AddmmBackward0>) tensor(-0.8447)\n",
      "tensor([[-0.6263]], grad_fn=<AddmmBackward0>) tensor(0.2125)\n",
      "tensor([[-0.7225]], grad_fn=<AddmmBackward0>) tensor(-0.2231)\n",
      "tensor([[-0.7209]], grad_fn=<AddmmBackward0>) tensor(-0.1123)\n",
      "tensor([[-0.5156]], grad_fn=<AddmmBackward0>) tensor(-0.5448)\n",
      "tensor([[-0.3382]], grad_fn=<AddmmBackward0>) tensor(-0.6219)\n",
      "tensor([[0.1110]], grad_fn=<AddmmBackward0>) tensor(0.2258)\n",
      "tensor([[-0.0941]], grad_fn=<AddmmBackward0>) tensor(-0.1325)\n",
      "tensor([[-0.8192]], grad_fn=<AddmmBackward0>) tensor(-0.8721)\n",
      "tensor([[-0.9411]], grad_fn=<AddmmBackward0>) tensor(-0.8940)\n",
      "tensor([[0.9502]], grad_fn=<AddmmBackward0>) tensor(0.7086)\n",
      "tensor([[0.4352]], grad_fn=<AddmmBackward0>) tensor(0.7492)\n",
      "tensor([[-0.5913]], grad_fn=<AddmmBackward0>) tensor(-0.4934)\n",
      "tensor([[-0.7143]], grad_fn=<AddmmBackward0>) tensor(-0.4127)\n",
      "tensor([[-0.6834]], grad_fn=<AddmmBackward0>) tensor(-0.8863)\n",
      "tensor([[-0.6993]], grad_fn=<AddmmBackward0>) tensor(-0.5930)\n",
      "tensor([[-0.6209]], grad_fn=<AddmmBackward0>) tensor(-0.5496)\n",
      "tensor([[-0.5160]], grad_fn=<AddmmBackward0>) tensor(-0.8062)\n",
      "tensor([[0.3071]], grad_fn=<AddmmBackward0>) tensor(0.5789)\n",
      "tensor([[0.2756]], grad_fn=<AddmmBackward0>) tensor(0.6034)\n",
      "tensor([[0.1982]], grad_fn=<AddmmBackward0>) tensor(0.1772)\n",
      "tensor([[0.1129]], grad_fn=<AddmmBackward0>) tensor(0.5610)\n",
      "tensor([[0.5611]], grad_fn=<AddmmBackward0>) tensor(0.7971)\n",
      "tensor([[0.0300]], grad_fn=<AddmmBackward0>) tensor(0.7504)\n",
      "tensor([[-0.5565]], grad_fn=<AddmmBackward0>) tensor(-0.4288)\n",
      "tensor([[0.1915]], grad_fn=<AddmmBackward0>) tensor(0.2085)\n",
      "tensor([[-0.2725]], grad_fn=<AddmmBackward0>) tensor(0.3423)\n",
      "tensor([[0.3254]], grad_fn=<AddmmBackward0>) tensor(0.6259)\n",
      "tensor([[-1.0674]], grad_fn=<AddmmBackward0>) tensor(0.4092)\n",
      "tensor([[-0.6353]], grad_fn=<AddmmBackward0>) tensor(-0.7063)\n",
      "tensor([[-0.6807]], grad_fn=<AddmmBackward0>) tensor(-0.4750)\n",
      "tensor([[-0.3466]], grad_fn=<AddmmBackward0>) tensor(-0.3596)\n",
      "tensor([[-0.7642]], grad_fn=<AddmmBackward0>) tensor(-0.6217)\n",
      "tensor([[0.0847]], grad_fn=<AddmmBackward0>) tensor(0.1156)\n",
      "tensor([[-0.9021]], grad_fn=<AddmmBackward0>) tensor(-0.7104)\n",
      "tensor([[-0.6033]], grad_fn=<AddmmBackward0>) tensor(-0.4223)\n",
      "tensor([[-0.8005]], grad_fn=<AddmmBackward0>) tensor(-0.3646)\n",
      "tensor([[-0.6612]], grad_fn=<AddmmBackward0>) tensor(-0.8938)\n",
      "tensor([[-0.6112]], grad_fn=<AddmmBackward0>) tensor(-0.9130)\n",
      "tensor([[0.2599]], grad_fn=<AddmmBackward0>) tensor(0.7822)\n",
      "tensor([[-0.3807]], grad_fn=<AddmmBackward0>) tensor(-0.0500)\n",
      "tensor([[-0.6818]], grad_fn=<AddmmBackward0>) tensor(-0.5715)\n",
      "tensor([[0.6177]], grad_fn=<AddmmBackward0>) tensor(0.5675)\n",
      "tensor([[-0.1406]], grad_fn=<AddmmBackward0>) tensor(-0.2234)\n",
      "tensor([[0.2728]], grad_fn=<AddmmBackward0>) tensor(0.6995)\n",
      "tensor([[-0.4646]], grad_fn=<AddmmBackward0>) tensor(-0.7717)\n",
      "tensor([[0.5218]], grad_fn=<AddmmBackward0>) tensor(0.5403)\n",
      "tensor([[-0.5052]], grad_fn=<AddmmBackward0>) tensor(-0.5440)\n",
      "tensor([[-0.0692]], grad_fn=<AddmmBackward0>) tensor(0.0174)\n",
      "tensor([[0.2352]], grad_fn=<AddmmBackward0>) tensor(0.3582)\n",
      "tensor([[-0.6136]], grad_fn=<AddmmBackward0>) tensor(-0.7779)\n",
      "tensor([[-0.6486]], grad_fn=<AddmmBackward0>) tensor(-0.8753)\n",
      "tensor([[0.7773]], grad_fn=<AddmmBackward0>) tensor(0.7306)\n",
      "tensor([[0.3795]], grad_fn=<AddmmBackward0>) tensor(0.6753)\n",
      "tensor([[-0.3955]], grad_fn=<AddmmBackward0>) tensor(0.0229)\n",
      "tensor([[-0.5123]], grad_fn=<AddmmBackward0>) tensor(-0.5627)\n",
      "tensor([[-0.6766]], grad_fn=<AddmmBackward0>) tensor(-0.6352)\n",
      "tensor([[-0.6863]], grad_fn=<AddmmBackward0>) tensor(-0.3128)\n",
      "tensor([[0.4703]], grad_fn=<AddmmBackward0>) tensor(0.8032)\n",
      "tensor([[-0.7710]], grad_fn=<AddmmBackward0>) tensor(-0.3617)\n",
      "tensor([[-0.1145]], grad_fn=<AddmmBackward0>) tensor(0.4562)\n",
      "tensor([[-0.5974]], grad_fn=<AddmmBackward0>) tensor(-0.7621)\n",
      "tensor([[-0.8816]], grad_fn=<AddmmBackward0>) tensor(-0.8999)\n",
      "tensor([[0.3033]], grad_fn=<AddmmBackward0>) tensor(0.7095)\n",
      "tensor([[-0.7556]], grad_fn=<AddmmBackward0>) tensor(-0.7678)\n",
      "tensor([[-0.4646]], grad_fn=<AddmmBackward0>) tensor(-0.7542)\n",
      "tensor([[0.0557]], grad_fn=<AddmmBackward0>) tensor(0.1882)\n",
      "tensor([[-0.5987]], grad_fn=<AddmmBackward0>) tensor(0.6802)\n",
      "tensor([[-0.2523]], grad_fn=<AddmmBackward0>) tensor(-0.1653)\n",
      "tensor([[-0.1564]], grad_fn=<AddmmBackward0>) tensor(0.3142)\n",
      "tensor([[-0.9579]], grad_fn=<AddmmBackward0>) tensor(-0.8628)\n",
      "tensor([[-0.7116]], grad_fn=<AddmmBackward0>) tensor(-0.8930)\n",
      "tensor([[-0.7522]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.1178]], grad_fn=<AddmmBackward0>) tensor(0.6147)\n",
      "tensor([[-0.6859]], grad_fn=<AddmmBackward0>) tensor(-0.7525)\n",
      "tensor([[-0.2768]], grad_fn=<AddmmBackward0>) tensor(0.3207)\n",
      "tensor([[0.3443]], grad_fn=<AddmmBackward0>) tensor(0.5823)\n",
      "tensor([[-0.6616]], grad_fn=<AddmmBackward0>) tensor(-0.7290)\n",
      "tensor([[0.2178]], grad_fn=<AddmmBackward0>) tensor(0.6809)\n",
      "tensor([[-0.6986]], grad_fn=<AddmmBackward0>) tensor(-0.8724)\n",
      "tensor([[-0.7334]], grad_fn=<AddmmBackward0>) tensor(-0.9073)\n",
      "tensor([[-0.5703]], grad_fn=<AddmmBackward0>) tensor(-0.1920)\n",
      "tensor([[-0.4802]], grad_fn=<AddmmBackward0>) tensor(-0.3628)\n",
      "tensor([[-0.2859]], grad_fn=<AddmmBackward0>) tensor(-0.6051)\n",
      "tensor([[-0.0055]], grad_fn=<AddmmBackward0>) tensor(-0.1893)\n",
      "tensor([[0.3272]], grad_fn=<AddmmBackward0>) tensor(0.5956)\n",
      "tensor([[0.3281]], grad_fn=<AddmmBackward0>) tensor(0.8464)\n",
      "tensor([[0.5078]], grad_fn=<AddmmBackward0>) tensor(0.5978)\n",
      "tensor([[-0.9121]], grad_fn=<AddmmBackward0>) tensor(-0.8650)\n",
      "tensor([[-0.1793]], grad_fn=<AddmmBackward0>) tensor(0.2317)\n",
      "tensor([[0.1620]], grad_fn=<AddmmBackward0>) tensor(0.4692)\n",
      "tensor([[-0.5423]], grad_fn=<AddmmBackward0>) tensor(-0.1635)\n",
      "tensor([[-0.6112]], grad_fn=<AddmmBackward0>) tensor(-0.6442)\n",
      "tensor([[-0.1513]], grad_fn=<AddmmBackward0>) tensor(-0.1949)\n",
      "tensor([[0.4039]], grad_fn=<AddmmBackward0>) tensor(0.5863)\n",
      "tensor([[-0.6483]], grad_fn=<AddmmBackward0>) tensor(-0.6732)\n",
      "tensor([[0.5872]], grad_fn=<AddmmBackward0>) tensor(0.1409)\n",
      "tensor([[0.4523]], grad_fn=<AddmmBackward0>) tensor(0.5553)\n",
      "tensor([[-0.5364]], grad_fn=<AddmmBackward0>) tensor(-0.3262)\n",
      "tensor([[0.3831]], grad_fn=<AddmmBackward0>) tensor(0.5615)\n",
      "tensor([[-0.5851]], grad_fn=<AddmmBackward0>) tensor(-0.5001)\n",
      "tensor([[-0.3998]], grad_fn=<AddmmBackward0>) tensor(-0.3274)\n",
      "tensor([[-0.5132]], grad_fn=<AddmmBackward0>) tensor(-0.9558)\n",
      "tensor([[-0.2748]], grad_fn=<AddmmBackward0>) tensor(-0.5041)\n",
      "tensor([[0.1880]], grad_fn=<AddmmBackward0>) tensor(0.6344)\n",
      "tensor([[0.0470]], grad_fn=<AddmmBackward0>) tensor(-0.0366)\n",
      "tensor([[-0.6891]], grad_fn=<AddmmBackward0>) tensor(-0.8569)\n",
      "tensor([[-0.8078]], grad_fn=<AddmmBackward0>) tensor(-0.8349)\n",
      "tensor([[-0.4485]], grad_fn=<AddmmBackward0>) tensor(-0.4011)\n",
      "tensor([[0.2589]], grad_fn=<AddmmBackward0>) tensor(0.1780)\n",
      "tensor([[0.8072]], grad_fn=<AddmmBackward0>) tensor(0.8011)\n",
      "tensor([[-0.7850]], grad_fn=<AddmmBackward0>) tensor(-0.6367)\n",
      "tensor([[-0.0475]], grad_fn=<AddmmBackward0>) tensor(0.0020)\n",
      "tensor([[-0.8872]], grad_fn=<AddmmBackward0>) tensor(-0.4758)\n",
      "tensor([[-0.0767]], grad_fn=<AddmmBackward0>) tensor(0.1948)\n",
      "tensor([[-0.7647]], grad_fn=<AddmmBackward0>) tensor(-0.6399)\n",
      "tensor([[-0.2857]], grad_fn=<AddmmBackward0>) tensor(0.1166)\n",
      "tensor([[-0.0920]], grad_fn=<AddmmBackward0>) tensor(-0.3951)\n",
      "tensor([[-0.6077]], grad_fn=<AddmmBackward0>) tensor(-0.4978)\n",
      "tensor([[-0.7962]], grad_fn=<AddmmBackward0>) tensor(-0.7979)\n",
      "tensor([[0.0401]], grad_fn=<AddmmBackward0>) tensor(-0.8042)\n",
      "tensor([[-0.5906]], grad_fn=<AddmmBackward0>) tensor(-0.4386)\n",
      "tensor([[-0.4781]], grad_fn=<AddmmBackward0>) tensor(-0.4277)\n",
      "tensor([[-0.8110]], grad_fn=<AddmmBackward0>) tensor(-0.9686)\n",
      "tensor([[-0.6176]], grad_fn=<AddmmBackward0>) tensor(-0.4738)\n",
      "tensor([[0.3876]], grad_fn=<AddmmBackward0>) tensor(-0.0957)\n",
      "tensor([[-0.0814]], grad_fn=<AddmmBackward0>) tensor(-0.3367)\n",
      "tensor([[-0.6064]], grad_fn=<AddmmBackward0>) tensor(-0.4822)\n",
      "tensor([[0.1555]], grad_fn=<AddmmBackward0>) tensor(-0.0083)\n",
      "tensor([[-0.4948]], grad_fn=<AddmmBackward0>) tensor(-0.4513)\n",
      "tensor([[-0.0252]], grad_fn=<AddmmBackward0>) tensor(-0.1609)\n",
      "tensor([[-0.8981]], grad_fn=<AddmmBackward0>) tensor(-0.8509)\n",
      "tensor([[0.4025]], grad_fn=<AddmmBackward0>) tensor(0.9422)\n",
      "tensor([[0.0527]], grad_fn=<AddmmBackward0>) tensor(0.1542)\n",
      "tensor([[-0.8594]], grad_fn=<AddmmBackward0>) tensor(-0.1732)\n",
      "tensor([[-0.6934]], grad_fn=<AddmmBackward0>) tensor(-0.4424)\n",
      "tensor([[0.0500]], grad_fn=<AddmmBackward0>) tensor(0.1414)\n",
      "tensor([[-0.7902]], grad_fn=<AddmmBackward0>) tensor(-0.5341)\n",
      "tensor([[-0.5890]], grad_fn=<AddmmBackward0>) tensor(-0.9189)\n",
      "tensor([[-0.5851]], grad_fn=<AddmmBackward0>) tensor(-0.4278)\n",
      "tensor([[-0.1064]], grad_fn=<AddmmBackward0>) tensor(0.2745)\n",
      "tensor([[-0.0765]], grad_fn=<AddmmBackward0>) tensor(0.6206)\n",
      "tensor([[0.5729]], grad_fn=<AddmmBackward0>) tensor(0.8383)\n",
      "tensor([[-0.5932]], grad_fn=<AddmmBackward0>) tensor(-0.5415)\n",
      "tensor([[-0.6617]], grad_fn=<AddmmBackward0>) tensor(-0.7740)\n",
      "tensor([[-0.7505]], grad_fn=<AddmmBackward0>) tensor(0.0533)\n",
      "tensor([[-0.0124]], grad_fn=<AddmmBackward0>) tensor(-0.2437)\n",
      "tensor([[0.5812]], grad_fn=<AddmmBackward0>) tensor(0.8424)\n",
      "tensor([[0.7743]], grad_fn=<AddmmBackward0>) tensor(0.7333)\n",
      "tensor([[-0.4836]], grad_fn=<AddmmBackward0>) tensor(-0.9396)\n",
      "tensor([[-0.7166]], grad_fn=<AddmmBackward0>) tensor(-0.7491)\n",
      "tensor([[-0.5933]], grad_fn=<AddmmBackward0>) tensor(-0.6631)\n",
      "tensor([[-0.7561]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.4734]], grad_fn=<AddmmBackward0>) tensor(0.0022)\n",
      "tensor([[-0.1246]], grad_fn=<AddmmBackward0>) tensor(0.1432)\n",
      "tensor([[-0.5271]], grad_fn=<AddmmBackward0>) tensor(-0.5769)\n",
      "tensor([[-0.6828]], grad_fn=<AddmmBackward0>) tensor(-0.7638)\n",
      "tensor([[-0.7938]], grad_fn=<AddmmBackward0>) tensor(-0.6540)\n",
      "tensor([[-0.5483]], grad_fn=<AddmmBackward0>) tensor(-0.9442)\n",
      "tensor([[-0.5757]], grad_fn=<AddmmBackward0>) tensor(-0.5414)\n",
      "tensor([[0.2924]], grad_fn=<AddmmBackward0>) tensor(0.5650)\n",
      "tensor([[-0.8968]], grad_fn=<AddmmBackward0>) tensor(-0.8520)\n",
      "tensor([[-0.4301]], grad_fn=<AddmmBackward0>) tensor(-0.6688)\n",
      "tensor([[0.2760]], grad_fn=<AddmmBackward0>) tensor(0.4346)\n",
      "tensor([[-0.5610]], grad_fn=<AddmmBackward0>) tensor(-0.4900)\n",
      "tensor([[-0.0555]], grad_fn=<AddmmBackward0>) tensor(-0.0684)\n",
      "tensor([[-0.5869]], grad_fn=<AddmmBackward0>) tensor(-0.4484)\n",
      "tensor([[-0.6098]], grad_fn=<AddmmBackward0>) tensor(-0.8675)\n",
      "tensor([[-0.4101]], grad_fn=<AddmmBackward0>) tensor(-0.2572)\n",
      "tensor([[-0.9198]], grad_fn=<AddmmBackward0>) tensor(-0.6506)\n",
      "tensor([[-0.5166]], grad_fn=<AddmmBackward0>) tensor(-0.2775)\n",
      "tensor([[-0.1599]], grad_fn=<AddmmBackward0>) tensor(-0.1615)\n",
      "tensor([[0.1758]], grad_fn=<AddmmBackward0>) tensor(0.5669)\n",
      "tensor([[-0.5786]], grad_fn=<AddmmBackward0>) tensor(-0.4077)\n",
      "tensor([[-0.6920]], grad_fn=<AddmmBackward0>) tensor(-0.8177)\n",
      "tensor([[-0.4906]], grad_fn=<AddmmBackward0>) tensor(-0.4630)\n",
      "tensor([[0.4095]], grad_fn=<AddmmBackward0>) tensor(0.5833)\n",
      "tensor([[-0.5628]], grad_fn=<AddmmBackward0>) tensor(-0.3373)\n",
      "tensor([[-0.7240]], grad_fn=<AddmmBackward0>) tensor(-0.4813)\n",
      "tensor([[-0.6214]], grad_fn=<AddmmBackward0>) tensor(-0.5607)\n",
      "tensor([[-0.4735]], grad_fn=<AddmmBackward0>) tensor(-0.4911)\n",
      "tensor([[-0.7484]], grad_fn=<AddmmBackward0>) tensor(-0.9784)\n",
      "tensor([[-0.6392]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.5997]], grad_fn=<AddmmBackward0>) tensor(-0.7618)\n",
      "tensor([[-0.4520]], grad_fn=<AddmmBackward0>) tensor(-0.5145)\n",
      "tensor([[-0.6978]], grad_fn=<AddmmBackward0>) tensor(-0.5881)\n",
      "tensor([[-0.0515]], grad_fn=<AddmmBackward0>) tensor(0.0545)\n",
      "tensor([[0.3057]], grad_fn=<AddmmBackward0>) tensor(0.3638)\n",
      "tensor([[-0.2910]], grad_fn=<AddmmBackward0>) tensor(-0.2138)\n",
      "tensor([[-0.1176]], grad_fn=<AddmmBackward0>) tensor(-0.1212)\n",
      "tensor([[-0.2943]], grad_fn=<AddmmBackward0>) tensor(-0.1483)\n",
      "tensor([[0.3361]], grad_fn=<AddmmBackward0>) tensor(0.5510)\n",
      "tensor([[-0.7798]], grad_fn=<AddmmBackward0>) tensor(-0.9721)\n",
      "tensor([[-0.5228]], grad_fn=<AddmmBackward0>) tensor(-0.5796)\n",
      "tensor([[-0.0584]], grad_fn=<AddmmBackward0>) tensor(-0.2915)\n",
      "tensor([[0.2755]], grad_fn=<AddmmBackward0>) tensor(0.5579)\n",
      "tensor([[-0.0694]], grad_fn=<AddmmBackward0>) tensor(-0.2469)\n",
      "tensor([[0.0042]], grad_fn=<AddmmBackward0>) tensor(-0.0002)\n",
      "tensor([[0.1868]], grad_fn=<AddmmBackward0>) tensor(0.3634)\n",
      "tensor([[-0.5593]], grad_fn=<AddmmBackward0>) tensor(-0.7118)\n",
      "tensor([[-0.4958]], grad_fn=<AddmmBackward0>) tensor(-0.5063)\n",
      "tensor([[-0.3308]], grad_fn=<AddmmBackward0>) tensor(-0.3228)\n",
      "tensor([[-0.0181]], grad_fn=<AddmmBackward0>) tensor(-0.2438)\n",
      "tensor([[-0.4921]], grad_fn=<AddmmBackward0>) tensor(-0.8416)\n",
      "tensor([[0.4364]], grad_fn=<AddmmBackward0>) tensor(0.5221)\n",
      "tensor([[0.0525]], grad_fn=<AddmmBackward0>) tensor(0.1428)\n",
      "tensor([[0.0795]], grad_fn=<AddmmBackward0>) tensor(0.1762)\n",
      "tensor([[-0.4583]], grad_fn=<AddmmBackward0>) tensor(0.6702)\n",
      "tensor([[0.6545]], grad_fn=<AddmmBackward0>) tensor(0.8044)\n",
      "tensor([[-0.6640]], grad_fn=<AddmmBackward0>) tensor(-0.5451)\n",
      "tensor([[-0.5629]], grad_fn=<AddmmBackward0>) tensor(-0.8976)\n",
      "tensor([[-0.7906]], grad_fn=<AddmmBackward0>) tensor(-0.4059)\n",
      "tensor([[-0.1932]], grad_fn=<AddmmBackward0>) tensor(0.3109)\n",
      "tensor([[-0.4741]], grad_fn=<AddmmBackward0>) tensor(0.6730)\n",
      "tensor([[0.0531]], grad_fn=<AddmmBackward0>) tensor(0.5991)\n",
      "tensor([[-0.7133]], grad_fn=<AddmmBackward0>) tensor(-0.8342)\n",
      "tensor([[-0.7807]], grad_fn=<AddmmBackward0>) tensor(-0.8486)\n",
      "tensor([[0.7523]], grad_fn=<AddmmBackward0>) tensor(0.7440)\n",
      "tensor([[-0.2954]], grad_fn=<AddmmBackward0>) tensor(-0.5142)\n",
      "tensor([[-0.6317]], grad_fn=<AddmmBackward0>) tensor(-0.4481)\n",
      "tensor([[-0.3657]], grad_fn=<AddmmBackward0>) tensor(-0.3278)\n",
      "tensor([[0.0264]], grad_fn=<AddmmBackward0>) tensor(0.7502)\n",
      "tensor([[0.6984]], grad_fn=<AddmmBackward0>) tensor(0.5879)\n",
      "tensor([[0.8070]], grad_fn=<AddmmBackward0>) tensor(0.8294)\n",
      "tensor([[-0.8132]], grad_fn=<AddmmBackward0>) tensor(-0.9195)\n",
      "tensor([[-0.0408]], grad_fn=<AddmmBackward0>) tensor(0.1366)\n",
      "tensor([[-0.4627]], grad_fn=<AddmmBackward0>) tensor(-0.8952)\n",
      "tensor([[-0.6697]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.6081]], grad_fn=<AddmmBackward0>) tensor(0.5578)\n",
      "tensor([[-0.8668]], grad_fn=<AddmmBackward0>) tensor(-0.8842)\n",
      "tensor([[-0.6805]], grad_fn=<AddmmBackward0>) tensor(-0.5950)\n",
      "tensor([[-0.4152]], grad_fn=<AddmmBackward0>) tensor(0.0396)\n",
      "tensor([[0.2192]], grad_fn=<AddmmBackward0>) tensor(0.6958)\n",
      "tensor([[0.4250]], grad_fn=<AddmmBackward0>) tensor(0.6290)\n",
      "tensor([[0.2731]], grad_fn=<AddmmBackward0>) tensor(0.5724)\n",
      "tensor([[-0.0171]], grad_fn=<AddmmBackward0>) tensor(0.3386)\n",
      "tensor([[0.2478]], grad_fn=<AddmmBackward0>) tensor(0.6314)\n",
      "tensor([[-0.3785]], grad_fn=<AddmmBackward0>) tensor(-0.2429)\n",
      "tensor([[-0.7657]], grad_fn=<AddmmBackward0>) tensor(-0.9097)\n",
      "tensor([[0.6124]], grad_fn=<AddmmBackward0>) tensor(0.6884)\n",
      "tensor([[-0.6337]], grad_fn=<AddmmBackward0>) tensor(-0.6507)\n",
      "tensor([[0.0139]], grad_fn=<AddmmBackward0>) tensor(-0.2622)\n",
      "tensor([[0.0854]], grad_fn=<AddmmBackward0>) tensor(0.3106)\n",
      "tensor([[-0.7913]], grad_fn=<AddmmBackward0>) tensor(-0.9702)\n",
      "tensor([[-0.0698]], grad_fn=<AddmmBackward0>) tensor(-0.0797)\n",
      "tensor([[-0.7941]], grad_fn=<AddmmBackward0>) tensor(-0.6442)\n",
      "tensor([[-0.1319]], grad_fn=<AddmmBackward0>) tensor(-0.3213)\n",
      "tensor([[0.0048]], grad_fn=<AddmmBackward0>) tensor(0.6112)\n",
      "tensor([[-0.0040]], grad_fn=<AddmmBackward0>) tensor(0.3693)\n",
      "tensor([[-0.3972]], grad_fn=<AddmmBackward0>) tensor(-0.4191)\n",
      "tensor([[-0.4589]], grad_fn=<AddmmBackward0>) tensor(0.3522)\n",
      "tensor([[-0.6481]], grad_fn=<AddmmBackward0>) tensor(-0.9319)\n",
      "tensor([[-0.7137]], grad_fn=<AddmmBackward0>) tensor(-0.3578)\n",
      "tensor([[-0.7212]], grad_fn=<AddmmBackward0>) tensor(-0.8237)\n",
      "tensor([[-0.5417]], grad_fn=<AddmmBackward0>) tensor(-0.5162)\n",
      "tensor([[-0.9460]], grad_fn=<AddmmBackward0>) tensor(-0.8528)\n",
      "tensor([[-0.8450]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.3990]], grad_fn=<AddmmBackward0>) tensor(0.7352)\n",
      "tensor([[-0.5328]], grad_fn=<AddmmBackward0>) tensor(-0.4135)\n",
      "tensor([[-0.2094]], grad_fn=<AddmmBackward0>) tensor(0.6878)\n",
      "tensor([[0.3210]], grad_fn=<AddmmBackward0>) tensor(0.5740)\n",
      "tensor([[-0.7607]], grad_fn=<AddmmBackward0>) tensor(-0.7911)\n",
      "tensor([[0.6761]], grad_fn=<AddmmBackward0>) tensor(0.7150)\n",
      "tensor([[-0.6710]], grad_fn=<AddmmBackward0>) tensor(0.4282)\n",
      "tensor([[0.5214]], grad_fn=<AddmmBackward0>) tensor(0.5602)\n",
      "tensor([[-0.4070]], grad_fn=<AddmmBackward0>) tensor(-0.3359)\n",
      "tensor([[-0.0110]], grad_fn=<AddmmBackward0>) tensor(-0.1655)\n",
      "tensor([[0.6830]], grad_fn=<AddmmBackward0>) tensor(0.5759)\n",
      "tensor([[0.6506]], grad_fn=<AddmmBackward0>) tensor(0.5077)\n",
      "tensor([[-0.6610]], grad_fn=<AddmmBackward0>) tensor(-0.7946)\n",
      "tensor([[-0.7742]], grad_fn=<AddmmBackward0>) tensor(-0.7151)\n",
      "tensor([[0.0302]], grad_fn=<AddmmBackward0>) tensor(0.5722)\n",
      "tensor([[-0.0424]], grad_fn=<AddmmBackward0>) tensor(0.0109)\n",
      "tensor([[-0.8556]], grad_fn=<AddmmBackward0>) tensor(-0.3233)\n",
      "tensor([[-0.6961]], grad_fn=<AddmmBackward0>) tensor(-0.5497)\n",
      "tensor([[-0.0253]], grad_fn=<AddmmBackward0>) tensor(0.6183)\n",
      "tensor([[0.2194]], grad_fn=<AddmmBackward0>) tensor(0.3583)\n",
      "tensor([[0.4591]], grad_fn=<AddmmBackward0>) tensor(0.6678)\n",
      "tensor([[0.2589]], grad_fn=<AddmmBackward0>) tensor(0.5873)\n",
      "tensor([[-0.8220]], grad_fn=<AddmmBackward0>) tensor(-0.4412)\n",
      "tensor([[0.3799]], grad_fn=<AddmmBackward0>) tensor(0.7184)\n",
      "tensor([[0.2637]], grad_fn=<AddmmBackward0>) tensor(0.3751)\n",
      "tensor([[0.1315]], grad_fn=<AddmmBackward0>) tensor(0.1119)\n",
      "tensor([[0.7585]], grad_fn=<AddmmBackward0>) tensor(0.7351)\n",
      "tensor([[-0.6702]], grad_fn=<AddmmBackward0>) tensor(-0.1281)\n",
      "tensor([[-0.7109]], grad_fn=<AddmmBackward0>) tensor(-0.8593)\n",
      "tensor([[-0.7535]], grad_fn=<AddmmBackward0>) tensor(-0.8567)\n",
      "tensor([[-0.2850]], grad_fn=<AddmmBackward0>) tensor(-0.3076)\n",
      "tensor([[-0.6426]], grad_fn=<AddmmBackward0>) tensor(-0.6749)\n",
      "tensor([[-0.1834]], grad_fn=<AddmmBackward0>) tensor(-0.0254)\n",
      "tensor([[-0.2831]], grad_fn=<AddmmBackward0>) tensor(-0.1095)\n",
      "tensor([[-0.6028]], grad_fn=<AddmmBackward0>) tensor(-0.3734)\n",
      "tensor([[-0.1710]], grad_fn=<AddmmBackward0>) tensor(-0.1391)\n",
      "tensor([[0.7044]], grad_fn=<AddmmBackward0>) tensor(0.7248)\n",
      "tensor([[-0.6816]], grad_fn=<AddmmBackward0>) tensor(-0.7128)\n",
      "tensor([[-0.2647]], grad_fn=<AddmmBackward0>) tensor(-0.0759)\n",
      "tensor([[-0.9465]], grad_fn=<AddmmBackward0>) tensor(-0.8447)\n",
      "tensor([[-0.7531]], grad_fn=<AddmmBackward0>) tensor(-0.9500)\n",
      "tensor([[0.2980]], grad_fn=<AddmmBackward0>) tensor(0.3884)\n",
      "tensor([[-0.4866]], grad_fn=<AddmmBackward0>) tensor(-0.7320)\n",
      "tensor([[0.5202]], grad_fn=<AddmmBackward0>) tensor(0.5859)\n",
      "tensor([[-0.0797]], grad_fn=<AddmmBackward0>) tensor(-0.0433)\n",
      "tensor([[-0.5645]], grad_fn=<AddmmBackward0>) tensor(-0.4859)\n",
      "tensor([[-0.1720]], grad_fn=<AddmmBackward0>) tensor(-0.2233)\n",
      "tensor([[-0.9199]], grad_fn=<AddmmBackward0>) tensor(-0.8472)\n",
      "tensor([[-0.7502]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.7594]], grad_fn=<AddmmBackward0>) tensor(-0.4475)\n",
      "tensor([[-0.1315]], grad_fn=<AddmmBackward0>) tensor(-0.7531)\n",
      "tensor([[-0.2686]], grad_fn=<AddmmBackward0>) tensor(0.8088)\n",
      "tensor([[-0.3730]], grad_fn=<AddmmBackward0>) tensor(-0.4286)\n",
      "tensor([[0.0022]], grad_fn=<AddmmBackward0>) tensor(0.2219)\n",
      "tensor([[0.2887]], grad_fn=<AddmmBackward0>) tensor(0.5826)\n",
      "tensor([[-0.3605]], grad_fn=<AddmmBackward0>) tensor(-0.1231)\n",
      "tensor([[-0.7261]], grad_fn=<AddmmBackward0>) tensor(-0.7514)\n",
      "tensor([[-0.0142]], grad_fn=<AddmmBackward0>) tensor(0.2811)\n",
      "tensor([[0.3138]], grad_fn=<AddmmBackward0>) tensor(0.5519)\n",
      "tensor([[0.3264]], grad_fn=<AddmmBackward0>) tensor(0.5716)\n",
      "tensor([[-0.2313]], grad_fn=<AddmmBackward0>) tensor(0.6306)\n",
      "tensor([[-0.8594]], grad_fn=<AddmmBackward0>) tensor(-0.8732)\n",
      "tensor([[-0.2143]], grad_fn=<AddmmBackward0>) tensor(-0.1045)\n",
      "tensor([[-0.4735]], grad_fn=<AddmmBackward0>) tensor(-0.9204)\n",
      "tensor([[-0.7714]], grad_fn=<AddmmBackward0>) tensor(-0.8843)\n",
      "tensor([[0.2133]], grad_fn=<AddmmBackward0>) tensor(0.5358)\n",
      "tensor([[0.2244]], grad_fn=<AddmmBackward0>) tensor(0.7070)\n",
      "tensor([[-0.3526]], grad_fn=<AddmmBackward0>) tensor(-0.1442)\n",
      "tensor([[-0.6322]], grad_fn=<AddmmBackward0>) tensor(-0.4677)\n",
      "tensor([[-0.6449]], grad_fn=<AddmmBackward0>) tensor(-0.5683)\n",
      "tensor([[0.0456]], grad_fn=<AddmmBackward0>) tensor(-0.0203)\n",
      "tensor([[-1.0056]], grad_fn=<AddmmBackward0>) tensor(-0.9483)\n",
      "tensor([[0.3292]], grad_fn=<AddmmBackward0>) tensor(0.8041)\n",
      "tensor([[0.6962]], grad_fn=<AddmmBackward0>) tensor(0.6043)\n",
      "tensor([[-0.4969]], grad_fn=<AddmmBackward0>) tensor(-0.8393)\n",
      "tensor([[-0.6376]], grad_fn=<AddmmBackward0>) tensor(-0.9740)\n",
      "tensor([[0.3358]], grad_fn=<AddmmBackward0>) tensor(0.8012)\n",
      "tensor([[-0.7734]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.2006]], grad_fn=<AddmmBackward0>) tensor(-0.0791)\n",
      "tensor([[-0.4773]], grad_fn=<AddmmBackward0>) tensor(0.0523)\n",
      "tensor([[-0.4899]], grad_fn=<AddmmBackward0>) tensor(-0.2476)\n",
      "tensor([[-0.2222]], grad_fn=<AddmmBackward0>) tensor(0.0097)\n",
      "tensor([[0.6411]], grad_fn=<AddmmBackward0>) tensor(0.7300)\n",
      "tensor([[-0.7735]], grad_fn=<AddmmBackward0>) tensor(-0.7167)\n",
      "tensor([[-0.8896]], grad_fn=<AddmmBackward0>) tensor(-0.9535)\n",
      "tensor([[-0.0580]], grad_fn=<AddmmBackward0>) tensor(0.7311)\n",
      "tensor([[0.2531]], grad_fn=<AddmmBackward0>) tensor(0.8251)\n",
      "tensor([[-0.6973]], grad_fn=<AddmmBackward0>) tensor(-0.8866)\n",
      "tensor([[-0.3367]], grad_fn=<AddmmBackward0>) tensor(-0.8091)\n",
      "tensor([[-0.4605]], grad_fn=<AddmmBackward0>) tensor(-0.5120)\n",
      "tensor([[0.3378]], grad_fn=<AddmmBackward0>) tensor(0.1213)\n",
      "tensor([[0.5987]], grad_fn=<AddmmBackward0>) tensor(0.5808)\n",
      "tensor([[0.4755]], grad_fn=<AddmmBackward0>) tensor(0.5044)\n",
      "tensor([[0.3666]], grad_fn=<AddmmBackward0>) tensor(0.5174)\n",
      "tensor([[-0.4531]], grad_fn=<AddmmBackward0>) tensor(-0.5170)\n",
      "tensor([[0.5129]], grad_fn=<AddmmBackward0>) tensor(0.5280)\n",
      "tensor([[-0.5387]], grad_fn=<AddmmBackward0>) tensor(-0.2750)\n",
      "tensor([[0.9126]], grad_fn=<AddmmBackward0>) tensor(0.7479)\n",
      "tensor([[-0.4234]], grad_fn=<AddmmBackward0>) tensor(-0.4132)\n",
      "tensor([[-0.0666]], grad_fn=<AddmmBackward0>) tensor(-0.1903)\n",
      "tensor([[0.0766]], grad_fn=<AddmmBackward0>) tensor(0.4549)\n",
      "tensor([[0.4978]], grad_fn=<AddmmBackward0>) tensor(0.4442)\n",
      "tensor([[-0.8254]], grad_fn=<AddmmBackward0>) tensor(-0.7877)\n",
      "tensor([[-0.2404]], grad_fn=<AddmmBackward0>) tensor(-0.1455)\n",
      "tensor([[0.5237]], grad_fn=<AddmmBackward0>) tensor(0.8350)\n",
      "tensor([[0.1747]], grad_fn=<AddmmBackward0>) tensor(0.5441)\n",
      "tensor([[0.2454]], grad_fn=<AddmmBackward0>) tensor(0.1324)\n",
      "tensor([[-0.7442]], grad_fn=<AddmmBackward0>) tensor(-0.5201)\n",
      "tensor([[-0.8526]], grad_fn=<AddmmBackward0>) tensor(-0.8293)\n",
      "tensor([[0.4753]], grad_fn=<AddmmBackward0>) tensor(0.5664)\n",
      "tensor([[0.3274]], grad_fn=<AddmmBackward0>) tensor(0.7918)\n",
      "tensor([[-0.6429]], grad_fn=<AddmmBackward0>) tensor(-0.9197)\n",
      "tensor([[0.3041]], grad_fn=<AddmmBackward0>) tensor(0.5692)\n",
      "tensor([[0.4689]], grad_fn=<AddmmBackward0>) tensor(0.5707)\n",
      "tensor([[0.4277]], grad_fn=<AddmmBackward0>) tensor(0.5421)\n",
      "tensor([[-0.5928]], grad_fn=<AddmmBackward0>) tensor(-0.4140)\n",
      "tensor([[-0.6595]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.0416]], grad_fn=<AddmmBackward0>) tensor(-0.1429)\n",
      "tensor([[-0.6552]], grad_fn=<AddmmBackward0>) tensor(-0.9767)\n",
      "tensor([[0.2279]], grad_fn=<AddmmBackward0>) tensor(0.6822)\n",
      "tensor([[-0.0428]], grad_fn=<AddmmBackward0>) tensor(-0.0663)\n",
      "tensor([[0.4515]], grad_fn=<AddmmBackward0>) tensor(0.5483)\n",
      "tensor([[-0.0145]], grad_fn=<AddmmBackward0>) tensor(-0.0407)\n",
      "tensor([[-0.3042]], grad_fn=<AddmmBackward0>) tensor(-0.1093)\n",
      "tensor([[-0.6336]], grad_fn=<AddmmBackward0>) tensor(-0.7674)\n",
      "tensor([[-0.0586]], grad_fn=<AddmmBackward0>) tensor(-0.4399)\n",
      "tensor([[-0.6345]], grad_fn=<AddmmBackward0>) tensor(-0.6750)\n",
      "tensor([[0.5237]], grad_fn=<AddmmBackward0>) tensor(0.5507)\n",
      "tensor([[0.3332]], grad_fn=<AddmmBackward0>) tensor(0.5408)\n",
      "tensor([[-0.6857]], grad_fn=<AddmmBackward0>) tensor(-0.5721)\n",
      "tensor([[-0.6691]], grad_fn=<AddmmBackward0>) tensor(-0.8003)\n",
      "tensor([[-0.1164]], grad_fn=<AddmmBackward0>) tensor(0.0129)\n",
      "tensor([[-0.6669]], grad_fn=<AddmmBackward0>) tensor(-0.5248)\n",
      "tensor([[0.7058]], grad_fn=<AddmmBackward0>) tensor(0.5903)\n",
      "tensor([[-0.6840]], grad_fn=<AddmmBackward0>) tensor(-0.4544)\n",
      "tensor([[-0.6849]], grad_fn=<AddmmBackward0>) tensor(-0.8524)\n",
      "tensor([[-0.6562]], grad_fn=<AddmmBackward0>) tensor(-0.8774)\n",
      "tensor([[-0.6172]], grad_fn=<AddmmBackward0>) tensor(-0.1274)\n",
      "tensor([[-0.8704]], grad_fn=<AddmmBackward0>) tensor(-0.8702)\n",
      "tensor([[0.2674]], grad_fn=<AddmmBackward0>) tensor(0.6171)\n",
      "tensor([[0.5476]], grad_fn=<AddmmBackward0>) tensor(0.5402)\n",
      "tensor([[-0.2883]], grad_fn=<AddmmBackward0>) tensor(-0.3816)\n",
      "tensor([[0.1012]], grad_fn=<AddmmBackward0>) tensor(0.9100)\n",
      "tensor([[-0.7045]], grad_fn=<AddmmBackward0>) tensor(-0.3654)\n",
      "tensor([[-0.1109]], grad_fn=<AddmmBackward0>) tensor(-0.1695)\n",
      "tensor([[0.2184]], grad_fn=<AddmmBackward0>) tensor(0.7562)\n",
      "tensor([[0.2318]], grad_fn=<AddmmBackward0>) tensor(0.6071)\n",
      "tensor([[-0.6734]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.2298]], grad_fn=<AddmmBackward0>) tensor(-0.3112)\n",
      "tensor([[-0.5452]], grad_fn=<AddmmBackward0>) tensor(-0.1879)\n",
      "tensor([[-0.1072]], grad_fn=<AddmmBackward0>) tensor(-0.1494)\n",
      "tensor([[-0.4619]], grad_fn=<AddmmBackward0>) tensor(-0.3491)\n",
      "tensor([[0.4602]], grad_fn=<AddmmBackward0>) tensor(0.5587)\n",
      "tensor([[-0.4137]], grad_fn=<AddmmBackward0>) tensor(-0.2766)\n",
      "tensor([[0.2989]], grad_fn=<AddmmBackward0>) tensor(0.8163)\n",
      "tensor([[0.1986]], grad_fn=<AddmmBackward0>) tensor(0.3799)\n",
      "tensor([[-0.2756]], grad_fn=<AddmmBackward0>) tensor(-0.0233)\n",
      "tensor([[-0.3994]], grad_fn=<AddmmBackward0>) tensor(-0.3972)\n",
      "tensor([[-0.8048]], grad_fn=<AddmmBackward0>) tensor(-0.8473)\n",
      "tensor([[-0.5660]], grad_fn=<AddmmBackward0>) tensor(-0.5388)\n",
      "tensor([[0.0438]], grad_fn=<AddmmBackward0>) tensor(0.4998)\n",
      "tensor([[0.4662]], grad_fn=<AddmmBackward0>) tensor(0.8200)\n",
      "tensor([[-0.6321]], grad_fn=<AddmmBackward0>) tensor(-0.1352)\n",
      "tensor([[-0.8130]], grad_fn=<AddmmBackward0>) tensor(-0.9775)\n",
      "tensor([[-0.7077]], grad_fn=<AddmmBackward0>) tensor(-0.8680)\n",
      "tensor([[0.4168]], grad_fn=<AddmmBackward0>) tensor(0.8297)\n",
      "tensor([[-0.5026]], grad_fn=<AddmmBackward0>) tensor(-0.9871)\n",
      "tensor([[-0.2648]], grad_fn=<AddmmBackward0>) tensor(-0.1114)\n",
      "tensor([[-0.4876]], grad_fn=<AddmmBackward0>) tensor(-0.5901)\n",
      "tensor([[-0.1815]], grad_fn=<AddmmBackward0>) tensor(-0.2504)\n",
      "tensor([[-0.1982]], grad_fn=<AddmmBackward0>) tensor(-0.4923)\n",
      "tensor([[-0.0175]], grad_fn=<AddmmBackward0>) tensor(0.0505)\n",
      "tensor([[-0.6321]], grad_fn=<AddmmBackward0>) tensor(-0.8758)\n",
      "tensor([[-0.1782]], grad_fn=<AddmmBackward0>) tensor(-0.2523)\n",
      "tensor([[-0.8220]], grad_fn=<AddmmBackward0>) tensor(-0.3502)\n",
      "tensor([[0.2845]], grad_fn=<AddmmBackward0>) tensor(0.5968)\n",
      "tensor([[-0.6919]], grad_fn=<AddmmBackward0>) tensor(-0.5169)\n",
      "tensor([[-0.7587]], grad_fn=<AddmmBackward0>) tensor(-0.8646)\n",
      "tensor([[0.3552]], grad_fn=<AddmmBackward0>) tensor(0.5145)\n",
      "tensor([[-0.6412]], grad_fn=<AddmmBackward0>) tensor(-0.5918)\n",
      "tensor([[0.3263]], grad_fn=<AddmmBackward0>) tensor(0.5785)\n",
      "tensor([[-0.0357]], grad_fn=<AddmmBackward0>) tensor(-0.3670)\n",
      "tensor([[-0.5775]], grad_fn=<AddmmBackward0>) tensor(-0.6114)\n",
      "tensor([[-0.4706]], grad_fn=<AddmmBackward0>) tensor(0.0200)\n",
      "tensor([[0.2843]], grad_fn=<AddmmBackward0>) tensor(0.6191)\n",
      "tensor([[0.7292]], grad_fn=<AddmmBackward0>) tensor(0.5414)\n",
      "tensor([[-0.4436]], grad_fn=<AddmmBackward0>) tensor(-0.5124)\n",
      "tensor([[-0.6086]], grad_fn=<AddmmBackward0>) tensor(-0.6146)\n",
      "tensor([[-0.2013]], grad_fn=<AddmmBackward0>) tensor(-0.3072)\n",
      "tensor([[-0.6309]], grad_fn=<AddmmBackward0>) tensor(-0.4135)\n",
      "tensor([[-0.4931]], grad_fn=<AddmmBackward0>) tensor(-0.4894)\n",
      "tensor([[-0.4788]], grad_fn=<AddmmBackward0>) tensor(-0.7241)\n",
      "tensor([[0.1024]], grad_fn=<AddmmBackward0>) tensor(0.5592)\n",
      "tensor([[-0.3285]], grad_fn=<AddmmBackward0>) tensor(-0.0271)\n",
      "tensor([[-0.9040]], grad_fn=<AddmmBackward0>) tensor(-0.8744)\n",
      "tensor([[-0.4038]], grad_fn=<AddmmBackward0>) tensor(0.4960)\n",
      "tensor([[0.5786]], grad_fn=<AddmmBackward0>) tensor(0.6294)\n",
      "tensor([[-0.3631]], grad_fn=<AddmmBackward0>) tensor(-0.2533)\n",
      "tensor([[-0.8258]], grad_fn=<AddmmBackward0>) tensor(-0.7278)\n",
      "tensor([[0.0299]], grad_fn=<AddmmBackward0>) tensor(0.6123)\n",
      "tensor([[-0.8714]], grad_fn=<AddmmBackward0>) tensor(-0.8066)\n",
      "tensor([[0.0617]], grad_fn=<AddmmBackward0>) tensor(0.4148)\n",
      "tensor([[-0.5357]], grad_fn=<AddmmBackward0>) tensor(-0.8108)\n",
      "tensor([[0.0233]], grad_fn=<AddmmBackward0>) tensor(0.8117)\n",
      "tensor([[0.2508]], grad_fn=<AddmmBackward0>) tensor(0.5871)\n",
      "tensor([[0.4004]], grad_fn=<AddmmBackward0>) tensor(0.8217)\n",
      "tensor([[-0.3508]], grad_fn=<AddmmBackward0>) tensor(-0.1065)\n",
      "tensor([[-0.6200]], grad_fn=<AddmmBackward0>) tensor(-0.5125)\n",
      "tensor([[0.5542]], grad_fn=<AddmmBackward0>) tensor(0.5546)\n",
      "tensor([[-0.5116]], grad_fn=<AddmmBackward0>) tensor(0.0705)\n",
      "tensor([[-0.0351]], grad_fn=<AddmmBackward0>) tensor(-0.0593)\n",
      "tensor([[0.3553]], grad_fn=<AddmmBackward0>) tensor(0.5959)\n",
      "tensor([[-0.4030]], grad_fn=<AddmmBackward0>) tensor(-0.6564)\n",
      "tensor([[0.4080]], grad_fn=<AddmmBackward0>) tensor(0.5225)\n",
      "tensor([[-0.8050]], grad_fn=<AddmmBackward0>) tensor(-0.6635)\n",
      "tensor([[0.5382]], grad_fn=<AddmmBackward0>) tensor(0.7466)\n",
      "tensor([[0.4213]], grad_fn=<AddmmBackward0>) tensor(0.7427)\n",
      "tensor([[-0.5759]], grad_fn=<AddmmBackward0>) tensor(-0.4423)\n",
      "tensor([[0.0786]], grad_fn=<AddmmBackward0>) tensor(0.2472)\n",
      "tensor([[-0.3980]], grad_fn=<AddmmBackward0>) tensor(-0.4166)\n",
      "tensor([[-0.0763]], grad_fn=<AddmmBackward0>) tensor(0.3090)\n",
      "tensor([[-0.6340]], grad_fn=<AddmmBackward0>) tensor(-0.6073)\n",
      "tensor([[0.4223]], grad_fn=<AddmmBackward0>) tensor(0.3357)\n",
      "tensor([[-0.7195]], grad_fn=<AddmmBackward0>) tensor(-0.7520)\n",
      "tensor([[0.4418]], grad_fn=<AddmmBackward0>) tensor(0.7824)\n",
      "tensor([[0.3738]], grad_fn=<AddmmBackward0>) tensor(0.4026)\n",
      "tensor([[0.2003]], grad_fn=<AddmmBackward0>) tensor(0.7114)\n",
      "tensor([[-0.3976]], grad_fn=<AddmmBackward0>) tensor(-0.6444)\n",
      "tensor([[-0.0152]], grad_fn=<AddmmBackward0>) tensor(0.2014)\n",
      "tensor([[0.4208]], grad_fn=<AddmmBackward0>) tensor(0.7881)\n",
      "tensor([[-0.4958]], grad_fn=<AddmmBackward0>) tensor(-0.3582)\n",
      "tensor([[-0.6588]], grad_fn=<AddmmBackward0>) tensor(-0.8945)\n",
      "tensor([[-0.5180]], grad_fn=<AddmmBackward0>) tensor(-0.5933)\n",
      "tensor([[-0.2506]], grad_fn=<AddmmBackward0>) tensor(-0.3319)\n",
      "tensor([[-0.0651]], grad_fn=<AddmmBackward0>) tensor(0.0007)\n",
      "tensor([[0.0198]], grad_fn=<AddmmBackward0>) tensor(-0.0100)\n",
      "tensor([[-0.1854]], grad_fn=<AddmmBackward0>) tensor(-0.2703)\n",
      "tensor([[-0.0076]], grad_fn=<AddmmBackward0>) tensor(-0.1137)\n",
      "tensor([[0.2438]], grad_fn=<AddmmBackward0>) tensor(0.0677)\n",
      "tensor([[-1.4523]], grad_fn=<AddmmBackward0>) tensor(-0.9608)\n",
      "tensor([[0.1864]], grad_fn=<AddmmBackward0>) tensor(-0.2135)\n",
      "tensor([[-0.7835]], grad_fn=<AddmmBackward0>) tensor(-0.9264)\n",
      "tensor([[0.7565]], grad_fn=<AddmmBackward0>) tensor(0.7474)\n",
      "tensor([[0.6086]], grad_fn=<AddmmBackward0>) tensor(0.5416)\n",
      "tensor([[-0.7695]], grad_fn=<AddmmBackward0>) tensor(-0.6881)\n",
      "tensor([[-0.1694]], grad_fn=<AddmmBackward0>) tensor(0.2630)\n",
      "tensor([[-0.7171]], grad_fn=<AddmmBackward0>) tensor(0.5842)\n",
      "tensor([[0.6556]], grad_fn=<AddmmBackward0>) tensor(0.6258)\n",
      "tensor([[0.5559]], grad_fn=<AddmmBackward0>) tensor(0.5069)\n",
      "tensor([[-0.7690]], grad_fn=<AddmmBackward0>) tensor(-0.9225)\n",
      "tensor([[0.2819]], grad_fn=<AddmmBackward0>) tensor(0.5282)\n",
      "tensor([[0.4391]], grad_fn=<AddmmBackward0>) tensor(-0.3715)\n",
      "tensor([[-0.7033]], grad_fn=<AddmmBackward0>) tensor(-0.7055)\n",
      "tensor([[-0.7976]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.1010]], grad_fn=<AddmmBackward0>) tensor(0.1516)\n",
      "tensor([[-0.4561]], grad_fn=<AddmmBackward0>) tensor(-0.4010)\n",
      "tensor([[-0.7758]], grad_fn=<AddmmBackward0>) tensor(-0.6479)\n",
      "tensor([[-0.6754]], grad_fn=<AddmmBackward0>) tensor(-0.8899)\n",
      "tensor([[-0.8060]], grad_fn=<AddmmBackward0>) tensor(-0.8623)\n",
      "tensor([[-0.6308]], grad_fn=<AddmmBackward0>) tensor(-0.7676)\n",
      "tensor([[0.4962]], grad_fn=<AddmmBackward0>) tensor(0.8897)\n",
      "tensor([[-0.7019]], grad_fn=<AddmmBackward0>) tensor(-0.6060)\n",
      "tensor([[-0.3887]], grad_fn=<AddmmBackward0>) tensor(-0.6605)\n",
      "tensor([[-0.8806]], grad_fn=<AddmmBackward0>) tensor(-0.8233)\n",
      "tensor([[-0.6229]], grad_fn=<AddmmBackward0>) tensor(-0.6638)\n",
      "tensor([[-1.0175]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.5549]], grad_fn=<AddmmBackward0>) tensor(0.5656)\n",
      "tensor([[-0.4675]], grad_fn=<AddmmBackward0>) tensor(-0.3903)\n",
      "tensor([[-0.1728]], grad_fn=<AddmmBackward0>) tensor(0.1417)\n",
      "tensor([[-0.0803]], grad_fn=<AddmmBackward0>) tensor(-0.1775)\n",
      "tensor([[-0.5122]], grad_fn=<AddmmBackward0>) tensor(0.8350)\n",
      "tensor([[-0.5308]], grad_fn=<AddmmBackward0>) tensor(-0.4883)\n",
      "tensor([[-0.4768]], grad_fn=<AddmmBackward0>) tensor(-0.8089)\n",
      "tensor([[-0.6647]], grad_fn=<AddmmBackward0>) tensor(-0.8794)\n",
      "tensor([[-0.0713]], grad_fn=<AddmmBackward0>) tensor(-0.2412)\n",
      "tensor([[-0.5515]], grad_fn=<AddmmBackward0>) tensor(-0.0855)\n",
      "tensor([[0.8300]], grad_fn=<AddmmBackward0>) tensor(0.5758)\n",
      "tensor([[-0.7133]], grad_fn=<AddmmBackward0>) tensor(-0.7477)\n",
      "tensor([[-0.6971]], grad_fn=<AddmmBackward0>) tensor(-0.4651)\n",
      "tensor([[0.5077]], grad_fn=<AddmmBackward0>) tensor(0.7307)\n",
      "tensor([[0.3219]], grad_fn=<AddmmBackward0>) tensor(0.7417)\n",
      "tensor([[-0.1226]], grad_fn=<AddmmBackward0>) tensor(-0.0005)\n",
      "tensor([[-0.1786]], grad_fn=<AddmmBackward0>) tensor(0.1971)\n",
      "tensor([[0.4195]], grad_fn=<AddmmBackward0>) tensor(-0.0184)\n",
      "tensor([[-0.8005]], grad_fn=<AddmmBackward0>) tensor(-0.7370)\n",
      "tensor([[-0.7156]], grad_fn=<AddmmBackward0>) tensor(-0.7660)\n",
      "tensor([[-0.7152]], grad_fn=<AddmmBackward0>) tensor(-0.8596)\n",
      "tensor([[0.3614]], grad_fn=<AddmmBackward0>) tensor(0.6318)\n",
      "tensor([[-0.7368]], grad_fn=<AddmmBackward0>) tensor(-0.7213)\n",
      "tensor([[-0.5390]], grad_fn=<AddmmBackward0>) tensor(0.3332)\n",
      "tensor([[-0.8933]], grad_fn=<AddmmBackward0>) tensor(-0.4826)\n",
      "tensor([[0.4198]], grad_fn=<AddmmBackward0>) tensor(0.6947)\n",
      "tensor([[0.0845]], grad_fn=<AddmmBackward0>) tensor(0.6768)\n",
      "tensor([[-0.4841]], grad_fn=<AddmmBackward0>) tensor(-0.7151)\n",
      "tensor([[-0.5821]], grad_fn=<AddmmBackward0>) tensor(-0.6015)\n",
      "tensor([[-0.3069]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.4103]], grad_fn=<AddmmBackward0>) tensor(-0.5099)\n",
      "tensor([[-0.5472]], grad_fn=<AddmmBackward0>) tensor(-0.8710)\n",
      "tensor([[-0.8060]], grad_fn=<AddmmBackward0>) tensor(-0.5691)\n",
      "tensor([[0.4449]], grad_fn=<AddmmBackward0>) tensor(0.5359)\n",
      "tensor([[0.0223]], grad_fn=<AddmmBackward0>) tensor(0.3951)\n",
      "tensor([[-0.3727]], grad_fn=<AddmmBackward0>) tensor(-0.5754)\n",
      "tensor([[0.1196]], grad_fn=<AddmmBackward0>) tensor(0.3622)\n",
      "tensor([[-0.7989]], grad_fn=<AddmmBackward0>) tensor(-0.8761)\n",
      "tensor([[0.8413]], grad_fn=<AddmmBackward0>) tensor(0.7939)\n",
      "tensor([[-0.6686]], grad_fn=<AddmmBackward0>) tensor(-0.8690)\n",
      "tensor([[-0.1730]], grad_fn=<AddmmBackward0>) tensor(-0.1274)\n",
      "tensor([[0.4854]], grad_fn=<AddmmBackward0>) tensor(0.5395)\n",
      "tensor([[-0.5720]], grad_fn=<AddmmBackward0>) tensor(-0.3246)\n",
      "tensor([[-0.5451]], grad_fn=<AddmmBackward0>) tensor(-0.5077)\n",
      "tensor([[-0.0615]], grad_fn=<AddmmBackward0>) tensor(-0.2417)\n",
      "tensor([[-0.7290]], grad_fn=<AddmmBackward0>) tensor(-0.6172)\n",
      "tensor([[0.4872]], grad_fn=<AddmmBackward0>) tensor(0.5254)\n",
      "tensor([[-0.6012]], grad_fn=<AddmmBackward0>) tensor(-0.7419)\n",
      "tensor([[0.1380]], grad_fn=<AddmmBackward0>) tensor(0.2431)\n",
      "tensor([[-0.3491]], grad_fn=<AddmmBackward0>) tensor(0.6394)\n",
      "tensor([[-0.5194]], grad_fn=<AddmmBackward0>) tensor(-0.4228)\n",
      "tensor([[-0.1616]], grad_fn=<AddmmBackward0>) tensor(0.7589)\n",
      "tensor([[-0.6453]], grad_fn=<AddmmBackward0>) tensor(-0.8704)\n",
      "tensor([[0.1164]], grad_fn=<AddmmBackward0>) tensor(0.3301)\n",
      "tensor([[-0.5459]], grad_fn=<AddmmBackward0>) tensor(-0.6090)\n",
      "tensor([[0.0462]], grad_fn=<AddmmBackward0>) tensor(0.3041)\n",
      "tensor([[0.7815]], grad_fn=<AddmmBackward0>) tensor(0.5377)\n",
      "tensor([[-0.7419]], grad_fn=<AddmmBackward0>) tensor(-0.7914)\n",
      "tensor([[0.3448]], grad_fn=<AddmmBackward0>) tensor(0.5705)\n",
      "tensor([[-0.0310]], grad_fn=<AddmmBackward0>) tensor(-0.2606)\n",
      "tensor([[0.7342]], grad_fn=<AddmmBackward0>) tensor(0.8306)\n",
      "tensor([[-0.7006]], grad_fn=<AddmmBackward0>) tensor(-0.7278)\n",
      "tensor([[0.4421]], grad_fn=<AddmmBackward0>) tensor(0.3894)\n",
      "tensor([[-1.2820]], grad_fn=<AddmmBackward0>) tensor(-0.9207)\n",
      "tensor([[-0.8143]], grad_fn=<AddmmBackward0>) tensor(-0.8649)\n",
      "tensor([[-0.2577]], grad_fn=<AddmmBackward0>) tensor(-0.2389)\n",
      "tensor([[-0.4487]], grad_fn=<AddmmBackward0>) tensor(-0.3752)\n",
      "tensor([[0.5267]], grad_fn=<AddmmBackward0>) tensor(0.5813)\n",
      "tensor([[-0.6977]], grad_fn=<AddmmBackward0>) tensor(-0.8803)\n",
      "tensor([[0.7979]], grad_fn=<AddmmBackward0>) tensor(0.7345)\n",
      "tensor([[0.1514]], grad_fn=<AddmmBackward0>) tensor(0.8945)\n",
      "tensor([[0.3304]], grad_fn=<AddmmBackward0>) tensor(0.7121)\n",
      "tensor([[0.2093]], grad_fn=<AddmmBackward0>) tensor(0.4611)\n",
      "tensor([[0.2658]], grad_fn=<AddmmBackward0>) tensor(0.0668)\n",
      "tensor([[0.3192]], grad_fn=<AddmmBackward0>) tensor(0.5099)\n",
      "tensor([[0.0202]], grad_fn=<AddmmBackward0>) tensor(0.6158)\n",
      "tensor([[-0.5432]], grad_fn=<AddmmBackward0>) tensor(-0.4505)\n",
      "tensor([[-0.1846]], grad_fn=<AddmmBackward0>) tensor(-0.4625)\n",
      "tensor([[-0.4161]], grad_fn=<AddmmBackward0>) tensor(-0.3474)\n",
      "tensor([[0.2230]], grad_fn=<AddmmBackward0>) tensor(-0.1682)\n",
      "tensor([[-0.4812]], grad_fn=<AddmmBackward0>) tensor(-0.6281)\n",
      "tensor([[-0.5063]], grad_fn=<AddmmBackward0>) tensor(-0.3744)\n",
      "tensor([[-0.7271]], grad_fn=<AddmmBackward0>) tensor(-0.5938)\n",
      "tensor([[-0.5232]], grad_fn=<AddmmBackward0>) tensor(-0.4554)\n",
      "tensor([[0.2978]], grad_fn=<AddmmBackward0>) tensor(0.7449)\n",
      "tensor([[0.4176]], grad_fn=<AddmmBackward0>) tensor(0.3921)\n",
      "tensor([[-0.6391]], grad_fn=<AddmmBackward0>) tensor(-0.5573)\n",
      "tensor([[-0.3978]], grad_fn=<AddmmBackward0>) tensor(-0.5159)\n",
      "tensor([[-0.3063]], grad_fn=<AddmmBackward0>) tensor(-0.4953)\n",
      "tensor([[-0.7829]], grad_fn=<AddmmBackward0>) tensor(-0.5005)\n",
      "tensor([[-0.7569]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.5103]], grad_fn=<AddmmBackward0>) tensor(-0.6612)\n",
      "tensor([[-0.6617]], grad_fn=<AddmmBackward0>) tensor(-0.6410)\n",
      "tensor([[-0.2034]], grad_fn=<AddmmBackward0>) tensor(-0.2959)\n",
      "tensor([[-0.5317]], grad_fn=<AddmmBackward0>) tensor(-0.8553)\n",
      "tensor([[-0.7734]], grad_fn=<AddmmBackward0>) tensor(-0.7397)\n",
      "tensor([[0.2942]], grad_fn=<AddmmBackward0>) tensor(-0.0766)\n",
      "tensor([[-0.1652]], grad_fn=<AddmmBackward0>) tensor(-0.5216)\n",
      "tensor([[0.3290]], grad_fn=<AddmmBackward0>) tensor(0.6033)\n",
      "tensor([[-0.3327]], grad_fn=<AddmmBackward0>) tensor(-0.8715)\n",
      "tensor([[-0.1243]], grad_fn=<AddmmBackward0>) tensor(-0.1734)\n",
      "tensor([[0.0164]], grad_fn=<AddmmBackward0>) tensor(0.5166)\n",
      "tensor([[-0.7466]], grad_fn=<AddmmBackward0>) tensor(-0.5763)\n",
      "tensor([[-0.7678]], grad_fn=<AddmmBackward0>) tensor(-0.8054)\n",
      "tensor([[-0.3465]], grad_fn=<AddmmBackward0>) tensor(-0.2208)\n",
      "tensor([[-0.9301]], grad_fn=<AddmmBackward0>) tensor(-0.7948)\n",
      "tensor([[0.1721]], grad_fn=<AddmmBackward0>) tensor(0.5773)\n",
      "tensor([[-0.3863]], grad_fn=<AddmmBackward0>) tensor(-0.4800)\n",
      "tensor([[0.1667]], grad_fn=<AddmmBackward0>) tensor(0.5735)\n",
      "tensor([[0.1244]], grad_fn=<AddmmBackward0>) tensor(0.3647)\n",
      "tensor([[0.4604]], grad_fn=<AddmmBackward0>) tensor(0.5536)\n",
      "tensor([[-0.5535]], grad_fn=<AddmmBackward0>) tensor(-0.4910)\n",
      "tensor([[-0.5106]], grad_fn=<AddmmBackward0>) tensor(-0.7023)\n",
      "tensor([[-0.9224]], grad_fn=<AddmmBackward0>) tensor(-0.7934)\n",
      "tensor([[-0.8934]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.1942]], grad_fn=<AddmmBackward0>) tensor(-0.3419)\n",
      "tensor([[0.3741]], grad_fn=<AddmmBackward0>) tensor(0.5124)\n",
      "tensor([[-0.7466]], grad_fn=<AddmmBackward0>) tensor(-0.6959)\n",
      "tensor([[0.2077]], grad_fn=<AddmmBackward0>) tensor(0.5732)\n",
      "tensor([[-0.9096]], grad_fn=<AddmmBackward0>) tensor(-0.7815)\n",
      "tensor([[0.5623]], grad_fn=<AddmmBackward0>) tensor(-0.3628)\n",
      "tensor([[-0.6274]], grad_fn=<AddmmBackward0>) tensor(-0.8986)\n",
      "tensor([[-0.6664]], grad_fn=<AddmmBackward0>) tensor(-0.8758)\n",
      "tensor([[-0.7402]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.4766]], grad_fn=<AddmmBackward0>) tensor(-0.0764)\n",
      "tensor([[0.2599]], grad_fn=<AddmmBackward0>) tensor(0.5747)\n",
      "tensor([[0.6439]], grad_fn=<AddmmBackward0>) tensor(0.5691)\n",
      "tensor([[-0.5097]], grad_fn=<AddmmBackward0>) tensor(-0.7458)\n",
      "tensor([[0.4059]], grad_fn=<AddmmBackward0>) tensor(0.5710)\n",
      "tensor([[0.9016]], grad_fn=<AddmmBackward0>) tensor(0.6327)\n",
      "tensor([[-0.5642]], grad_fn=<AddmmBackward0>) tensor(-0.4570)\n",
      "tensor([[-0.0769]], grad_fn=<AddmmBackward0>) tensor(0.6831)\n",
      "tensor([[-0.0981]], grad_fn=<AddmmBackward0>) tensor(0.0297)\n",
      "tensor([[-0.2845]], grad_fn=<AddmmBackward0>) tensor(-0.3397)\n",
      "tensor([[0.1803]], grad_fn=<AddmmBackward0>) tensor(0.5684)\n",
      "tensor([[0.0181]], grad_fn=<AddmmBackward0>) tensor(-0.0324)\n",
      "tensor([[-0.5640]], grad_fn=<AddmmBackward0>) tensor(-0.4733)\n",
      "tensor([[-0.1155]], grad_fn=<AddmmBackward0>) tensor(0.5789)\n",
      "tensor([[-0.6068]], grad_fn=<AddmmBackward0>) tensor(-0.4480)\n",
      "tensor([[-0.4370]], grad_fn=<AddmmBackward0>) tensor(-0.3020)\n",
      "tensor([[0.3611]], grad_fn=<AddmmBackward0>) tensor(0.3745)\n",
      "tensor([[0.3770]], grad_fn=<AddmmBackward0>) tensor(0.5826)\n",
      "tensor([[0.7526]], grad_fn=<AddmmBackward0>) tensor(0.5140)\n",
      "tensor([[0.2679]], grad_fn=<AddmmBackward0>) tensor(0.5300)\n",
      "tensor([[0.2629]], grad_fn=<AddmmBackward0>) tensor(0.5076)\n",
      "tensor([[-0.5540]], grad_fn=<AddmmBackward0>) tensor(-0.2373)\n",
      "tensor([[0.6812]], grad_fn=<AddmmBackward0>) tensor(0.5778)\n",
      "tensor([[-0.6747]], grad_fn=<AddmmBackward0>) tensor(-0.8720)\n",
      "tensor([[0.6427]], grad_fn=<AddmmBackward0>) tensor(0.5836)\n",
      "tensor([[-0.0126]], grad_fn=<AddmmBackward0>) tensor(-0.0288)\n",
      "tensor([[0.0090]], grad_fn=<AddmmBackward0>) tensor(-0.0636)\n",
      "tensor([[-0.4583]], grad_fn=<AddmmBackward0>) tensor(-0.9374)\n",
      "tensor([[-0.1430]], grad_fn=<AddmmBackward0>) tensor(-0.1404)\n",
      "tensor([[0.2566]], grad_fn=<AddmmBackward0>) tensor(0.6633)\n",
      "tensor([[-0.1182]], grad_fn=<AddmmBackward0>) tensor(0.0286)\n",
      "tensor([[-0.9127]], grad_fn=<AddmmBackward0>) tensor(-0.7395)\n",
      "tensor([[-0.9508]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.7341]], grad_fn=<AddmmBackward0>) tensor(-0.5953)\n",
      "tensor([[0.4271]], grad_fn=<AddmmBackward0>) tensor(0.5718)\n",
      "tensor([[-0.8404]], grad_fn=<AddmmBackward0>) tensor(-0.8853)\n",
      "tensor([[0.5459]], grad_fn=<AddmmBackward0>) tensor(0.7442)\n",
      "tensor([[-0.8934]], grad_fn=<AddmmBackward0>) tensor(-0.8084)\n",
      "tensor([[-0.7963]], grad_fn=<AddmmBackward0>) tensor(-0.7604)\n",
      "tensor([[-0.5090]], grad_fn=<AddmmBackward0>) tensor(-0.8533)\n",
      "tensor([[-0.4892]], grad_fn=<AddmmBackward0>) tensor(-0.6905)\n",
      "tensor([[0.1719]], grad_fn=<AddmmBackward0>) tensor(0.3925)\n",
      "tensor([[0.2555]], grad_fn=<AddmmBackward0>) tensor(0.2335)\n",
      "tensor([[-0.7742]], grad_fn=<AddmmBackward0>) tensor(-0.7636)\n",
      "tensor([[-0.3631]], grad_fn=<AddmmBackward0>) tensor(-0.2213)\n",
      "tensor([[-0.9312]], grad_fn=<AddmmBackward0>) tensor(-0.9393)\n",
      "tensor([[0.5016]], grad_fn=<AddmmBackward0>) tensor(0.7147)\n",
      "tensor([[0.7196]], grad_fn=<AddmmBackward0>) tensor(0.7268)\n",
      "tensor([[0.3591]], grad_fn=<AddmmBackward0>) tensor(0.7511)\n",
      "tensor([[0.7662]], grad_fn=<AddmmBackward0>) tensor(0.7144)\n",
      "tensor([[-0.8841]], grad_fn=<AddmmBackward0>) tensor(-0.8236)\n",
      "tensor([[-0.2643]], grad_fn=<AddmmBackward0>) tensor(-0.2489)\n",
      "tensor([[0.5720]], grad_fn=<AddmmBackward0>) tensor(0.5894)\n",
      "tensor([[0.5532]], grad_fn=<AddmmBackward0>) tensor(0.7394)\n",
      "tensor([[-0.5621]], grad_fn=<AddmmBackward0>) tensor(-0.4126)\n",
      "tensor([[0.0511]], grad_fn=<AddmmBackward0>) tensor(0.3508)\n",
      "tensor([[-0.0434]], grad_fn=<AddmmBackward0>) tensor(0.1710)\n",
      "tensor([[-0.4980]], grad_fn=<AddmmBackward0>) tensor(-0.3774)\n",
      "tensor([[-0.3315]], grad_fn=<AddmmBackward0>) tensor(-0.8979)\n",
      "tensor([[-0.4163]], grad_fn=<AddmmBackward0>) tensor(-0.1518)\n",
      "tensor([[-0.3496]], grad_fn=<AddmmBackward0>) tensor(-0.1866)\n",
      "tensor([[0.8381]], grad_fn=<AddmmBackward0>) tensor(0.5988)\n",
      "tensor([[0.4590]], grad_fn=<AddmmBackward0>) tensor(0.5684)\n",
      "tensor([[-0.3115]], grad_fn=<AddmmBackward0>) tensor(-0.3550)\n",
      "tensor([[0.4541]], grad_fn=<AddmmBackward0>) tensor(0.5130)\n",
      "tensor([[0.0841]], grad_fn=<AddmmBackward0>) tensor(0.2973)\n",
      "tensor([[0.3550]], grad_fn=<AddmmBackward0>) tensor(0.8302)\n",
      "tensor([[0.1989]], grad_fn=<AddmmBackward0>) tensor(0.2319)\n",
      "tensor([[0.3294]], grad_fn=<AddmmBackward0>) tensor(0.5563)\n",
      "tensor([[0.0469]], grad_fn=<AddmmBackward0>) tensor(0.5388)\n",
      "tensor([[0.5268]], grad_fn=<AddmmBackward0>) tensor(0.5697)\n",
      "tensor([[0.1353]], grad_fn=<AddmmBackward0>) tensor(0.0910)\n",
      "tensor([[-0.1520]], grad_fn=<AddmmBackward0>) tensor(0.3057)\n",
      "tensor([[-1.0245]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.3409]], grad_fn=<AddmmBackward0>) tensor(0.0165)\n",
      "tensor([[0.0743]], grad_fn=<AddmmBackward0>) tensor(-0.0033)\n",
      "tensor([[0.4595]], grad_fn=<AddmmBackward0>) tensor(0.5189)\n",
      "tensor([[-0.6027]], grad_fn=<AddmmBackward0>) tensor(-0.4841)\n",
      "tensor([[-0.6889]], grad_fn=<AddmmBackward0>) tensor(-0.8889)\n",
      "tensor([[-0.1076]], grad_fn=<AddmmBackward0>) tensor(0.1495)\n",
      "tensor([[-0.5820]], grad_fn=<AddmmBackward0>) tensor(-0.5649)\n",
      "tensor([[0.3915]], grad_fn=<AddmmBackward0>) tensor(0.5410)\n",
      "tensor([[-0.2796]], grad_fn=<AddmmBackward0>) tensor(-0.8290)\n",
      "tensor([[0.2702]], grad_fn=<AddmmBackward0>) tensor(0.6311)\n",
      "tensor([[-0.5880]], grad_fn=<AddmmBackward0>) tensor(-0.9640)\n",
      "tensor([[-0.1562]], grad_fn=<AddmmBackward0>) tensor(0.5574)\n",
      "tensor([[-0.7605]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.5389]], grad_fn=<AddmmBackward0>) tensor(0.5455)\n",
      "tensor([[-0.1693]], grad_fn=<AddmmBackward0>) tensor(0.4205)\n",
      "tensor([[0.1451]], grad_fn=<AddmmBackward0>) tensor(0.6076)\n",
      "tensor([[-0.1688]], grad_fn=<AddmmBackward0>) tensor(-0.2599)\n",
      "tensor([[-0.1969]], grad_fn=<AddmmBackward0>) tensor(0.6189)\n",
      "tensor([[0.2437]], grad_fn=<AddmmBackward0>) tensor(0.3556)\n",
      "tensor([[-0.0550]], grad_fn=<AddmmBackward0>) tensor(-0.4135)\n",
      "tensor([[0.4090]], grad_fn=<AddmmBackward0>) tensor(0.7483)\n",
      "tensor([[0.0075]], grad_fn=<AddmmBackward0>) tensor(0.1824)\n",
      "tensor([[-0.9854]], grad_fn=<AddmmBackward0>) tensor(-0.8785)\n",
      "tensor([[0.3359]], grad_fn=<AddmmBackward0>) tensor(0.7959)\n",
      "tensor([[0.0473]], grad_fn=<AddmmBackward0>) tensor(-0.0770)\n",
      "tensor([[0.1794]], grad_fn=<AddmmBackward0>) tensor(0.7469)\n",
      "tensor([[0.3951]], grad_fn=<AddmmBackward0>) tensor(0.8016)\n",
      "tensor([[0.4041]], grad_fn=<AddmmBackward0>) tensor(0.5750)\n",
      "tensor([[0.5512]], grad_fn=<AddmmBackward0>) tensor(0.5657)\n",
      "tensor([[0.0078]], grad_fn=<AddmmBackward0>) tensor(0.5141)\n",
      "tensor([[-0.7033]], grad_fn=<AddmmBackward0>) tensor(-0.8666)\n",
      "tensor([[0.1947]], grad_fn=<AddmmBackward0>) tensor(0.5506)\n",
      "tensor([[-0.5445]], grad_fn=<AddmmBackward0>) tensor(-0.5253)\n",
      "tensor([[0.4334]], grad_fn=<AddmmBackward0>) tensor(0.5425)\n",
      "tensor([[0.7292]], grad_fn=<AddmmBackward0>) tensor(0.5398)\n",
      "tensor([[0.0315]], grad_fn=<AddmmBackward0>) tensor(0.3545)\n",
      "tensor([[-0.5447]], grad_fn=<AddmmBackward0>) tensor(-0.5419)\n",
      "tensor([[-0.4459]], grad_fn=<AddmmBackward0>) tensor(-0.6128)\n",
      "tensor([[0.2558]], grad_fn=<AddmmBackward0>) tensor(0.5699)\n",
      "tensor([[0.2094]], grad_fn=<AddmmBackward0>) tensor(0.6365)\n",
      "tensor([[-0.3776]], grad_fn=<AddmmBackward0>) tensor(0.3193)\n",
      "tensor([[-0.1812]], grad_fn=<AddmmBackward0>) tensor(-0.1678)\n",
      "tensor([[-0.0459]], grad_fn=<AddmmBackward0>) tensor(0.0034)\n",
      "tensor([[-0.6866]], grad_fn=<AddmmBackward0>) tensor(-0.6292)\n",
      "tensor([[0.6940]], grad_fn=<AddmmBackward0>) tensor(0.5946)\n",
      "tensor([[0.0384]], grad_fn=<AddmmBackward0>) tensor(0.1086)\n",
      "tensor([[-0.4052]], grad_fn=<AddmmBackward0>) tensor(-0.1227)\n",
      "tensor([[0.3185]], grad_fn=<AddmmBackward0>) tensor(0.3969)\n",
      "tensor([[-0.5859]], grad_fn=<AddmmBackward0>) tensor(-0.3564)\n",
      "tensor([[-0.9494]], grad_fn=<AddmmBackward0>) tensor(-0.8382)\n",
      "tensor([[-0.6810]], grad_fn=<AddmmBackward0>) tensor(-0.8572)\n",
      "tensor([[-0.6370]], grad_fn=<AddmmBackward0>) tensor(-0.8398)\n",
      "tensor([[-0.8693]], grad_fn=<AddmmBackward0>) tensor(-0.7570)\n",
      "tensor([[-0.8099]], grad_fn=<AddmmBackward0>) tensor(-0.7415)\n",
      "tensor([[-0.4329]], grad_fn=<AddmmBackward0>) tensor(0.4674)\n",
      "tensor([[-0.0166]], grad_fn=<AddmmBackward0>) tensor(0.6600)\n",
      "tensor([[0.1606]], grad_fn=<AddmmBackward0>) tensor(0.6491)\n",
      "tensor([[-0.5727]], grad_fn=<AddmmBackward0>) tensor(-0.7898)\n",
      "tensor([[-0.5157]], grad_fn=<AddmmBackward0>) tensor(-0.5281)\n",
      "tensor([[-0.4871]], grad_fn=<AddmmBackward0>) tensor(0.0005)\n",
      "tensor([[-0.4673]], grad_fn=<AddmmBackward0>) tensor(-0.3230)\n",
      "tensor([[-0.0112]], grad_fn=<AddmmBackward0>) tensor(0.6339)\n",
      "tensor([[-0.4582]], grad_fn=<AddmmBackward0>) tensor(-0.3304)\n",
      "tensor([[-0.4010]], grad_fn=<AddmmBackward0>) tensor(-0.8384)\n",
      "tensor([[-0.4762]], grad_fn=<AddmmBackward0>) tensor(-0.7210)\n",
      "tensor([[-0.1769]], grad_fn=<AddmmBackward0>) tensor(-0.2853)\n",
      "tensor([[0.0839]], grad_fn=<AddmmBackward0>) tensor(0.2852)\n",
      "tensor([[-0.9701]], grad_fn=<AddmmBackward0>) tensor(-0.7709)\n",
      "tensor([[-0.6832]], grad_fn=<AddmmBackward0>) tensor(-0.6199)\n",
      "tensor([[-0.6602]], grad_fn=<AddmmBackward0>) tensor(0.0057)\n",
      "tensor([[-0.2975]], grad_fn=<AddmmBackward0>) tensor(0.1267)\n",
      "tensor([[0.1010]], grad_fn=<AddmmBackward0>) tensor(-0.0227)\n",
      "tensor([[-0.8867]], grad_fn=<AddmmBackward0>) tensor(-0.8119)\n",
      "tensor([[-0.7570]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.9078]], grad_fn=<AddmmBackward0>) tensor(-0.5607)\n",
      "tensor([[-0.5374]], grad_fn=<AddmmBackward0>) tensor(-0.5036)\n",
      "tensor([[0.4502]], grad_fn=<AddmmBackward0>) tensor(0.5309)\n",
      "tensor([[-0.5598]], grad_fn=<AddmmBackward0>) tensor(-0.4833)\n",
      "tensor([[-0.4464]], grad_fn=<AddmmBackward0>) tensor(-0.4799)\n",
      "tensor([[-0.5041]], grad_fn=<AddmmBackward0>) tensor(0.4726)\n",
      "tensor([[0.0963]], grad_fn=<AddmmBackward0>) tensor(0.7132)\n",
      "tensor([[-0.5946]], grad_fn=<AddmmBackward0>) tensor(-0.6956)\n",
      "tensor([[0.5001]], grad_fn=<AddmmBackward0>) tensor(0.7376)\n",
      "tensor([[-0.3187]], grad_fn=<AddmmBackward0>) tensor(0.0006)\n",
      "tensor([[-0.2771]], grad_fn=<AddmmBackward0>) tensor(-0.3132)\n",
      "tensor([[-0.5131]], grad_fn=<AddmmBackward0>) tensor(-0.3843)\n",
      "tensor([[-0.5597]], grad_fn=<AddmmBackward0>) tensor(-0.9218)\n",
      "tensor([[-0.6386]], grad_fn=<AddmmBackward0>) tensor(-0.4155)\n",
      "tensor([[-0.3938]], grad_fn=<AddmmBackward0>) tensor(-0.9658)\n",
      "tensor([[-0.3368]], grad_fn=<AddmmBackward0>) tensor(0.0096)\n",
      "tensor([[-0.5586]], grad_fn=<AddmmBackward0>) tensor(-0.5185)\n",
      "tensor([[-0.4833]], grad_fn=<AddmmBackward0>) tensor(-0.5973)\n",
      "tensor([[0.3298]], grad_fn=<AddmmBackward0>) tensor(0.7526)\n",
      "tensor([[-0.3318]], grad_fn=<AddmmBackward0>) tensor(0.0210)\n",
      "tensor([[-0.5985]], grad_fn=<AddmmBackward0>) tensor(-0.6787)\n",
      "tensor([[0.2190]], grad_fn=<AddmmBackward0>) tensor(0.4472)\n",
      "tensor([[-0.6704]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.1628]], grad_fn=<AddmmBackward0>) tensor(0.1222)\n",
      "tensor([[-0.8812]], grad_fn=<AddmmBackward0>) tensor(-0.6744)\n",
      "tensor([[-0.1730]], grad_fn=<AddmmBackward0>) tensor(0.2193)\n",
      "tensor([[-0.4813]], grad_fn=<AddmmBackward0>) tensor(-0.3272)\n",
      "tensor([[-1.1445]], grad_fn=<AddmmBackward0>) tensor(-0.3848)\n",
      "tensor([[0.5091]], grad_fn=<AddmmBackward0>) tensor(0.5448)\n",
      "tensor([[-0.6001]], grad_fn=<AddmmBackward0>) tensor(-0.1289)\n",
      "tensor([[0.3822]], grad_fn=<AddmmBackward0>) tensor(0.7468)\n",
      "tensor([[-0.0802]], grad_fn=<AddmmBackward0>) tensor(-0.1727)\n",
      "tensor([[0.7553]], grad_fn=<AddmmBackward0>) tensor(0.6298)\n",
      "tensor([[-0.7030]], grad_fn=<AddmmBackward0>) tensor(-0.5012)\n",
      "tensor([[-0.5980]], grad_fn=<AddmmBackward0>) tensor(-0.5490)\n",
      "tensor([[-0.2353]], grad_fn=<AddmmBackward0>) tensor(-0.2447)\n",
      "tensor([[-0.1704]], grad_fn=<AddmmBackward0>) tensor(-0.0629)\n",
      "tensor([[0.3432]], grad_fn=<AddmmBackward0>) tensor(0.6250)\n",
      "tensor([[-0.8259]], grad_fn=<AddmmBackward0>) tensor(-0.9505)\n",
      "tensor([[0.2658]], grad_fn=<AddmmBackward0>) tensor(0.3568)\n",
      "tensor([[-0.4767]], grad_fn=<AddmmBackward0>) tensor(-0.5575)\n",
      "tensor([[0.5767]], grad_fn=<AddmmBackward0>) tensor(0.6025)\n",
      "tensor([[-0.5569]], grad_fn=<AddmmBackward0>) tensor(-0.7146)\n",
      "tensor([[-0.6340]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.8344]], grad_fn=<AddmmBackward0>) tensor(-0.4231)\n",
      "tensor([[-0.5099]], grad_fn=<AddmmBackward0>) tensor(-0.3034)\n",
      "tensor([[-0.6917]], grad_fn=<AddmmBackward0>) tensor(-0.7158)\n",
      "tensor([[-0.3132]], grad_fn=<AddmmBackward0>) tensor(-0.5220)\n",
      "tensor([[0.4574]], grad_fn=<AddmmBackward0>) tensor(0.5349)\n",
      "tensor([[-0.0956]], grad_fn=<AddmmBackward0>) tensor(-0.0370)\n",
      "tensor([[-0.0530]], grad_fn=<AddmmBackward0>) tensor(-0.0055)\n",
      "tensor([[-0.9016]], grad_fn=<AddmmBackward0>) tensor(-0.9150)\n",
      "tensor([[-0.2571]], grad_fn=<AddmmBackward0>) tensor(-0.1033)\n",
      "tensor([[0.0797]], grad_fn=<AddmmBackward0>) tensor(0.4775)\n",
      "tensor([[-0.5237]], grad_fn=<AddmmBackward0>) tensor(-0.4946)\n",
      "tensor([[0.0758]], grad_fn=<AddmmBackward0>) tensor(0.5959)\n",
      "tensor([[0.0700]], grad_fn=<AddmmBackward0>) tensor(0.2078)\n",
      "tensor([[0.0539]], grad_fn=<AddmmBackward0>) tensor(0.4708)\n",
      "tensor([[-0.7708]], grad_fn=<AddmmBackward0>) tensor(-0.6187)\n",
      "tensor([[-0.3839]], grad_fn=<AddmmBackward0>) tensor(-0.7126)\n",
      "tensor([[0.2023]], grad_fn=<AddmmBackward0>) tensor(-0.0129)\n",
      "tensor([[-0.7286]], grad_fn=<AddmmBackward0>) tensor(-0.9062)\n",
      "tensor([[-0.8391]], grad_fn=<AddmmBackward0>) tensor(-0.9720)\n",
      "tensor([[-0.3249]], grad_fn=<AddmmBackward0>) tensor(-0.3472)\n",
      "tensor([[-0.9216]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.0722]], grad_fn=<AddmmBackward0>) tensor(-0.0017)\n",
      "tensor([[-0.4738]], grad_fn=<AddmmBackward0>) tensor(-0.6392)\n",
      "tensor([[0.4736]], grad_fn=<AddmmBackward0>) tensor(0.6151)\n",
      "tensor([[-0.7625]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.6705]], grad_fn=<AddmmBackward0>) tensor(-0.6162)\n",
      "tensor([[-0.2310]], grad_fn=<AddmmBackward0>) tensor(0.3459)\n",
      "tensor([[-0.7314]], grad_fn=<AddmmBackward0>) tensor(-0.3860)\n",
      "tensor([[0.8531]], grad_fn=<AddmmBackward0>) tensor(0.8316)\n",
      "tensor([[0.7154]], grad_fn=<AddmmBackward0>) tensor(0.6591)\n",
      "tensor([[-0.7613]], grad_fn=<AddmmBackward0>) tensor(-0.4449)\n",
      "tensor([[0.0683]], grad_fn=<AddmmBackward0>) tensor(0.2901)\n",
      "tensor([[-0.7028]], grad_fn=<AddmmBackward0>) tensor(-0.9165)\n",
      "tensor([[-0.3920]], grad_fn=<AddmmBackward0>) tensor(-0.0839)\n",
      "tensor([[0.5464]], grad_fn=<AddmmBackward0>) tensor(0.6804)\n",
      "tensor([[-0.4518]], grad_fn=<AddmmBackward0>) tensor(-0.4533)\n",
      "tensor([[-0.6136]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.1870]], grad_fn=<AddmmBackward0>) tensor(0.0573)\n",
      "tensor([[0.3719]], grad_fn=<AddmmBackward0>) tensor(0.1250)\n",
      "tensor([[-0.2710]], grad_fn=<AddmmBackward0>) tensor(-0.4596)\n",
      "tensor([[-0.2268]], grad_fn=<AddmmBackward0>) tensor(0.1658)\n",
      "tensor([[-0.6147]], grad_fn=<AddmmBackward0>) tensor(-0.5267)\n",
      "tensor([[-0.1234]], grad_fn=<AddmmBackward0>) tensor(0.0101)\n",
      "tensor([[-0.6738]], grad_fn=<AddmmBackward0>) tensor(-0.0808)\n",
      "tensor([[-0.2369]], grad_fn=<AddmmBackward0>) tensor(-0.0412)\n",
      "tensor([[0.0244]], grad_fn=<AddmmBackward0>) tensor(0.2368)\n",
      "tensor([[-0.7510]], grad_fn=<AddmmBackward0>) tensor(-0.7026)\n",
      "tensor([[-0.8063]], grad_fn=<AddmmBackward0>) tensor(-0.7658)\n",
      "tensor([[-0.4252]], grad_fn=<AddmmBackward0>) tensor(-0.3148)\n",
      "tensor([[-0.4944]], grad_fn=<AddmmBackward0>) tensor(-0.2755)\n",
      "tensor([[-0.4371]], grad_fn=<AddmmBackward0>) tensor(-0.9480)\n",
      "tensor([[-0.4309]], grad_fn=<AddmmBackward0>) tensor(-0.6621)\n",
      "tensor([[0.5795]], grad_fn=<AddmmBackward0>) tensor(0.7913)\n",
      "tensor([[-0.5510]], grad_fn=<AddmmBackward0>) tensor(-0.4220)\n",
      "tensor([[-0.1155]], grad_fn=<AddmmBackward0>) tensor(-0.2340)\n",
      "tensor([[-0.7170]], grad_fn=<AddmmBackward0>) tensor(-0.4761)\n",
      "tensor([[-0.2945]], grad_fn=<AddmmBackward0>) tensor(-0.4016)\n",
      "tensor([[-0.1335]], grad_fn=<AddmmBackward0>) tensor(-0.2713)\n",
      "tensor([[-0.6219]], grad_fn=<AddmmBackward0>) tensor(-0.7375)\n",
      "tensor([[-0.5010]], grad_fn=<AddmmBackward0>) tensor(-0.6915)\n",
      "tensor([[-0.8962]], grad_fn=<AddmmBackward0>) tensor(-0.8648)\n",
      "tensor([[0.8363]], grad_fn=<AddmmBackward0>) tensor(0.7042)\n",
      "tensor([[0.2837]], grad_fn=<AddmmBackward0>) tensor(0.1182)\n",
      "tensor([[-0.2888]], grad_fn=<AddmmBackward0>) tensor(-0.2681)\n",
      "tensor([[-0.8497]], grad_fn=<AddmmBackward0>) tensor(-0.8674)\n",
      "tensor([[0.1196]], grad_fn=<AddmmBackward0>) tensor(0.1776)\n",
      "tensor([[-0.7652]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[-0.2222]], grad_fn=<AddmmBackward0>) tensor(-0.1041)\n",
      "tensor([[-0.5200]], grad_fn=<AddmmBackward0>) tensor(-0.1861)\n",
      "tensor([[-0.3726]], grad_fn=<AddmmBackward0>) tensor(-0.1482)\n",
      "tensor([[-0.6494]], grad_fn=<AddmmBackward0>) tensor(-0.5150)\n",
      "tensor([[0.0440]], grad_fn=<AddmmBackward0>) tensor(0.0178)\n",
      "tensor([[-0.3213]], grad_fn=<AddmmBackward0>) tensor(-0.2857)\n",
      "tensor([[0.1772]], grad_fn=<AddmmBackward0>) tensor(0.5936)\n",
      "tensor([[0.0074]], grad_fn=<AddmmBackward0>) tensor(-0.3030)\n",
      "tensor([[0.5193]], grad_fn=<AddmmBackward0>) tensor(0.5643)\n",
      "tensor([[-0.7812]], grad_fn=<AddmmBackward0>) tensor(-0.6554)\n",
      "tensor([[0.4177]], grad_fn=<AddmmBackward0>) tensor(0.4199)\n",
      "tensor([[-0.4306]], grad_fn=<AddmmBackward0>) tensor(-0.1159)\n",
      "tensor([[-0.6479]], grad_fn=<AddmmBackward0>) tensor(-0.6865)\n",
      "tensor([[0.6805]], grad_fn=<AddmmBackward0>) tensor(0.5352)\n",
      "tensor([[-0.8100]], grad_fn=<AddmmBackward0>) tensor(-0.3675)\n",
      "tensor([[-0.4395]], grad_fn=<AddmmBackward0>) tensor(-0.4147)\n",
      "tensor([[0.3919]], grad_fn=<AddmmBackward0>) tensor(0.5350)\n",
      "tensor([[-0.7323]], grad_fn=<AddmmBackward0>) tensor(-0.8701)\n",
      "tensor([[-0.1251]], grad_fn=<AddmmBackward0>) tensor(0.1012)\n",
      "tensor([[-1.1360]], grad_fn=<AddmmBackward0>) tensor(-0.9173)\n",
      "tensor([[-0.4302]], grad_fn=<AddmmBackward0>) tensor(-0.8407)\n",
      "tensor([[0.0541]], grad_fn=<AddmmBackward0>) tensor(0.2571)\n",
      "tensor([[0.4740]], grad_fn=<AddmmBackward0>) tensor(0.5623)\n",
      "tensor([[-0.6871]], grad_fn=<AddmmBackward0>) tensor(-0.6358)\n",
      "tensor([[-0.8354]], grad_fn=<AddmmBackward0>) tensor(-0.8534)\n",
      "tensor([[0.4899]], grad_fn=<AddmmBackward0>) tensor(0.6038)\n",
      "tensor([[-0.0217]], grad_fn=<AddmmBackward0>) tensor(0.5210)\n",
      "tensor([[-0.3608]], grad_fn=<AddmmBackward0>) tensor(-0.0332)\n",
      "tensor([[-0.1629]], grad_fn=<AddmmBackward0>) tensor(-0.2356)\n",
      "tensor([[0.0804]], grad_fn=<AddmmBackward0>) tensor(0.5043)\n",
      "tensor([[-0.4828]], grad_fn=<AddmmBackward0>) tensor(-0.2536)\n",
      "tensor([[-0.2230]], grad_fn=<AddmmBackward0>) tensor(0.3103)\n",
      "tensor([[-0.0279]], grad_fn=<AddmmBackward0>) tensor(-0.3000)\n",
      "tensor([[-0.6398]], grad_fn=<AddmmBackward0>) tensor(-0.6893)\n",
      "tensor([[-0.7693]], grad_fn=<AddmmBackward0>) tensor(-0.8724)\n",
      "tensor([[-0.3662]], grad_fn=<AddmmBackward0>) tensor(-0.3790)\n",
      "tensor([[0.2061]], grad_fn=<AddmmBackward0>) tensor(0.5895)\n",
      "tensor([[-0.2899]], grad_fn=<AddmmBackward0>) tensor(-0.7492)\n",
      "tensor([[-0.1470]], grad_fn=<AddmmBackward0>) tensor(-0.3285)\n",
      "tensor([[0.4002]], grad_fn=<AddmmBackward0>) tensor(0.8246)\n",
      "tensor([[0.3064]], grad_fn=<AddmmBackward0>) tensor(0.3327)\n",
      "tensor([[-0.2234]], grad_fn=<AddmmBackward0>) tensor(0.1851)\n",
      "tensor([[0.6701]], grad_fn=<AddmmBackward0>) tensor(0.6252)\n",
      "tensor([[-0.7567]], grad_fn=<AddmmBackward0>) tensor(-0.4130)\n",
      "tensor([[0.6558]], grad_fn=<AddmmBackward0>) tensor(0.7147)\n",
      "tensor([[-0.6703]], grad_fn=<AddmmBackward0>) tensor(-0.7401)\n",
      "tensor([[-0.2867]], grad_fn=<AddmmBackward0>) tensor(-0.0305)\n",
      "tensor([[0.1719]], grad_fn=<AddmmBackward0>) tensor(0.5363)\n",
      "tensor([[-0.7463]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.2810]], grad_fn=<AddmmBackward0>) tensor(0.7320)\n",
      "tensor([[-0.7971]], grad_fn=<AddmmBackward0>) tensor(-0.9728)\n",
      "tensor([[-0.0113]], grad_fn=<AddmmBackward0>) tensor(-0.1227)\n",
      "tensor([[-0.1210]], grad_fn=<AddmmBackward0>) tensor(-0.1070)\n",
      "tensor([[-0.1939]], grad_fn=<AddmmBackward0>) tensor(-0.1466)\n",
      "tensor([[-0.8295]], grad_fn=<AddmmBackward0>) tensor(-0.9718)\n",
      "tensor([[0.2584]], grad_fn=<AddmmBackward0>) tensor(0.5608)\n",
      "tensor([[0.2146]], grad_fn=<AddmmBackward0>) tensor(0.0049)\n",
      "tensor([[-0.8762]], grad_fn=<AddmmBackward0>) tensor(-0.3674)\n",
      "tensor([[0.2456]], grad_fn=<AddmmBackward0>) tensor(0.1736)\n",
      "tensor([[-0.6070]], grad_fn=<AddmmBackward0>) tensor(-0.7022)\n",
      "tensor([[-0.0042]], grad_fn=<AddmmBackward0>) tensor(0.1757)\n",
      "tensor([[-0.4871]], grad_fn=<AddmmBackward0>) tensor(-0.8665)\n",
      "tensor([[-0.6986]], grad_fn=<AddmmBackward0>) tensor(-0.8849)\n",
      "tensor([[0.0626]], grad_fn=<AddmmBackward0>) tensor(-0.0196)\n",
      "tensor([[0.5395]], grad_fn=<AddmmBackward0>) tensor(0.7392)\n",
      "tensor([[-0.3806]], grad_fn=<AddmmBackward0>) tensor(-0.0537)\n",
      "tensor([[-0.5484]], grad_fn=<AddmmBackward0>) tensor(-0.4412)\n",
      "tensor([[0.3348]], grad_fn=<AddmmBackward0>) tensor(0.8339)\n",
      "tensor([[0.4058]], grad_fn=<AddmmBackward0>) tensor(0.7928)\n",
      "tensor([[-0.5681]], grad_fn=<AddmmBackward0>) tensor(-0.4868)\n",
      "tensor([[-0.7288]], grad_fn=<AddmmBackward0>) tensor(-0.6426)\n",
      "tensor([[-0.3796]], grad_fn=<AddmmBackward0>) tensor(-0.4850)\n",
      "tensor([[-0.9924]], grad_fn=<AddmmBackward0>) tensor(0.8329)\n",
      "tensor([[-0.9246]], grad_fn=<AddmmBackward0>) tensor(-0.9979)\n",
      "tensor([[0.0393]], grad_fn=<AddmmBackward0>) tensor(0.0211)\n",
      "tensor([[0.0026]], grad_fn=<AddmmBackward0>) tensor(0.6301)\n",
      "tensor([[0.4710]], grad_fn=<AddmmBackward0>) tensor(0.7410)\n",
      "tensor([[-0.7465]], grad_fn=<AddmmBackward0>) tensor(-0.8310)\n",
      "tensor([[-0.5343]], grad_fn=<AddmmBackward0>) tensor(0.0200)\n",
      "tensor([[-0.1627]], grad_fn=<AddmmBackward0>) tensor(-0.6750)\n",
      "tensor([[0.0029]], grad_fn=<AddmmBackward0>) tensor(0.3818)\n",
      "tensor([[-0.6632]], grad_fn=<AddmmBackward0>) tensor(-0.6883)\n",
      "tensor([[0.1716]], grad_fn=<AddmmBackward0>) tensor(-0.7889)\n",
      "tensor([[-0.8014]], grad_fn=<AddmmBackward0>) tensor(-0.8962)\n",
      "tensor([[-0.4156]], grad_fn=<AddmmBackward0>) tensor(-0.7034)\n",
      "tensor([[-0.4901]], grad_fn=<AddmmBackward0>) tensor(-0.6792)\n",
      "tensor([[0.3636]], grad_fn=<AddmmBackward0>) tensor(0.5715)\n",
      "tensor([[-0.7283]], grad_fn=<AddmmBackward0>) tensor(-0.7609)\n",
      "tensor([[-0.6734]], grad_fn=<AddmmBackward0>) tensor(-1.)\n",
      "tensor([[0.8678]], grad_fn=<AddmmBackward0>) tensor(0.5742)\n",
      "tensor([[-0.6477]], grad_fn=<AddmmBackward0>) tensor(-0.8606)\n",
      "tensor([[-0.6956]], grad_fn=<AddmmBackward0>) tensor(-0.8876)\n",
      "tensor([[-0.6608]], grad_fn=<AddmmBackward0>) tensor(-0.4925)\n",
      "tensor([[0.3259]], grad_fn=<AddmmBackward0>) tensor(0.5501)\n",
      "tensor([[-0.4675]], grad_fn=<AddmmBackward0>) tensor(-0.6143)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x,y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(frame_data_tensor,labels_tensor):\n\u001b[1;32m      2\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(out,y)\n",
      "Cell \u001b[0;32mIn[44], line 121\u001b[0m, in \u001b[0;36mSpeedNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    117\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatchnorm4(x)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m#print(\"After pool2d_1:\", x.shape)\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Second 2D convolution and pooling\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatchnorm5(x)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m#print(\"After conv2d_2:\", x.shape)\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \n\u001b[1;32m    125\u001b[0m \n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Flatten the data\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for x,y in zip(frame_data_tensor,labels_tensor):\n",
    "    x = x.unsqueeze(0)\n",
    "    out = model.forward(x)\n",
    "    print(out,y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../sp_data/Full_data/SpeedNet_64.pth') #Has 64bins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previously the speed bins were 64 hence the Speed res was less\n",
    "### Now increase the features for 10 x kernel to 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before unsqueeze: torch.Size([2, 2, 256, 64])\n",
      "After unsqueeze: torch.Size([2, 1, 2, 256, 64])\n",
      "After conv3d_1: torch.Size([2, 8, 2, 252, 60])\n",
      "After pool3d_1: torch.Size([2, 8, 2, 252, 29])\n",
      "After conv3d_2: torch.Size([2, 16, 2, 248, 25])\n",
      "After pool3d_2: torch.Size([2, 16, 2, 248, 12])\n",
      "After conv3d_3: torch.Size([2, 32, 1, 244, 8])\n",
      "After squeeze: torch.Size([2, 32, 244, 8])\n",
      "After conv2d_1: torch.Size([2, 128, 235, 2])\n",
      "After pool2d_1: torch.Size([2, 128, 26, 1])\n",
      "After conv2d_2: torch.Size([2, 256, 22, 1])\n",
      "After pool2d_2: torch.Size([2, 256, 4, 1])\n",
      "After flatten: torch.Size([2, 1024])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x1024 and 768x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 150\u001b[0m\n\u001b[1;32m    147\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m train_input[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    149\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdeb_forward(input_tensor)\n\u001b[0;32m--> 150\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 136\u001b[0m, in \u001b[0;36mSpeedNet2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    133\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m#print(\"After flatten:\", x.shape)\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    137\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m    138\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x1024 and 768x64)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SpeedNet2(nn.Module):\n",
    "    def __init__(self, in_channels, f1,f2,f3,f4,f5):\n",
    "        super(SpeedNet2, self).__init__()\n",
    "        \n",
    "        self.batchnorm1 = nn.BatchNorm3d(f1)\n",
    "        self.batchnorm2 = nn.BatchNorm3d(f2)\n",
    "        self.batchnorm3 = nn.BatchNorm3d(f3)\n",
    "        self.batchnorm4 = nn.BatchNorm2d(f4)\n",
    "        self.batchnorm5 = nn.BatchNorm2d(f5)\n",
    "        # First 3D convolution layer\n",
    "        self.conv3d_1 = nn.Conv3d(in_channels, f1, kernel_size=(1, 5, 5))\n",
    "        self.pool3d_1 = nn.MaxPool3d(kernel_size=(1, 1, 3), stride=(1, 1, 2))\n",
    "        \n",
    "        # Second 3D convolution layer\n",
    "        self.conv3d_2 = nn.Conv3d(f1, f2, kernel_size=(1, 5, 5))\n",
    "        self.pool3d_2 = nn.MaxPool3d(kernel_size=(1, 1, 2), stride=(1, 1, 2))\n",
    "        \n",
    "        # Third 3D convolution layer\n",
    "        self.conv3d_3 = nn.Conv3d(f2, f3, kernel_size=(2, 5, 5))\n",
    "        \n",
    "        # 2D convolution layer\n",
    "        self.conv2d_1 = nn.Conv2d(f3, f4, kernel_size=(10, 7))\n",
    "        self.pool2d_1 = nn.MaxPool2d(kernel_size=(9, 1), stride=(9, 2))\n",
    "\n",
    "        self.conv2d_2 = nn.Conv2d(f4, f5, kernel_size=(5, 1))\n",
    "        self.pool2d_2 = nn.MaxPool2d(kernel_size=(5, 1), stride=(5, 1))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(1024, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32,1)\n",
    "        \n",
    "    def deb_forward(self, x):\n",
    "        # Reshape input tensor to add the single channel dimension\n",
    "        print(\"Before unsqueeze:\", x.shape)\n",
    "        x = x.unsqueeze(1)\n",
    "        print(\"After unsqueeze:\", x.shape)\n",
    "        \n",
    "        # First 3D convolution and pooling\n",
    "        x = self.conv3d_1(x)\n",
    "        print(\"After conv3d_1:\", x.shape)\n",
    "        x = self.pool3d_1(x)\n",
    "        print(\"After pool3d_1:\", x.shape)\n",
    "        \n",
    "        \n",
    "        # Second 3D convolution and pooling\n",
    "        x = self.conv3d_2(x)\n",
    "        print(\"After conv3d_2:\", x.shape)\n",
    "        x = self.pool3d_2(x)\n",
    "        print(\"After pool3d_2:\", x.shape)\n",
    "        \n",
    "        # Third 3D convolution\n",
    "        x = self.conv3d_3(x)\n",
    "        print(\"After conv3d_3:\", x.shape)\n",
    "        \n",
    "        # Squeeze the third dimension\n",
    "        x = x.squeeze(2)\n",
    "        print(\"After squeeze:\", x.shape)\n",
    "        \n",
    "        # First 2D convolution and pooling\n",
    "        x = self.conv2d_1(x)\n",
    "        print(\"After conv2d_1:\", x.shape)\n",
    "        x = self.pool2d_1(x)\n",
    "        print(\"After pool2d_1:\", x.shape)\n",
    "\n",
    "        # Second 2D convolution and pooling\n",
    "        x = self.conv2d_2(x)\n",
    "        print(\"After conv2d_2:\", x.shape)\n",
    "\n",
    "        x = self.pool2d_2(x)\n",
    "        print(\"After pool2d_2:\", x.shape)\n",
    "\n",
    "        \n",
    "        # Flatten the data\n",
    "        x = x.view(x.size(0), -1)\n",
    "        print(\"After flatten:\", x.shape)\n",
    "\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = F.relu(self.fc2(x))fter conv2d_1: torch.Size([2, 128, 235, 2\n",
    "        # x = self.fc3(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reshape input tensor to add the single channel dimension\n",
    "        #print(\"Before unsqueeze:\", x.shape)\n",
    "        x = x.unsqueeze(1)\n",
    "        #print(\"After unsqueeze:\", x.shape)\n",
    "        \n",
    "        # First 3D convolution and pooling\n",
    "        x = self.conv3d_1(x)\n",
    "        #print(\"After conv3d_1:\", x.shape)\n",
    "        x = self.pool3d_1(x)\n",
    "        #print(\"After pool3d_1:\", x.shape)\n",
    "        x = self.batchnorm1(x)\n",
    "        \n",
    "        # Second 3D convolution and pooling\n",
    "        x = self.conv3d_2(x)\n",
    "        #print(\"After conv3d_2:\", x.shape)\n",
    "        x = self.pool3d_2(x)\n",
    "        #print(\"After pool3d_2:\", x.shape)\n",
    "        x = self.batchnorm2(x)\n",
    "        # Third 3D convolution\n",
    "        x = self.conv3d_3(x)\n",
    "        #print(\"After conv3d_3:\", x.shape)\n",
    "        x = self.batchnorm3(x)\n",
    "        # Squeeze the third dimension\n",
    "        x = x.squeeze(2)\n",
    "        #print(\"After squeeze:\", x.shape)\n",
    "        \n",
    "        # First 2D convolution and pooling\n",
    "        x = self.conv2d_1(x)\n",
    "        #print(\"After conv2d_1:\", x.shape)\n",
    "        x = self.pool2d_1(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        #print(\"After pool2d_1:\", x.shape)\n",
    "\n",
    "        # Second 2D convolution and pooling\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.pool2d_2(x)\n",
    "        x = self.batchnorm5(x)\n",
    "        #print(\"After conv2d_2:\", x.shape)\n",
    "        \n",
    "        \n",
    "        # Flatten the data\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(\"After flatten:\", x.shape)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = SpeedNet2(1, 8,16,32,128,256)  # Increased features to 128 \n",
    "input_tensor = train_input[0:2]\n",
    "\n",
    "output = model.deb_forward(input_tensor)\n",
    "output = model.forward(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshayd/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpeedNet2(\n",
       "  (batchnorm1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm3): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3d_1): Conv3d(1, 8, kernel_size=(1, 5, 5), stride=(1, 1, 1))\n",
       "  (pool3d_1): MaxPool3d(kernel_size=(1, 1, 3), stride=(1, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3d_2): Conv3d(8, 16, kernel_size=(1, 5, 5), stride=(1, 1, 1))\n",
       "  (pool3d_2): MaxPool3d(kernel_size=(1, 1, 2), stride=(1, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3d_3): Conv3d(16, 32, kernel_size=(2, 5, 5), stride=(1, 1, 1))\n",
       "  (conv2d_1): Conv2d(32, 128, kernel_size=(10, 7), stride=(1, 1))\n",
       "  (pool2d_1): MaxPool2d(kernel_size=(5, 1), stride=(5, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2d_2): Conv2d(128, 256, kernel_size=(10, 1), stride=(1, 1))\n",
       "  (pool2d_2): MaxPool2d(kernel_size=(10, 1), stride=(10, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=768, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adagrad(model.parameters())\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(tensor_data:torch.tensor,tensor_label:torch.tensor,batch_size=64):\n",
    "    start_index = np.random.randint(0,len(tensor_data)-batch_size-1)\n",
    "    end_index = start_index+batch_size\n",
    "    return tensor_data[start_index:end_index],tensor_label[start_index:end_index] #return a view of data\n",
    "\n",
    "def eval_batch(tensor_data:torch.tensor,tensor_label:torch.tensor,epoch,eval_batch_size = 256):\n",
    "    start = np.random.randint(0,len(tensor_label)-eval_batch_size-1)\n",
    "    end = start+eval_batch_size\n",
    "    output_test = model(tensor_data[start:end])\n",
    "    loss_test = criterion(output_test.squeeze(),tensor_label[start:end])\n",
    "    print(f'Epoch {epoch+1}, Loss: {np.sqrt(loss_test.item())*max(labeldata)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating\n",
      "Epoch 1, Loss: 16.589617124678814\n",
      "Epoch 1, Loss: 19.682605804917\n",
      "Epoch 21, Loss: 10.282722945718856\n",
      "Epoch 41, Loss: 11.226940825752548\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), y_data)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "for epoch in range(30000):\n",
    "    # Zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    #random_batch = np.random.randint(5000,6000)\n",
    "    x_data,y_data = get_batch(frame_data_tensor,labels_tensor,batch_size=16)\n",
    "    outputs = model(x_data)\n",
    "    loss = criterion(outputs.squeeze(), y_data)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch%100==0:\n",
    "        print(\"Evaluating\")\n",
    "        eval_batch(test_input,test_output,epoch)\n",
    "    if epoch%20==0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {np.sqrt(loss.item())*max(labeldata)}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
